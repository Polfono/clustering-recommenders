\documentclass[a4paper,12pt]{report}
\usepackage{pdfpages}
\usepackage[utf8]{inputenc}
\usepackage[catalan]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[sorting=none]{biblatex}
\usepackage{float}
\usepackage{verbatim}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{multirow}



\usepackage[x11names]{xcolor}      % o simplemente \usepackage{ xcolor }

% Definición de colores personalizados
\definecolor{customBlue}   {HTML}{1F77B4} % azul
\definecolor{customOrange} {HTML}{FF7F0E} % naranja
\definecolor{customGreen}  {HTML}{2CA02C} % verde
\definecolor{customRed}    {HTML}{D62728} % rojo
\definecolor{customPurple}{HTML}{9467BD}  % púrpura
\definecolor{customBrown} {HTML}{8C564B}  % marrón
\definecolor{customPink}  {HTML}{E377C2}  % rosa

\addbibresource{biblio.bib}

\title{DISSENY I ANÀLISI DE TÈCNIQUES DE
CLUSTERING APLICADES A SISTEMES DE
RECOMANACIÓ}
\author{POL FONOYET GONZÁLEZ}
\date{\today}

\begin{document}


\setcounter{secnumdepth}{4}  % Assegurar-se que es numeren fins a subsubsecció

\setlength{\parskip}{10pt}

\begin{titlepage}
    
    % imatge de la UPC
    \vspace*{-3cm}
    \hspace{-0.15\textwidth}
    \includegraphics[width=0.15\textwidth]{Figuras/logoUPC.png}
    \hspace{0.02\textwidth}
    \includegraphics[width=1\textwidth]{Figuras/logoFIB.png}\par

    % título en negrita
    \vspace{3cm}
    \centering
    \Large\textbf{DISSENY I ANÀLISI DE TÈCNIQUES DE CLUSTERING APLICADES A SISTEMES DE RECOMANACIÓ}

    % nombre del autor
    \vspace{3cm}
    \centering
    \large\textbf{POL FONOYET GONZÁLEZ}\\
    \vspace{0.5cm}
    \textbf{Treball Final de Grau}\\
    \vspace{0.5cm}
    \Large \today

    % información de la universidad
    \vspace{3cm}
    \centering
    \small
    \textbf{Director/a}\\
    CONRADO MARTÍNEZ PARRA (Departament de Ciències de la Computació)\\
    \vspace{0.5cm}
    \textbf{Titulació}\\
    Grau en Enginyeria Informàtica (Computació)\\
    \vspace{0.5cm}
    \vspace{0.5cm}
    \textbf{Facultat d'Informàtica de Barcelona (FIB)}\\
    \vspace{0.5cm}
    \textbf{Universitat Politècnica de Catalunya (UPC) - BarcelonaTech}\\


\end{titlepage}

\newgeometry{top=2.5cm, left=3cm, right=2.5cm, bottom=2.5cm} % Ajustar el marge superior després de la portada

% --- INICI SECCIONS DE RESUMS ---
\selectlanguage{catalan}
\chapter*{Resum}
% \addcontentsline{toc}{chapter}{Resum} % Descomenta si vols que aparegui a l'índex
Els sistemes de recomanació són fonamentals en l'entorn digital, però s'enfronten a reptes significatius com l'escalabilitat i la dispersió de dades. L'ús de tècniques de clustering, agrupant usuaris amb interessos similars, es presenta com una solució potencial per mitigar aquests problemes. Aquest treball de final de grau se centra en el disseny, la implementació i l'anàlisi comparativa de diverses tècniques de clustering (K-means, Fuzzy C-Means amb assignació dura, Aglomeratiu Jeràrquic, DBSCAN i Gaussian Mixture Models) aplicades com a pas previ al filtratge col·laboratiu en sistemes de recomanació. La metodologia emprada inclou l'ús de múltiples conjunts de dades, tant reals com sintètics, i l'anàlisi de diferents mètriques de distància com l'Euclidiana i la de Pearson. S'avalua la precisió de les prediccions mitjançant NMAE i NRMSE, i la qualitat de la segmentació dels clústers amb l'entropia normalitzada. L'objectiu és determinar com aquestes tècniques de clustering, amb les seves diferents configuracions, influeixen en el rendiment global dels sistemes de recomanació en diversos escenaris.
\\
\\
Paraules clau: clustering, sistemes de recomanació, aprenentatge automàtic, anàlisi de dades.

\clearpage % Opcional, per començar l'Abstract en una nova pàgina

\selectlanguage{english}
\chapter*{Abstract}
% \addcontentsline{toc}{chapter}{Abstract} % Descomenta si vols que aparegui a l'índex
Recommendation systems are fundamental in the digital environment but face significant challenges such as scalability and data sparsity. The use of clustering techniques, by grouping users with similar interests, is presented as a potential solution to mitigate these problems. This final degree project focuses on the design, implementation, and comparative analysis of various clustering techniques (K-means, Fuzzy C-Means with hard assignment, Hierarchical Agglomerative Clustering, DBSCAN, and Gaussian Mixture Models) applied as a preliminary step to collaborative filtering in recommendation systems. The methodology employed includes the use of multiple datasets, both real and synthetic, and the analysis of different distance metrics such as Euclidean and Pearson. Prediction accuracy is evaluated using NMAE and NRMSE, and cluster segmentation quality is assessed with normalized entropy. The aim is to determine how these clustering techniques, with their different configurations, influence the overall performance of recommendation systems in various scenarios.
\\
\\
Keywords: clustering, recommendation systems, machine learning, data analysis.

\clearpage % Opcional, per començar el Resumen en una nova pàgina

\selectlanguage{spanish}
\chapter*{Resumen}
% \addcontentsline{toc}{chapter}{Resumen} % Descomenta si vols que aparegui a l'índex
Los sistemas de recomendación son fundamentales en el entorno digital, pero se enfrentan a desafíos significativos como la escalabilidad y la dispersión de datos. El uso de técnicas de clustering, agrupando usuarios con intereses similares, se presenta como una solución potencial para mitigar estos problemas. Este trabajo de fin de grado se centra en el diseño, la implementación y el análisis comparativo de diversas técnicas de clustering (K-means, Fuzzy C-Means con asignación dura, Aglomerativo Jerárquico, DBSCAN y Gaussian Mixture Models) aplicadas como paso previo al filtrado colaborativo en sistemas de recomendación. La metodología empleada incluye el uso de múltiples conjuntos de datos, tanto reales como sintéticos, y el análisis de diferentes métricas de distancia como la Euclidiana y la de Pearson. Se evalúa la precisión de las predicciones mediante NMAE y NRMSE, y la calidad de la segmentación de los clústeres con la entropía normalizada. El objetivo es determinar cómo estas técnicas de clustering, con sus diferentes configuraciones, influyen en el rendimiento global de los sistemas de recomendación en diversos escenarios.
\\
\\
Palabras clave: clustering, sistemas de recomendación, aprendizaje automático, análisis de datos.

\selectlanguage{catalan} % Tornar a l'idioma principal del document
% --- FI SECCIONS DE RESUMS ---

\clearpage

\tableofcontents

\listoffigures

\listoftables

\part{Memoria Tècnica}

\chapter{Contextualització i Abast del projecte}

\section{Introducció}

Aquest treball de final de grau s'ha dut a terme en el Grau d'Enginyeria Informàtica impartit en el marc de la Facultat d'Informàtica de Barcelona (FIB), pertanyent a la Universitat Politècnica de Catalunya (UPC). Dins d'aquest programa acadèmic, s'ofereixen diverses àrees d'especialització, entre les quals destaca la de Computació. El present projecte se situa en aquesta especialització, centrant el seu enfocament en temàtiques clau com la Intel·ligència Artificial (IA) i l'Aprenentatge Automàtic (AA).

\subsection{Context}

A mesura que la quantitat de dades generades en l'entorn digital segueix augmentant, la necessitat de tècniques avançades per analitzar i processar aquesta informació es torna cada vegada més crítica.
En aquest escenari, els sistemes de recomanació han emergit com a eines essencials per personalitzar l'experiència d'usuari, filtrant i suggerint continguts rellevants en funció de les seves preferències i comportaments \cite{Idrees_2024}.
Aquests sistemes exerceixen un paper fonamental en àmbits tan diversos com el comerç electrònic, l'entreteniment i l'educació, permetent millorar la interacció i satisfacció dels usuaris \cite{Kantor_Ricci_Rokach_Shapira_2011}.
Segons un informe de McKinsey, el 35\% del que els consumidors compren a Amazon i el 75\% del que veuen a Netflix provenen de recomanacions \cite{MacKenzie_Meyer_Noble_2013}.

La personalització efectiva requereix comprendre les preferències i comportaments dels usuaris.
El filtratge col·laboratiu, una tècnica àmpliament utilitzada, enfronta dos grans desafiaments: l'escalabilitat i la dispersió.
A mesura que augmenta la quantitat d'usuaris i ítems, les matrius d'utilitat es tornen enormes, cosa que incrementa el cost computacional \cite{sarwar2001item}.
A més, la majoria dels usuaris interactuen amb només una fracció dels ítems, resultant en matrius molt disperses, cosa que dificulta el càlcul precís de similituds \cite{adomavicius2005toward}.

Les tècniques de clustering, com k-means, DBSCAN i k-NN, permeten agrupar usuaris amb interessos similars, facilitant la identificació de patrons i la generació de recomanacions més precises \cite{jain2008data}.
En aplicar clustering com a pas previ al filtratge col·laboratiu, es poden crear grups més petits i homogènics, reduint la complexitat de les matrius i el temps de còmput.
A més, en treballar amb grups d'usuaris similars, es redueix el problema de la dispersió, ja que els usuaris dins d'un mateix clúster tendeixen a tenir més interaccions en comú \cite{Kantor_Ricci_Rokach_Shapira_2011}.

Aquest projecte se centra en el disseny i l'anàlisi de tècniques de clustering aplicades a sistemes de recomanació, amb l'objectiu d'optimitzar la qualitat i el rendiment de les recomanacions.

\subsection{Conceptes}

A continuació, es defineixen els termes i conceptes clau relacionats amb el tema d'estudi.

\subsubsection{Sistema de Recomanació}

Un sistema de recomanació és un sistema que calcula i proporciona contingut rellevant a l'usuari basant-se en el coneixement de l'usuari, el contingut i les interaccions entre l'usuari i l'element. \cite{falk2019practical}

El propòsit d’una plataforma de recomanació de contingut és facilitar als usuaris l’accés a informació rellevant de manera ràpida i eficient, millorant la seva experiència i fomentant la seva permanència en el servei. Al mateix temps, busca optimitzar recursos i maximitzar beneficis per al proveïdor, assegurant que el contingut ofert sigui atractiu i sostenible econòmicament. Això es fa mitjançant sistemes intel·ligents que analitzen el comportament dels usuaris per oferir opcions personalitzades, tot mantenint un equilibri entre satisfacció i rendibilitat.

\subsubsection{Filtratge Col·laboratiu}

La idea principal dels enfocaments de recomanació col·laborativa és aprofitar la informació sobre el comportament passat o les opinions d'una comunitat d'usuaris existent per predir quins elements li agradaran o interessaran més a l'usuari actual del sistema \cite{Jannach2011}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figuras/DiagramaFiltratgeCol.jpg}
    \caption{Diagrama de filtratge col·laboratiu \cite{falk2019practical}}
    \label{fig:collaborative-filtering-diagram}
\end{figure}

La figura \ref{fig:collaborative-filtering-diagram} il·lustra com funciona el filtrat col·laboratiu. El cercle exterior representa el catàleg complet. El cercle intermedi representa un grup d'usuaris que han consumit elements similars. El sistema de recomanació suggereix elements del cercle més proper (davant), partint de la idea que si aquests usuaris coincideixen en preferències amb l’usuari actual, aquest també preferirà els elements que han consumit.

El grup es determina mitjançant la superposició entre els elements que han agradat als usuaris i els que ha agradat a l’usuari actual. Les recomanacions corresponen a la part del cercle intermedi no coberta pel cercle de l’usuari actual (és a dir, els elements que ell/ella no ha consumit però sí el grup semblant).


\subsubsection{Clustering}

El clustering, segons la perspectiva de \cite{jain2008data}, es concep fonamentalment com l'organització de dades dins de grups cohesionats, on la semblança entre els elements d'un mateix grup supera la que mantenen amb els d'altres grups.
Aquesta tècnica, que es fonamenta en mesures de similitud o distància, és clau en l’anàlisi de dades sense supervisió, ja que permet identificar estructures o patrons latents en els conjunts de dades sense necessitat d’etiquetes prèvies. Així, el clustering facilita la comprensió i la interpretació de grans volums d’informació, sent una eina fonamental tant en l’exploració de dades com en l’aprenentatge automàtic. 

\subsection{Identificació del problema}

En aquest treball es desenvoluparà una comparativa entre diferents tècniques de clustering aplicades als sistemes de recomanació. El problema es pot dividir en tres blocs de treball:

\subsubsection{Selecció i tractament del conjunt de dades}

Aquest primer bloc consisteix en la recerca, selecció i/o creació de datasets, tant reals com sintètics, que representin diferents volums de dades i nivells de dispersió,  
assegurant que els escenaris simulats reflecteixin les diverses situacions que es poden presentar en entorns reals.

\subsubsection{Desenvolupament del sistema de recomanació}

En aquest segon bloc es construirà el sistema de recomanació, on el clustering actuarà com a pas previ al filtratge col·laboratiu.  
Aquest també capturarà diferents mètriques de precisió i eficiència computacional.  
A més, estarà dissenyat de manera que es pugui escollir amb quina tècnica de clusterització ha de funcionar.

\subsubsection{Comparació de resultats i anàlisi del rendiment}

Finalment, es compararan els resultats de les diferents tècniques emprades amb els diferents datasets amb l'objectiu de determinar quina tècnica ofereix el millor equilibri entre eficàcia i eficiència en diferents escenaris.

\subsection{Actors implicats}

En aquest projecte hi ha diversos actors implicats:

\begin{itemize}
    \item \textbf{Estudiant/investigador}: Responsable de desenvolupar, implementar i analitzar les tècniques de clustering aplicades als sistemes de recomanació.
    \item \textbf{Director/tutor acadèmic}: Responsable de supervisar el progrés del projecte i oferir orientació acadèmica.
    \item \textbf{Comunitat acadèmica}: Inclou altres estudiants, investigadors i professionals interessats en els resultats del projecte. Els principals beneficiaris dels resultats serien els investigadors en temes de sistemes de recomanació i aprenentatge automàtic.
    \item \textbf{Proveïdors de datasets}: Organitzacions o comunitats que proporcionen conjunts de dades reals i sintètics per a l'entrenament i proves dels algorismes de clustering.
\end{itemize}

\section{Justificació}

L'augment exponencial de la informació digital ha generat un repte important per als sistemes de recomanació, ja que els mètodes tradicionals, com el filtratge col·laboratiu, es veuen limitats per problemes d'escala i dispersió de dades.
Aquesta situació posa de manifest la necessitat d'explorar noves estratègies per millorar tant la precisió com l'eficiència en la generació de recomanacions.

En aquest context, l'aplicació de tècniques de clustering com a pas previ ofereix un potencial significatiu: agrupar usuaris i ítems en clústers més cohesius redueix la dimensió de la matriu usuari-ítem, minimitza el cost computacional i afavoreix una major densitat en les dades per a càlculs més precisos.
Tot i que existeixen diverses solucions informàtiques que integren el clustering en sistemes de recomanació, en la revisió de la literatura no s'han trobat estudis que comparin explícitament els diferents algoritmes de clustering en aquest context de manera sistemàtica.

\subsection{Estat de l'art}

En els darrers anys, l'interès per optimitzar els sistemes de recomanació ha condüït a l'aplicació de diverses tècniques d'aprenentatge automàtic, amb especial èmfasi en el filtratge col·laboratiu.
Paral·lelament, s'ha observat una creixent utilització de tècniques de clustering per reduir la dimensionalitat de la matriu usuari-ítem i millorar la densitat de les dades, aspecte clau per a l'eficiència computacional i la precisió de les recomanacions.

Diversos estudis han abordat l'ús d'algoritmes com k-means i DBSCAN, entre d'altres, per agrupar usuaris amb interessos similars, facilitant així la generació de recomanacions personalitzades.
Aquestes investigacions han demostrat que el pre-processament de les dades mitjançant clustering pot reduir significativament el temps de càlcul i millorar els resultats en entorns amb grans volums de dades.

No obstant això, en la revisió de la literatura existent s'ha constatat la manca d'estudis que realitzin una comparativa explícita i sistemàtica dels diferents algoritmes de clustering aplicats específicament en el context dels sistemes de recomanació.
La major part dels treballs se centra en la implementació d'un sol mètode o en l'aplicació general del clustering sense aprofundir en la comparació directa dels resultats obtinguts per cadascun d'ells.

Aquest buit en la recerca representa una oportunitat per aportar una anàlisi més detallada i una valoració crítica dels avantatges i inconvenients de cada tècnica.
El present projecte pretén omplir aquesta llacuna aportant una comparativa exhaustiva dels principals algoritmes de clustering, la qual cosa servirà com a referència tant per a la comunitat acadèmica com per a la indústria, i contribuirà a la millora dels sistemes de recomanació en entorns reals.

\subsection{Selecció d'eines}

En aquest projecte s'ha realitzat una acurada elecció de les eines i entorns de treball, tenint en compte la necessitat d'executar càlculs numèrics, processar grans volums de dades, implementar tècniques d'aprenentatge automàtic i visualitzar els resultats d'una manera clara i professional.

\textbf{Python} és el llenguatge de programació seleccionat per la seva sintaxi senzilla, la seva versatilitat i la seva àmplia comunitat, que garanteix un suport continu i una gran quantitat de recursos per al desenvolupament d'aplicacions científiques i d'aprenentatge automàtic.

Per al processament de dades i càlculs numèrics, s'empren les llibreries \textbf{NumPy} i \textbf{Pandas}. NumPy facilita la manipulació d'arrays i l'execució d'operacions matemàtiques d'alt rendiment, mentre que Pandas proporciona estructures de dades eficients (com els DataFrames) per a la manipulació i anàlisi de dades, essent essencials per a la preparació i neteja dels datasets.

La implementació i comparativa dels diferents algoritmes de clustering es realitzarà amb la llibreria \textbf{Scikit-learn}, que inclou una àmplia varietat d'eines per a l'aprenentatge automàtic, incloent mètodes de classificació, regressió, clustering i reducció de dimensionalitat. Aquesta eina és fonamental per a dur a terme experiments rigorosos i comparatius.

Per a la visualització dels resultats i l'anàlisi exploratòria, s'utilitzarà \textbf{Matplotlib}, que permet generar gràfics i representacions visuals personalitzables i de gran qualitat, facilitant així la interpretació de les dades.

A més, s'incorpora el paquet \textbf{Surprise}, especialitzat en sistemes de recomanació, que facilita la implementació de tècniques de filtratge col·laboratiu i la comparativa amb els mètodes basats en clustering.

Pel que fa als entorns de desenvolupament, s'ha optat per \textbf{Jupyter Notebook}, que permet combinar codi, visualitzacions i documentació en un entorn interactiu, afavorint la iteració i validació ràpida de hipòtesis.
El control de versions es gestionarà amb \textbf{Git}, assegurant un seguiment acurat dels canvis.
Finalment, \textbf{LaTeX} s'utilitzarà per a la redacció i formatatge del document final, garantint una presentació clara, estructurada i professional dels resultats.

\section{Abast}

\subsection{Objectius}

L'objectiu principal d'aquest projecte és desenvolupar i comparar diferents tècniques de clustering en el context dels sistemes de recomanació. Això implica: 

\begin{itemize} 
    \item Implementar un mòdul de clustering basat en tècniques particionals clàssiques. 
    \item Desenvolupar versions alternatives que utilitzin tècniques com clustering difús i jeràrquic. 
    \item Analitzar i comprendre el funcionament i les particularitats de cada mètode de clustering. 
    \item Avaluar, mitjançant mètriques específiques, la precisió, l'eficiència computacional i la robustesa de cada enfocament. 
    \item Contribuir al coneixement en l'àmbit dels sistemes de recomanació, aportant una comparativa rigorosa i documentada que pugui servir de referència tant per a la comunitat acadèmica com per a futurs desenvolupaments en l'àrea. 
\end{itemize}

\subsection{Subobjectius}

Per assolir l'objectiu principal es plantegen els següents subobjectius: 

\begin{itemize} 
    \item Dur a terme una revisió bibliogràfica exhaustiva sobre tècniques de clustering i la seva aplicació en sistemes de recomanació. 
    \item Dissenyar una arquitectura modular que permeti integrar fàcilment diferents algoritmes de clustering en el sistema de recomanació. 
    \item Implementar els algorismes seleccionats de manera que es puguin comparar de forma sistemàtica. 
    \item Capturar i analitzar mètriques clau, com ara el temps d'execució, la densitat de dades i la precisió de les recomanacions. 
    \item Elaborar una documentació detallada que reculli els resultats experimentals i les conclusions obtingudes. 
\end{itemize}

\subsection{Requeriments}

\subsubsection{Funcionals}

\begin{itemize} 
    \item \textbf{Recollida i tractament de dades:} Seleccionar conjunts de dades (reals o sintètics) que representin diferents escenaris en sistemes de recomanació.
    Aplicar tècniques de preprocessament i neteja per garantir la qualitat de les dades per a l'anàlisi.
    \item \textbf{Implementació dels algorismes de clustering:} Desenvolupar implementacions de diferents tècniques de clustering (particional clàssic, difús i jeràrquic).
    Adaptar els algorismes per estudiar el seu comportament i comparativa en diferents contextos.
    \item \textbf{Anàlisi dels resultats:} Definir i aplicar mètriques d'avaluació que permetin comparar de manera rigorosa els diferents mètodes.
    Interpretar els resultats per identificar avantatges, limitacions i possibles àrees de millora de cada tècnica.
\end{itemize}

\subsubsection{No funcionals}

\begin{itemize} 
    \item \textbf{Claredat i documentació:} Redactar una documentació completa i exhaustiva que reculli la metodologia, els procediments experimentals i l'anàlisi dels resultats.
    \item \textbf{Reproductibilitat:} Assegurar que els experiments es puguin reproduir seguint la metodologia descrita, facilitant la verificació i validació dels resultats.
    \item \textbf{Validesa dels resultats:} Garantir que les tècniques aplicades permetin arribar a conclusions rigoroses i ben fonamentades, centrant-se en l'anàlisi comparativa més que en la implementació de solucions optimitzades.
\end{itemize}

\subsection{Obstacles i riscos}

\subsubsection{Obstacles}

Els principals obstacles que es poden trobar en el desenvolupament del projecte són:

\begin{itemize}
    \item \textbf{Falta de coneixement profund en la matèria:} L'autor del treball no té un gran coneixement previ de les diferents tècniques de clustering ni de sistemes de recomanació. Menys de la seva aplicació conjunta.
    \item \textbf{Potència computacional limitada:} El processament i anàlisi de grans volums de dades poden requerir recursos computacionals elevats.
    \item \textbf{Documentació extensa:} La normativa actual dels TFG exigeix la generació d’una àmplia documentació, incloent-hi entregables de GEP i la memòria. Tot i que la seva elaboració demanda una inversió de temps considerable, aquests materials permeten demostrar que s’han aplicat correctament les competències adquirides durant la titulació.
\end{itemize}

\subsubsection{Riscos}

Els riscos associats al projecte inclouen: 

\begin{itemize}
    \item \textbf{Elements imprevistos en el desenvolupament:} Dificultats tècniques o situacions no anticipades poden afectar el progrés del projecte.
    \item  \textbf{Gestió del temps:} La coordinació entre la implementació, els experiments i la redacció de la documentació pot resultar més complexa del previst. L'autor també està cursant assignatures que podrien demandar pics d'activitat inesperats.
    \item \textbf{Accidents o imprevistos personals:} Eventualitats que afectin la disponibilitat de temps per al desenvolupament del projecte.
\end{itemize}

\section{Metodologia i rigor}

L'èxit de qualsevol projecte depèn en gran mesura de la implementació de mètodes estructurats i rigorosos, que estableixen un marc organitzat per afavorir l'eficiència, la qualitat i una comunicació efectiva.

\subsection{Metodologia de treball}

Atès que és un projecte individual, una metodologia àgil i flexible com Kanban és ideal. El mètode Kanban és una metodologia àgil que es fonamenta en la visualització del projecte per millorar la transparència i la col·laboració entre els membres de l'equip.
Va ser creat per Taiichi Ohno als anys 40 per a la gestió de la producció, però es va adaptar al món de la gestió de projectes.
Es distingeix per la seva simplicitat i per la seva capacitat d’adaptar-se a organitzacions amb estructures jeràrquiques tradicionals.
Kanban funciona amb un sistema visual que permet fer un seguiment clar del progrés del projecte \cite{Person_2025}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figuras/KANBAN.png}
    \caption{Diagrama il·lustratiu del flux de treball segons la metodologia Kanban \cite{Person_2025}}
    \label{fig:kanban}
\end{figure}

A la figura \ref{fig:kanban} es mostren tres columnes principals: \textit{To Do} (Per fer), \textit{Doing} (En procés) i \textit{Done} (Fet).
Aquestes columnes representen les fases de treball en què es troben les tasques. Els membres de l'equip traslladen les tasques des de la columna de \textit{To Do} cap a \textit{In Progress} quan comencen a treballar-hi i, finalment, a \textit{Done} quan s’acaben, seguint els límits de treball en curs (WIP) per garantir que el flux de treball es mantingui eficaç.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figuras/Notion.png}
    \caption{Captura de pantalla de la plataforma Notion}
    \label{fig:notion}
\end{figure}

Per a la implementació d'aquesta metodologia s'utilitzarà la web de Notion.
Notion és una plataforma col·laborativa i de gestió de projectes que facilita l'organització de tasques de manera visual i flexible.
Com es veu a la figura \ref{fig:notion}, permet dividir les tasques en els tres grups esmentats, cosa que s'alinea perfectament amb l'enfocament Kanban, assegurant una gestió eficient, una comunicació fluida i una transparència en el seguiment del progrés del projecte.

\subsection{Seguiment}

Per tal de verificar que el desenvolupament del projecte s’ajusta als requeriments establerts i per corregir sense dilació qualsevol desviació que pugui sorgir, es realitzarà el seguiment del projecte mitjançant reunions presencials cada dues setmanes. En aquestes trobades es procedirà a:

\begin{itemize}
    \item \textbf{Analitzar el progrés global:} Revisarem l’evolució tant del programari com de la documentació, identificant els avanços assolits i les possibles àrees de millora.
    \item \textbf{Detectar dificultats i incidències:} S'exposaran les dificultats trobades durant el desenvolupament, així com qualsevol desviació respecte als requisits inicials.
    \item \textbf{Establir mesures correctores:} Es discutiran i acordaran les millores i ajustaments necessaris per garantir l'alineació amb els objectius del projecte.
    \item \textbf{Comprovar el compliment dels requisits:} Amb l’ajuda d’indicadors clars, es verificarà el grau de compliment dels requisits i subrequisits, facilitant així una gestió proactiva de les incidències.
\end{itemize}

Aquest sistema de seguiment, basat en reunions presencials periòdiques, permetrà una presa de decisions àgil i una coordinació efectiva entre els actors implicats, assegurant que el projecte es desenvolupi de manera coherent i amb el nivell de qualitat desitjat.

\chapter{Conjunt de dades}

Una selecció acurada dels conjunts de dades és fonamental per a l'avaluació rigorosa i la comparació de les tècniques de clustering aplicades als sistemes de recomanació.
Aquest capítol detalla els datasets emprats en aquest estudi, escollits per la seva rellevància en la comunitat d'investigació de sistemes de recomanació i per representar una varietat de dominis, mides i nivells de dispersió.
L'ús de múltiples datasets permet analitzar la robustesa i la capacitat de generalització dels mètodes de clustering en diferents contextos.

\section{Restriccions i requisits dels conjunts de dades}

Amb l’objectiu de facilitar la comparació i la reproduïbilitat dels experiments, s’ha definit una sèrie de restriccions i requisits que han de complir els conjunts de dades utilitzats. Aquestes condicions tenen com a finalitat garantir una certa homogeneïtat entre els conjunts i, alhora, millorar la qualitat i la interpretabilitat dels resultats obtinguts.

En primer lloc, s’exigeix que les dades estiguin emmagatzemades en format CSV. Aquest format ha estat escollit perquè és àmpliament utilitzat, fàcilment llegible tant per humans com per màquines, i compatible amb la majoria de biblioteques i entorns de programació emprats habitualment en l’anàlisi de dades. A més, el format CSV afavoreix una estructura tabular clara, la qual facilita el processament i la validació automàtica dels conjunts de dades.

Aquest arxiu contindrà exactament tres columnes amb els següents noms, que inclouran la informació següent:
\begin{itemize}
    \item \textbf{userId:} Identificador únic de cada usuari del sistema de recomanació.
    \item \textbf{itemId:} Identificador únic de cada element o producte que es recomana.
\item \textbf{rating:} Valoració atorgada per l'usuari a l'element.
\end{itemize}

Tant \textbf{userId} com \textbf{itemId} es representaran com a enters de 64 bits, i \textbf{rating} com un valor de tipus float de 64 bits.

Cap de les tres columnes pot contenir valors no numèrics o NaN; en cas que se'n detectin, es descartaran.

No es permetrà que hi hagi parells \textbf{(userId, itemId)} repetits. Si se'n troben, es mantindrà l’última valoració afegida per aquest parell.

Com a restricció tècnica, s’imposarà un màxim d’uns 10.000 usuaris.
Aquesta limitació es deu a l’ús de matrius per emmagatzemar la similitud entre usuaris, amb un creixement quadràtic en funció del nombre d’usuaris.
L’espai necessari ve donat per la fórmula següent:

\[
\text{Nombre d’usuaris}^2 \times 8 \text{ bytes}
\]

Els 8 bytes corresponen a la representació de nombres amb comes flotants de 64 bits per expressar la similitud.
Assumint 10.000 usuaris, això suposa un total de $10^8$ posicions, amb un requeriment d’uns 800 MB, la qual cosa comença a representar un desafiament per executar els experiments en local.

Una solució senzilla seria calcular la similitud entre usuaris de manera dinàmica, només quan sigui necessària, en lloc de calcular-les totes prèviament i emmagatzemar-les.
Tanmateix, aquesta estratègia implica un increment significatiu del temps de càlcul, que pot resultar inviable per a una execució local.
Per aquest motiu, s’ha descartat aquesta opció.

De manera similar a l’anterior, s’imposarà un màxim d’uns 10.000 elements.
Aquesta limitació també es deu a l’ús de matrius per emmagatzemar la valoració entre usuari i element, la qual cosa implica un creixement proporcional al producte entre el nombre d’usuaris i el nombre d’elements.

En aquest cas, l’espai necessari ve donat per:

\[
\text{Nombre d’usuaris} \times \text{Nombre d’elements} \times 8 \text{ bytes}
\]

Els 8 bytes corresponen a la representació de nombres amb comes flotants de 64 bits per expressar la valoració.
De la mateixa manera que amb la matriu de similituds, l’espai comença a ser significatiu i pot comprometre el rendiment en entorns amb recursos limitats.

Una manera de reduir dràsticament l'espai utilitzat per aquesta matriu és fer ús de matrius disperses.
El motiu d’això és el fet que la majoria d’elements no solen estar valorats per l’usuari i, per tant, la gran majoria de les posicions de la matriu contindran valors nuls o buits.

L’ús de matrius disperses permet emmagatzemar només les posicions que contenen informació rellevant (és a dir, valoracions efectives), reduint dràsticament l’espai requerit.

Tot i això, finalment s’ha descartat l’ús de matrius disperses, ja que, malgrat l’estalvi d’espai, aquest enfocament incrementa considerablement la complexitat del codi i el temps de càlcul.
La gestió de matrius disperses implica estructures de dades més sofisticades i operacions més costoses, especialment en operacions massives o en entorns on es requereix un accés ràpid i freqüent a les valoracions.
Per aquest motiu, s’ha optat per mantenir l’enfocament basat en matrius densament poblades, tot i la limitació del nombre màxim d’elements.

\section{Conjunt de dades utilitzades}

\subsection{Conjunt de dades MovieLens} 

El conjunt de dades MovieLens \cite{harper2015movielens} és un dels més utilitzats en la recerca sobre sistemes de recomanació. Proporcionat pel GroupLens Research Project, aquest conjunt inclou valoracions d’usuaris sobre pel·lícules i s’ha convertit en un estàndard de facto per a l’avaluació d’algorismes de filtratge col·laboratiu.

Aquest conjunt recull valoracions amb una escala de fins a 5 estrelles del servei de recomanació de pel·lícules MovieLens.

En aquest estudi s’han emprat dos subconjunts del conjunt de dades MovieLens: el MovieLens Latest Small i el MovieLens 1M.

El MovieLens Latest Small és un subconjunt recent que conté 100.836 valoracions sobre 9.724 pel·lícules, realitzades per 610 usuaris entre el 29 de març de 1996 i el 24 de setembre de 2018.

A partir d’una anàlisi exploratòria de les dades (Figura~\ref{fig:analisis_ml_small}), s’observa que aquest conjunt presenta una densitat de l’1,7\,\%, fet que indica que només l’1,7\,\% de les possibles valoracions estan registrades al conjunt.

\[
\text{Densitat} = \frac{\text{Nombre de valoracions}}{\text{Nombre d’usuaris} \times \text{Nombre de pel·lícules}} \times 100
\]

Les valoracions són predominantment positives, amb una mitjana de 3,5 estrelles i una desviació estàndard d’1,04.
A més, les valoracions per usuari i pel·lícula tenen una distribució fortament sesgada a la dreta.
El sistema té una majoria d'usuaris i pel·lícules amb poques valoracions, mentre que una petita fracció d’usuaris i pel·lícules concentra una gran quantitat de valoracions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{Figuras/ml-small-ratings.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/ml-small-users.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/ml-small-items.png}
    \caption{Anàlisi exploratòria del conjunt MovieLens Latest Small}
    \label{fig:analisis_ml_small}
\end{figure}

D’altra banda, el MovieLens 1M és un conjunt de dades més gran que conté 1.000.209 valoracions de 6.040 usuaris sobre 3.706 pel·lícules, recollides entre els anys 2000 i 2003.

A partir de l’anàlisi exploratòria (Figura~\ref{fig:analisis_ml_1m}), s’observa una densitat del 4,5\,\%, és a dir, només el 4,5\,\% de les possibles valoracions estan recollides en el conjunt.

També en aquest cas, les valoracions són majoritàriament positives, amb una mitjana de 3,58 estrelles i una desviació estàndard d’1,12.
De manera similar al conjunt MovieLens Latest Small, les valoracions per usuari i pel·lícula presenten una distribució fortament sesgada a la dreta, però amb una densitat més alta que en el conjunt anterior.

Cal destacar que les valoracions en aquest conjunt no inclouen puntuacions fraccionàries (com 0,5, 1,5, 2,5, etc.), ja que el sistema original de valoració utilitzava només valors enters de 1 a 5 estrelles.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{Figuras/ml-1m-ratings.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/ml-1m-users.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/ml-1m-items.png}
    \caption{Anàlisi exploratòria del conjunt MovieLens 1M}
    \label{fig:analisis_ml_1m}
\end{figure}

\subsection{Conjunt de dades Book-Crossing}

El conjunt de dades \emph{Book-Crossing} \cite{ziegler2005improving} prové de la comunitat en línia \emph{BookCrossing.com}, una plataforma destinada a amants de la lectura d’arreu del món que intercanvien llibres i comparteixen experiències. Aquest conjunt ha estat àmpliament utilitzat en estudis sobre sistemes de recomanació, especialment en escenaris que combinen valoracions explícites i implícites.

Les dades es van recollir durant un període de quatre setmanes (agost/setembre de 2004), recopilant informació sobre 278.858 usuaris i 1.157.112 valoracions corresponents a 271.379 llibres diferents (identificats mitjançant ISBN).

Les valoracions s’expressen en una escala de fins a 10 estrelles, segons el sistema de recomanació del servei \emph{Book-Crossing}.

Dels 278.858 usuaris inicials, només una fracció significativa ha realitzat valoracions:

\begin{itemize}
    \item 99.053 usuaris han valorat almenys un llibre.
    \item 43.385 usuaris han valorat almenys dos llibres.
    \item 12.306 usuaris han valorat almenys deu llibres.
\end{itemize}

Pel que fa als 271.379 llibres presents al conjunt:

\begin{itemize}
    \item 270.171 llibres han estat valorats almenys una vegada.
    \item 124.513 llibres han rebut almenys dues valoracions.
    \item 17.480 llibres han rebut almenys deu valoracions.
\end{itemize}

Degut a l’elevada dispersió del conjunt original, s’ha realitzat un procés de filtratge per obtenir resultats més significatius en l’avaluació d’algorismes de filtratge col·laboratiu. Primer, s’han seleccionat els usuaris que han valorat almenys 10 llibres; a continuació, s’ha mantingut només aquells llibres que han rebut un mínim de 10 valoracions dins aquest subconjunt.

Aquest procés ha generat un conjunt de dades més compacte i manejable, compost per 74.907 valoracions, 6.570 usuaris i 3.411 llibres.

A partir d’una anàlisi exploratòria de les dades (Figura~\ref{fig:analisis_books}), s’observa que aquest conjunt presenta una densitat del 0,33\,\%, la qual cosa indica que només el 0,33\,\% de les valoracions possibles estan registrades.

Les valoracions són predominantment positives, amb una mitjana de 7,87 estrelles i una desviació estàndard d’1,77. A més, com ja es detectava en conjunts com \emph{MovieLens}, tant les valoracions per usuari com per llibre mostren una distribució amb una llarga cua a la dreta: la majoria d’usuaris i llibres tenen poques valoracions, mentre que una petita fracció concentra un nombre elevat de puntuacions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{Figuras/books-ratings.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/books-users.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/books-items.png}
    \caption{Anàlisi exploratòria del conjunt Book-Crossing}
    \label{fig:analisis_books}
\end{figure}

\subsection{Conjunt de dades Jester}

El conjunt de dades Jester \cite{jesterdataset,goldberg2001eigentaste} prové d’un sistema de recomanació d’acudits, en què els usuaris valoren els acudits en una escala contínua de -10 a +10. Tot i el seu caràcter informal, aquest conjunt ha estat àmpliament utilitzat per la seva densitat i per una estructura peculiar que el fa especialment útil en l’àmbit de la recerca en sistemes de recomanació.

El conjunt original conté 100 acudits i 73.421 usuaris, amb un total de 4,1 milions de valoracions recollides entre abril de 1999 i maig de 2003.

Per reduir la mida del conjunt i facilitar l’anàlisi, s’ha aplicat un filtre per conservar únicament 10.000 usuaris seleccionats aleatòriament.

Una de les característiques més destacades d’aquest conjunt és la seva elevada densitat: cada usuari ha valorat un nombre considerable d’ítems, cosa que permet observar amb més claredat el comportament dels mètodes de clustering en entorns amb una gran quantitat d’informació disponible.

A partir d’una anàlisi exploratòria de les dades (Figura~\ref{fig:analisis_jester}), s’observa una densitat del 56,27\,\%, és a dir, s’han registrat més de la meitat de les valoracions possibles. Es tracta d’un valor molt elevat en comparació amb altres conjunts de dades de sistemes de recomanació, que habitualment presenten una cobertura molt més escassa.

Les valoracions mostren una tendència generalment positiva, amb una mitjana de 0,72 i una desviació estàndard de 5,3. Tot i que la majoria d’usuaris només valoren una part dels acudits, s’observa un pic significatiu d’usuaris que han valorat gairebé la totalitat dels acudits disponibles. Pel que fa als acudits, les valoracions es distribueixen de manera relativament uniforme, amb dos pics: alguns acudits han estat valorats per la majoria d’usuaris, mentre que d’altres només han rebut valoracions d’una minoria.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{Figuras/jester-ratings.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/jester-users.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/jester-items.png}
    \caption{Anàlisi exploratòria del conjunt Jester}
    \label{fig:analisis_jester}
\end{figure}

A continuació es presenta una taula resum amb les característiques dels conjunts de dades utilitzats en aquest estudi (Taula~\ref{tab:datasets}).

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{Conjunt} & \textbf{Usuaris} & \textbf{Elements} & \textbf{Valoracions} & \textbf{Densitat (\%)} & \textbf{Escala} \\ \hline
    MovieLens Small & 610   & 9.724 & 100.836 & 1,7 & 0.5-5  \\ \hline
    MovieLens 1M         & 6.040 & 3.706 & 1.000.209 & 4,5 & 1-5   \\ \hline
    Book-Crossing        & 6.570 & 3.411 & 74.907   & 0,33 & 1-10 \\ \hline
    Jester               & 10.000& 100   & 4.134.000& 56,27 & -10-10 \\ \hline
    \end{tabular}
    \caption{Característiques dels conjunts de dades utilitzats}
    \label{tab:datasets}
\end{table}

\section{Conjunts de dades sintètiques}

El nombre i la varietat de conjunts de dades de valoracions disponibles públicament són sovint limitats, especialment en àmbits menys convencionals.

Les empreses que poden recopilar conjunts de dades de valoracions solen ser reticents a compartir-los, per por de vulnerar la privadesa dels seus usuaris o d’exposar informació comercialment sensible als seus competidors.

A causa d’aquesta escassetat de dades públiques, els professionals han començat a dependre de valoracions sintètiques.

Tanmateix, els resultats obtinguts d’aquests experiments poden ser qüestionables, ja que els conjunts de dades generats sovint no són capaços de captar adequadament les característiques pròpies d’un domini d’interès concret.

En aquest estudi, ens proposem analitzar la capacitat de generalització dels mètodes de clustering en situacions especialment extremes pel que fa a la densitat de les valoracions.

Per aconseguir aquests conjunts de dades peculiars, utilitzem un conjunt de dades de referència com a punt de partida, el qual codifica les particularitats d’un domini específic. Aquest mètode generatiu permet crear diversos conjunts de dades de valoracions amb usuaris que mostren comportaments similars als dels usuaris del conjunt de dades original. Tanmateix, aquests usuaris sintètics no tenen cap relació directa amb persones reals, de manera que no es compromet cap informació privada ni comercialment sensible.

Sostenim que confiar únicament en unes poques distribucions estadístiques calculades empíricament a partir d’un conjunt de dades existent, o definides per un investigador, no és suficient per simular de manera realista els gustos individuals dels éssers humans. Mètodes d’aquest tipus generen conjunts de dades amb usuaris sense preferències pròpies, fet que dificulta enormement el funcionament efectiu de qualsevol sistema de recomanació.

Així doncs, utilitzarem un enfoc similar al  de l'artice \cite{monti2019all} amb un enfocament basat en clustering per generar conjunts de dades sintètiques a partir de conjunts de dades reals, amb l’objectiu d’obtenir conjunts de dades.

Aquesta generació de conjunts sintètics es basa en un procés en dues fases: una fase d'anàlisi del conjunt de referència i una fase de mostreig de valoracions.

\textbf{Fase 1: Anàlisi del conjunt de dades}


En primer lloc, s’aplica una anàlisi detallada del conjunt de dades de referència per obtenir una representació fidel del domini. L’element clau d’aquesta fase és la identificació de comunitats d’usuaris mitjançant el mètode de clustering K-means. Per capturar millor la variabilitat de les valoracions, en lloc d’utilitzar una representació binària (valoració positiva o negativa), s’ha optat per una aproximació que considera cada valor de puntuació per separat. Això vol dir que el procés es repeteix per a cada possible valor de valoració (per exemple, de 1 a 5), generant representacions específiques per a cada cas.

Per a cada valor de valoració, es representa cada usuari com un vector binari d'interaccions amb els ítems: un 1 indica que l’usuari ha assignat aquell valor concret a l’ítem, i un 0 indica absència d’aquesta valoració. Aquesta representació permet aplicar K-means per agrupar usuaris amb patrons similars de puntuació en cada nivell.

A partir d’aquesta agrupació, es construeixen tres distribucions empíriques per a cada valor de valoració:

\begin{itemize}
    \item $\mathbf{P_C^r}$: Distribució de probabilitat que un usuari pertanyi a un determinat clúster per a la valoració $r$.
    \item $\mathbf{P_U^{k,r}}$: Distribució del nombre de valoracions amb valor $r$ per usuari dins del clúster $k$.
    \item $\mathbf{P_I^{k,r}}$: Distribució de probabilitat d’interaccions amb valoració $r$ per ítem dins del clúster $k$.
\end{itemize}

Aquestes distribucions es construeixen comptant les freqüències observades dins cada comunitat i per cada valor de valoració, reflectint així el comportament col·lectiu detallat dels usuaris.

\medskip

\textbf{Fase 2: Generació de valoracions}

Un cop obtingudes les distribucions, es procedeix a generar el nou conjunt de dades mitjançant un procés de mostreig estocàstic, repetit per a cada valor de valoració $r$.

Per cada valor $r$ del conjunt de puntuacions:
\begin{enumerate}
    \item Per cada usuari sintètic, se l’assigna a un clúster $k$ segons la distribució $\mathbf{P_C^r}$.
    \item Es determina el nombre de valoracions amb valor $r$ mitjançant un mostreig de $\mathbf{P_U^{k,r}}$.
    \item Es seleccionen ítems a valorar amb puntuació $r$, mitjançant un mostreig sense reemplaçament de la distribució $\mathbf{P_I^{k,r}}$.
\end{enumerate}

Després de repetir aquest procés per a tots els valors possibles, es fusionen les valoracions generades per construir el conjunt de dades final, que conté valoracions multinivell i reflecteix una estructura rica i fidel al conjunt original.

Aquesta metodologia assegura que els conjunts sintètics reflecteixin una estructura latent realista, en què els usuaris tenen gustos diferenciats i les seves interaccions segueixen patrons similars als del conjunt de referència. Això fa que els conjunts generats siguin adequats per a l’avaluació comparativa d’algorismes de clustering i sistemes de recomanació.

Un cop definit i validat el mètode de generació de conjunts de dades sintètiques, el següent pas és escollir el conjunt de dades de referència a partir del qual es generarà. En aquest estudi, s’ha seleccionat el conjunt MovieLens Latest Small, un dels més utilitzats en la recerca sobre sistemes de recomanació, conegut per la seva estructura rica i diversa. A partir d’aquest conjunt, es generaran tres conjunts de dades sintètiques. El primer actuarà com a conjunt base o de control, reproduint de manera fidel les característiques del conjunt original. En canvi, en el segon i tercer conjunts es modificarà manualment la distribució $\mathbf{P_U^{k,r}}$ per tal de crear situacions amb densitats de valoració extremadament baixes i altes, respectivament. Aquest disseny experimental permet avaluar la robustesa i la capacitat de generalització dels mètodes de clustering en escenaris amb densitats diverses.

\subsection{MovieLens sintètic (base)}

El conjunt de dades MovieLens sintètic (base) es genera a partir del conjunt MovieLens Latest Small, amb l’objectiu de reproduir les mateixes característiques i patrons de valoració que el conjunt original. Això permet establir un punt de referència sòlid per a la comparació amb altres conjunts generats.

A partir de l’anàlisi exploratòria del conjunt MovieLens sintètic (base) (Figura~\ref{fig:analisis_ml_synthetic_base}), s’observa que aquest presenta una densitat del 2,38\,\%, similar a la del conjunt original.

La corba de distribució de les valoracions mostra una tendència comparable a la del conjunt original, tot i que amb una distribució més uniforme. La mitjana de les valoracions és de 3,3 estrelles, amb una desviació estàndard d’1,16.

A més, la distribució de valoracions per usuari i per pel·lícula és també semblant a la del conjunt original, tot i que amb una pèrdua notable de la seva llarga cua característica.

En resum, degut a la naturalesa estocàstica del procés de generació, el conjunt sintètic manté l’estructura general del conjunt original, però amb una variabilitat lleugerament més baixa.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.49\textwidth]{Figuras/syn-ratings.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/syn-users.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/syn-items.png}
    \caption{Anàlisi exploratòria del conjunt MovieLens sintètic (base)}
    \label{fig:analisis_ml_synthetic_base}
\end{figure}

\subsection{MovieLens sintètic (baixa densitat)}

Per generar aquest conjunt, s'ha aplicat una reducció del 98\,\% a la distribució $\mathbf{P_U^{k,r}}$ amb l'objectiu de disminuir la quantitat de valoracions per usuari.
Aquesta reducció dràstica provoca una disminució significativa de la densitat del conjunt, que passa a ser del 0,29\,\%.

En l'anàlisi (Figura~\ref{fig:analisis_ml_synthetic_low_dens}), s'observen propietats similars, amb una reducció evident en la quantitat de valoracions per usuari i per pel·lícula. La mitjana de les valoracions és de 3,4 estrelles, amb una desviació estàndard d'0,92.
Es pot observar una clara diferenciació entre la distribució de les valoracions en valoracions majors que 3 estrelles i les inferiors.
També s'observa una reducció en el nombre de valoracions per pel·lícula i per usuari.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.49\textwidth]{Figuras/syn03-ratings.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/syn03-users.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/syn03-items.png}
    \caption{Anàlisi exploratòria del conjunt MovieLens sintètic (baixa densitat)}
    \label{fig:analisis_ml_synthetic_low_dens}
\end{figure}

\subsection{MovieLens sintètic (alta densitat)}

En generar aquest conjunt, s'ha assolit una densitat del 83,9\,\%, fet que representa un increment significatiu respecte al conjunt original.

En l'anàlisi (Figura~\ref{fig:analisis_ml_synthetic_high_dens}), es poden observar canvis notables en la distribució de les valoracions. Aquest fenomen es deu al fet que s'ha forçat que cada usuari sintètic valori un gran nombre d'ítems, incloent-hi molts que probablement no li agradin. Això contradiu la naturalesa del conjunt original, en què els usuaris tendeixen a valorar només aquells ítems que els resulten d'interès.

A conseqüència d'això, la mitjana de les valoracions se situa en 2,39 estrelles, amb una desviació estàndard d'1,21. A més, la distribució de les valoracions per usuari i per pel·lícula s'inverteix respecte a les anteriors, mostrant una gran concentració de valoracions per cada usuari i per cada pel·lícula.
\begin{figure} [H]
    \centering
    \includegraphics[width=0.49\textwidth]{Figuras/syn80-ratings.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/syn80-users.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figuras/syn80-items.png}
    \caption{Anàlisi exploratòria del conjunt MovieLens sintètic (alta densitat)}
    \label{fig:analisis_ml_synthetic_high_dens}
\end{figure}

A continuació es presenta una taula resum amb les característiques dels conjunts de dades sintètiques generats (Taula~\ref{tab:datasets_synthetic}).

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{Conjunt} & \textbf{Usuaris} & \textbf{Elements} & \textbf{Valoracions} & \textbf{Densitat (\%)} & \textbf{Escala} \\ \hline
    Sintètic (base) & 610   & 8.481 & 123.033 & 2,38 & 0.5-5  \\ \hline
    Sintètic (baixa densitat) & 432   & 1.060 & 1.319  & 0,29 & 0.5-5 \\ \hline
    Sintètic (alta densitat)        & 610   & 9.694 & 4.963.951 & 83,9 & 0.5-5 \\ \hline
    \end{tabular}
    \caption{Característiques dels conjunts de dades sintètiques generats}
    \label{tab:datasets_synthetic}
\end{table}

Encara que s’utilitzin dades sintètiques, es donarà més importància a les que provenen d’interaccions reals d’usuaris. Tot i així, aquestes dades s’empraran per analitzar el comportament de les implementacions en situacions anòmales.

\chapter{Sistema de recomanació}

\textbf{Arquitectura comuna del sistema de recomanació}

Aquest capítol descriu el sistema de recomanació implementat per a l’avaluació dels diferents mètodes de clustering considerats en aquest estudi. Amb l’objectiu de garantir una comparació justa entre les diferents variants, s’ha establert una arquitectura comuna que comparteix tant la funció de similitud com la metodologia de predicció. Això permet que qualsevol diferència en el rendiment dels sistemes sigui atribuïble únicament a la tècnica de clustering emprada.

Per mesurar la similitud entre usuaris, s’ha optat per la correlació de Pearson. Aquesta és una mesura de similitud fonamental en els sistemes de recomanació i compta amb una àmplia literatura que en recolza l’eficàcia \cite{chowdhury2024evaluating}.

Aquesta mesura és àmpliament utilitzada en sistemes de recomanació basats en el filtratge col·laboratiu, ja que captura la correlació lineal entre els patrons de valoració dels usuaris, tot tenint en compte les diferències en les seves escales personals de valoració. Donats dos usuaris \( u \) i \( v \), i el conjunt d’elements que ambdós han valorat \( I_{uv} \), la correlació de Pearson s’expressa de la forma següent:

\[
\text{sim}(u,v) = \frac{\sum_{i \in I_{uv}} (r_{u,i} - \bar{r}_u)(r_{v,i} - \bar{r}_v)}{\sqrt{\sum_{i \in I_{uv}} (r_{u,i} - \bar{r}_u)^2} \cdot \sqrt{\sum_{i \in I_{uv}} (r_{v,i} - \bar{r}_v)^2}}
\]

on \( r_{u,i} \) és la valoració de l’usuari \( u \) sobre l’element \( i \), i \( \bar{r}_u \) és la mitjana de les valoracions de l’usuari \( u \). Aquesta mesura ofereix una normalització implícita respecte a les mitjanes individuals i afavoreix els casos on els usuaris valoren de forma coherent, encara que utilitzin escales diferents.

La correlació de Pearson oscil·la típicament entre -1 i +1, on -1 indica una correlació negativa forta, +1 una correlació positiva forta, i 0 implica absència d’associació. Visualment, una correlació positiva es manifesta quan els valors augmenten conjuntament, mentre que una correlació negativa apareix quan disminueixen conjuntament.

Per a la predicció de valoracions, s’utilitza la tècnica coneguda com a predicció centrada en la mitjana o basada en la desviació respecte a la mitjana. Aquesta metodologia es basa en ajustar la predicció a partir de la mitjana de l’usuari i les desviacions observades en altres usuaris similars. La predicció \( \hat{r}_{u,i} \) que fa l’usuari \( u \) sobre l’element \( i \) es calcula com:

\[
\hat{r}_{u,i} = \bar{r}_u + \frac{\sum_{v \in N(u)} \text{sim}(u,v) \cdot (r_{v,i} - \bar{r}_v)}{\sum_{v \in N(u)} |\text{sim}(u,v)|}
\]

on \( N(u) \) és el conjunt d’usuaris més similars a \( u \) que han valorat l’element \( i \), i \( \text{sim}(u,v) \) és la similitud entre els usuaris \( u \) i \( v \), segons la correlació de Pearson. De manera similar a la mesura de similitud, la predicció centrada en la mitjana també té en compte les escales personals de valoració, cosa que permet obtenir estimacions més precises i comparables entre usuaris.

Segons l’anàlisi empírica presentada a \cite{herlocker2002empirical}, la formulació basada en la desviació respecte a la mitjana resulta ser una de les estratègies de predicció no personalitzada més precises, superant altres enfocaments habituals. Això reforça l’ús d’aquest model com a base de comparació robusta en aquest estudi.

L’ús conjunt de la similitud de Pearson i de la predicció centrada en la mitjana com a base comuna respon a diversos motius. En primer lloc, són tècniques consolidades i àmpliament adoptades en la literatura sobre sistemes de recomanació. A més, proporcionen una base robusta per analitzar l’impacte específic del clustering en el rendiment del sistema, i faciliten la comparació directa entre mètodes, evitant que les diferències puguin atribuir-se a variacions en la funció de similitud o en la fórmula de predicció.

Per dur a terme el procés de clustering és imprescindible definir prèviament una funció de distància que mesuri quant “allunyats” estan dos punts (en el nostre cas, dos usuaris) dins l’espai de característiques. Aquesta funció de distància actua com a nucli de qualsevol mètode de partició ja que depenen íntegrament de la mètrica que comparem.

Per seguir garantint que qualsevol variació en el rendiment obtingut es degui només a la mecànica pròpia de l’algorisme de clustering hem optat per experimentar de forma consistent les mateixes dues mesures de distància en tots els procediments d’agrupament. D’aquesta manera, la comparació entre tècniques és estrictament “com a resultat de l’estructura” que el clustering imposa, i no d’algun component extern a l’algorisme.

La primera mètrica de distància que emprem és la distància euclidiana, probablement la més clàssica i intuïtiva en l’anàlisi de dades. Donats dos vectors de característiques $x = (x_1, x_2, \ldots, x_n)$ i $y = (y_1, y_2, \ldots, y_n)$, la seva distància euclidiana ve donada per:

\[
d_{Euc}(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
\]

Aquesta fórmula mesura directament la separació geomètrica entre punts en un espai $n$-dimensional, ponderant igualment totes les dimensions.

Els seus punts forts són la seva simplicitat i la seva interpretabilitat, que reflecteix clarament la magnitud total del desplaçament.

Per incorporar la noció de “coherència de patró” en les valoracions, utilitzarem a més una distància basada en la similitud de Pearson. 
Aquesta segona mètrica permet agrupar usuaris que, tot i fer servir escales de puntuació diferents, mostren un comportament sincronitzat (pujades i baixades conjuntes). 
Hipotetitzem que aquesta donarà millors resultats, ja que esta relacionada amb la funció de predicció utilitzada. 

En una primera instancia es va implementar la mesura de distància proposada a \cite{zahra2015novel}:

\[
d_{Pearson}(x,y) = \frac{1}{1 + \text{sim}(x,y)}
\]

Tanmateix, es va acabar descartant aquesta mesura perquè no compleix les propietats d’una mètrica de distància (identitat, simetria i desigualtat triangular).
Tot i que es va considerar utilitzar la distància angular, finalment es va optar per implementar la transformació:

\[
d_{Pearson}(x,y) = \frac{1 - \text{sim}(x,y)}{2}
\]

Ja que és més senzilla de calcular i compleix les propietats requerides d’una mètrica

\vspace{0.5cm}
\textbf{Algorismes de clustering emprats}

A grans trets, els algorismes de clustering més comuns, i que tractarem en aquest treball, es poden agrupar en cinc famílies principals segons com entenen i construeixen els grups. \cite{ezugwu2022comprehensive}

En primer lloc, tenim el clustering dur, que divideix el conjunt de dades en $k$ grups mútuament excloents. Entre els algorismes més coneguts d’aquesta família hi ha el K-means, K-means++, Mini-Batch K-means i K-medoids, entre d'altres. En aquest treball ens centrarem en el K-means, ja que, a més de ser àmpliament utilitzat en entorns pràctics, destaca per la seva simplicitat conceptual i eficiència computacional, la qual cosa el converteix en un bon punt de partida per introduir el clustering dur.

A continuació, trobem el clustering difús, que permet que un mateix element pugui pertànyer a més d’un clúster amb un grau de pertinença associat. Els algorismes més representatius d’aquesta categoria són el Fuzzy C-means (FCM), Possibilistic C-means (PCM) i Gustafson–Kessel (GK). En aquest treball ens centrarem en el FCM, atès que és àmpliament emprat en problemes on es busca gestionar la incertesa o l’ambigüitat en la classificació, com ara en el reconeixement de patrons.

El tercer tipus és el clustering jeràrquic, que construeix una jerarquia d’agrupacions mitjançant un arbre o dendrograma. Els mètodes més coneguts d’aquesta família són els algorismes aglomeratius i divisoris. En aquest treball ens centrarem en l’aglomeratiu, ja que ofereix una major flexibilitat en l'elecció de la mesura de distància i permet visualitzar fàcilment l’estructura de les dades, cosa que el fa especialment útil per a anàlisis exploratòries.

El quart enfocament és el clustering basat en densitat, que identifica agrupacions com a regions de major densitat separades per àrees de baixa densitat. Els algorismes més coneguts d’aquesta família són DBSCAN, OPTICS i HDBSCAN. En aquest treball posarem l’accent en DBSCAN, perquè destaca per la seva capacitat per detectar formes arbitràries de clústers i gestionar bé el soroll, cosa que el fa adequat per a dades reals amb estructures no lineals.

Finalment, trobem el clustering basat en models, que assumeix que les dades provenen d’una combinació de distribucions estadístiques subjacents. Els algorismes més habituals en aquesta categoria són el Gaussian Mixture Model (GMM) i el Dirichlet Process Mixture Model (DPMM). En aquest treball analitzarem el GMM, ja que ofereix una aproximació probabilística robusta i flexible que permet modelar clústers amb formes el·líptiques i proporciona una estimació explícita de la probabilitat de pertinença de cada punt.

A continuació es presenta una taula resum amb els algorismes de clustering analitzats (Taula~\ref{tab:algorithms}).

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Algorisme} & \textbf{Tipus} & \textbf{Complexitat} \\ \hline
    K-means & Dur  & $O(n)$ \\ \hline
    Fuzzy C-means & Difús  & $O(n)$ \\ \hline
    Aglomeratiu & Jeràrquic  & $O(n^2)$ \\ \hline
    DBSCAN & Densitat  & $O(n \cdot \log n)$ \\ \hline
    GMM & Model  & $O(n)$ \\ \hline
    \end{tabular}
    \caption{Algorismes de clustering analitzats}
    \label{tab:algorithms}
\end{table}
 
\section{Clustering dur}

Tal com es fa en altres treballs \cite{dakhel2011new}, els usuaris s’agrupen exclusivament en un únic clúster mitjançant l’algorisme K-means. Aquesta tècnica de partició busca dividir l’espai d’usuaris en $K$ grups disjunts, de manera que cada usuari pertanyi al clúster amb el centre (centroid) més proper.

L’algorisme K-means funciona de la manera següent:

\begin{enumerate}
    \item \textbf{Inicialització:} Es trien $K$ punts inicials com a centroids, habitualment de manera aleatòria o mitjançant una estratègia com K-means++ per millorar la convergència.
    \item \textbf{Assignació d’usuaris:} Cada usuari, representat com un vector de característiques (per exemple, el vector de valoracions normalitzat), s’assigna al clúster amb el centroid més proper, mesurat normalment amb la distància euclidiana.
    \item \textbf{Reajustament de centroids:} Per a cada clúster, es recalcula el centroid com la mitjana de tots els vectors d’usuaris assignats a aquell clúster.
    \item \textbf{Iteració:} Els passos d’assignació i reajustament es repeteixen fins que els centroids deixen de moure’s de manera significativa (convergeixen) o s’arriba a un nombre màxim d’iteracions.
\end{enumerate}

Aquest procediment assegura que la suma dels quadrats de les distàncies internes als clústers (inertia) es minimitza localment, tot i que K-means no garanteix trobar el mínim global.

Un cop determinats els clústers, es calcula la predicció de les valoracions considerant només els veïns dins del mateix clúster que han valorat l’element corresponent.

Per a la implementació amb distància euclidiana s'ha fet ús de la biblioteca Python \texttt{scikit-learn} \cite{pedregosa2011scikit}, que ofereix la classe \texttt{KMeans} només amb aquesta mesura. Com que no permet canviar la funció de distància, s'ha desenvolupat una implementació pròpia de l'algorisme K-means que utilitza la distància de Pearson definida prèviament. Aquesta versió s'ha optimitzat amb la biblioteca \texttt{numba} per a una major eficiència i ha estat paral·lelitzada per accelerar els càlculs.


\section{Clustering difús}

S’ha descartat la implementació proposada per \cite{treerattanapitak2009items}, que feia ús del grau de pertinença i dels centroids per calcular les prediccions. Tot i que aquesta proposta aprofita millor les propietats del clustering difús, contradiu la nostra premissa de mantenir una fórmula de predicció comuna entre totes les variants. A més, en una anàlisi preliminar s’ha detectat un increment significatiu del temps de còmput i un empitjorament considerable dels resultats en comparació amb altres alternatives.

Seguint la proposta de \cite{koohi2016user}, s’ha implementat un mètode de clustering difús per a la creació de clústers d’usuaris. Aquest mètode permet que cada usuari pugui pertànyer a més d’un clúster, amb un grau de pertinença que es determina mitjançant la distància entre l’usuari i els centroids dels clústers.

L’algorisme utilitzat és el Fuzzy C-Means (FCM), una extensió del K-means que introdueix la possibilitat que un mateix usuari tingui diversos graus de pertinença a diferents clústers, en lloc de ser assignat exclusivament a un sol grup. Aquesta característica reflecteix millor la naturalesa difusa de les preferències dels usuaris en sistemes de recomanació, on sovint comparteixen interessos amb múltiples comunitats.

El funcionament de l’algorisme FCM es pot descriure amb els passos següents:

\begin{enumerate}
    \item \textbf{Inicialització:} Es tria un nombre de clústers \( K \) i un paràmetre de difusió \( m > 1 \) (habitualment \( m = 2 \)), que controla el grau de “fuzziness”. També s’inicialitza la matriu de pertinences \( U \), on \( u_{ij} \) representa el grau de pertinença de l’usuari \( i \) al clúster \( j \), amb valors entre 0 i 1 i la restricció que \( \sum_{j=1}^{K} u_{ij} = 1 \) per a tot \( i \).
    
    \item \textbf{Càlcul dels centroids:} Per a cada clúster \( j \), es calcula el centroid \( c_j \) com una mitjana ponderada dels vectors d’usuaris, utilitzant els graus de pertinença elevats a la potència \( m \):
    \[
    c_j = \frac{\sum_{i=1}^{N} u_{ij}^m \cdot x_i}{\sum_{i=1}^{N} u_{ij}^m}
    \]
    on \( x_i \) és el vector de característiques de l’usuari \( i \).

    \item \textbf{Actualització dels graus de pertinença:} Es recalculen els valors de \( u_{ij} \) a partir de la distància entre l’usuari \( i \) i cada centroid \( c_j \):
    \[
    u_{ij} = \left( \sum_{k=1}^{K} \left( \frac{||x_i - c_j||}{||x_i - c_k||} \right)^{\frac{2}{m-1}} \right)^{-1}
    \]
    Aquest pas assegura que els usuaris més propers a un centroid tinguin un grau de pertinença més elevat.

    \item \textbf{Iteració:} Es repeteixen els passos de càlcul de centroids i actualització de pertinences fins que es compleixi un criteri de convergència, com ara un canvi mínim en \( U \) o un nombre màxim d’iteracions.
\end{enumerate}

Seguint la proposta adoptada, una vegada obtinguts els graus de pertinença dels usuaris a cada clúster, per obtenir una assignació final i discreta de cada usuari a un únic clúster, s’aplica la tècnica del \textit{Centre de Gravetat (Center of Gravity)} sobre els graus de pertinença:

\[
\text{COG}_i = \sum_{j=1}^{K} u_{ij} \cdot j, \quad \text{cluster}_i = \text{round}(\text{COG}_i)
\]

Això permet capturar la distribució global de la pertinença de l’usuari i assignar-lo al clúster més representatiu segons la seva posició mitjana entre els clústers.

Un cop determinada l’assignació, es calculen les prediccions de valoracions considerant només els veïns del mateix clúster.

Per a la implementació de l’algorisme Fuzzy C-Means s'ha fet ús de la biblioteca Python \texttt{scikit-fuzzy} (\texttt{skfuzzy}). Com que aquesta implementació no permet definir una funció de distància pròpia, s'ha modificat manualment la llibreria per afegir l'opció d'utilitzar la distància de Pearson en el càlcul dels graus de pertinença i dels centroides. Aquestes adaptacions s'han integrat de manera nadiua en les funcions \texttt{cmeans} i \texttt{cmeans\_predict} de \texttt{skfuzzy}, afegint un nou paràmetre \texttt{metric='pearson'}.

\section{Clustering jeràrquic}

El clustering jeràrquic és una tècnica d’agrupament que construeix una jerarquia de clústers, en lloc de generar una partició plana com fan algorismes com K-means o Fuzzy C-means.

Seguint altres treballs \cite{chalco2019hierarchical} i inspirant-nos en una proposta similar, en aquesta secció es descriu la implementació de l’algorisme d’Agrupament Jeràrquic Aglomeratiu (HAC), utilitzant el mètode d’enllaç promig (\textit{average linkage}).

L’algorisme HAC parteix del supòsit que cada usuari constitueix un clúster individual. A cada iteració, es fusionen els dos clústers amb major similitud.

El mètode de fusió utilitzat ha estat l’\textit{average linkage}, o Mètode No Ponderat de Grups amb Mitjanes Aritmètiques (UPGMA, per les seves sigles en anglès). Aquest enfocament calcula la distància entre dos clústers com la mitjana de les distàncies entre totes les parelles d’usuaris que els componen:

\[
d(A, B) = \frac{1}{|A| \cdot |B|} \sum_{u \in A} \sum_{v \in B} d(u, v)
\]

Aquest mètode presenta avantatges respecte a altres com l’enllaç simple o complet, ja que produeix clústers amb formes més equilibrades i és menys sensible als valors atípics.

Així, en cada pas del procés d’agrupament aglomeratiu:
\begin{enumerate}
    \item S’identifica la parella de clústers amb la distància \(d\) mínima.
    \item Es fusionen en un nou clúster i es recalculen les distàncies amb la resta segons la fórmula anterior.
    \item El procés es repeteix fins a obtenir el nombre desitjat de clústers \(K\).
\end{enumerate}

Un cop determinada l’assignació de clústers, les prediccions de valoracions es calculen considerant exclusivament els veïns que pertanyen al mateix clúster.

Per a la implementació de l’algorisme d’Agrupament Jeràrquic Aglomeratiu (HAC) s'ha fet ús de les funcions \texttt{linkage} i \texttt{fcluster} del mòdul \texttt{scipy.cluster.hierarchy}. Abans d’invocar \texttt{linkage}, les mesures de distància entre usuaris es precalculen en funció de si s’ha utilitzat la distància euclidiana o la distància de Pearson, i es transmeten a la funció mitjançant el paràmetre \texttt{metric='precomputed'}. Això permet aplicar el mateix procés de clustering sense dependre de la distància interna per defecte.

\section{Clustering per densitat}

Per a l’agrupament basat en densitat s’ha utilitzat l’algorisme DBSCAN (\emph{Density-Based Spatial Clustering of Applications with Noise}), com també s’ha fet en altres treballs \cite{satsiou2017hybrid}.

El funcionament de DBSCAN es basa en dos paràmetres principals:
\begin{itemize}
    \item \textbf{\( \varepsilon \)}: radi del veïnat considerat per avaluar la densitat al voltant d’un punt.
    \item \textbf{MinPts}: nombre mínim de punts que han d’existir dins del radi \( \varepsilon \) perquè un punt pugui iniciar la formació d’un grup.
\end{itemize}

El funcionament de l’algorisme DBSCAN es pot resumir en els següents passos:

\begin{enumerate}
    \item \textbf{Inicialització:} Es fixen els valors dels paràmetres \(\varepsilon\) (distància màxima per considerar dos punts com a veïns) i \(\mathit{MinPts}\) (nombre mínim de veïns per formar un grup).
    
    \item \textbf{Avaluació dels punts:} Per a cada usuari, es compta quants altres usuaris es troben dins la distància \(\varepsilon\).
    \begin{itemize}
        \item Si un usuari té com a mínim \(\mathit{MinPts}\) veïns, es considera apte per iniciar un grup i unir-hi els punts propers.
        \item Si en té menys, només podrà afegir-se a un grup si es troba dins del veïnat d’un altre usuari que sí compleixi el criteri anterior.
        \item Els usuaris que no compleixen cap dels dos casos anteriors no s’inclouen en cap grup.
    \end{itemize}
    
    \item \textbf{Formació dels grups:} Per a cada usuari que pot formar grup:
    \begin{enumerate}
        \item Es crea un nou grup i s’hi afegeix aquest usuari.
        \item Es continua afegint al grup qualsevol usuari proper que compleixi els criteris de densitat, així com els seus veïns, de manera recursiva.
    \end{enumerate}
    
    \item \textbf{Finalització:} Quan tots els grups han estat formats, l’algorisme conclou. Els usuaris que no han estat assignats a cap grup en formen un de nou.
\end{enumerate}

Un cop formats els grups, la predicció de valoracions es calcula seguint la mateixa lògica que en les tècniques anteriors: per a cada parella usuari–ítem, només es tenen en compte les valoracions d’altres usuaris que formen part del mateix grup.

Per a l’agrupament basat en densitat s'ha fet ús de la classe \texttt{DBSCAN} de la biblioteca \texttt{scikit-learn} \cite{pedregosa2011scikit}. Igual que en el cas jeràrquic, les distàncies entre usuaris es precalculen prèviament (euclidiana o Pearson) i es passen a l’algorisme mitjançant el paràmetre \texttt{metric='precomputed'}. D’aquesta manera es manté consistència en la mesura de similitud utilitzada.


\section{Clustering per model}

Per al clustering per model s’ha escollit l’ús del \emph{Gaussian Mixture Model} (GMM), una tècnica probabilística que modela la distribució dels usuaris com una combinació de $K$ distribucions gaussiànes. A diferència de K-means, que realitza una partició dura, GMM permet un modelatge de pertinença suau, obtenint per a cada usuari responsabilitats (\emph{responsibilities}) associades a cada component gaussià. Aquesta aproximació ja ha estat utilitzada en sistemes de recomanació col·laboratius, com es mostra a \cite{yan2019collaborative}.

L’algorisme GMM mitjançant l’EM (\emph{Expectation–Maximization}) es desenvolupa de la següent manera:

\begin{enumerate}
    \item \textbf{Inicialització:} Es defineix el nombre de components $K$ i s’inicialitzen els paràmetres del model:
    \[
    \Theta^{(0)} = \{\pi_j^{(0)}, \mu_j^{(0)}, \Sigma_j^{(0)}\}_{j=1}^K,
    \]
    on $\pi_j$ són els pesos de barreja (satisfent $\sum_j \pi_j = 1$), $\mu_j$ els vectors de mitjana i $\Sigma_j$ les matrius de covariància diagonals de cada component.

    \item \textbf{E–step (Expectació):} Per a cada usuari $i$ (representat pel vector de característiques $x_i$), es calculen les responsabilitats $\gamma_{ij}$, és a dir, la probabilitat de pertinença de $x_i$ al component $j$:
    \[
    \gamma_{ij} = \frac{\pi_j \,\mathcal{N}(x_i \mid \mu_j, \Sigma_j)}{\sum_{l=1}^K \pi_l \,\mathcal{N}(x_i \mid \mu_l, \Sigma_l)},
    \]
    on $\mathcal{N}(x \mid \mu, \Sigma)$ és la densitat de la distribució normal multivariante.

    \item \textbf{M–step (Maximització):} Es reestimen els paràmetres del model utilizant les responsabilitats calculades:
    \[
    N_j = \sum_{i=1}^N \gamma_{ij},
    \]
    \[
    \pi_j^{\text{new}} = \frac{N_j}{N}, \quad
    \mu_j^{\text{new}} = \frac{1}{N_j} \sum_{i=1}^N \gamma_{ij}\, x_i, \quad
    \Sigma_j^{\text{new}} = \frac{1}{N_j} \sum_{i=1}^N \gamma_{ij}\, (x_i - \mu_j^{\text{new}})(x_i - \mu_j^{\text{new}})^\top.
    \]

    \item \textbf{Iteració:} Els passos d’\emph{E–step} i \emph{M–step} es repeteixen fins que la variació en el log‑vèrtex o en els paràmetres sigui inferior a un llindar predeterminat, o s’arribi al nombre màxim d’iteracions.
\end{enumerate}

Un cop convergit el model, es pot assignar cada usuari $i$ al clúster $\hat{j}$ per al qual $\gamma_{i\hat{j}}$ és màxima (assignació dura) o bé mantenir les responsabilitats per a un càlcul de predicció difús. En la nostra implementació hem optat per l’assignació dura:

\[
\hat{j}_i = \arg\max_{j} \;\gamma_{ij}.
\]

Finalment, la predicció de la valoració de l’usuari $u$ per l’ítem $v$ es calcula considerant exclusivament les valoracions aportades pels usuaris que han estat assignats al mateix component gaussià que $u$.

Per a la implementació del \emph{Gaussian Mixture Model} s'ha fet ús de la classe \texttt{GaussianMixture} del mòdul \texttt{sklearn.mixture} de la biblioteca \texttt{scikit-learn} \cite{pedregosa2011scikit}.

\chapter{Anàlisi de resultats}
\label{chap:analisi_resultats}

En aquest capítol es presenten i s'analitzen els resultats obtinguts en l'avaluació dels diferents mètodes de clustering aplicats al sistema de recomanació. Per tal d'assegurar la fiabilitat i generalitzabilitat de les conclusions, s'ha emprat una metodologia de validació creuada (cross-validation). Concretament, s'ha aplicat una validació creuada de 5 particions (5-fold cross-validation), on el conjunt de dades es divideix en 5 subconjunts o \textit{folds}. En cada iteració, un dels \textit{folds} s'utilitza com a conjunt de prova, mentre que els 4 restants s'empren com a conjunt d'entrenament. Aquest procés es repeteix 5 vegades, de manera que cada \textit{fold} actua com a conjunt de prova exactament una vegada. Les mètriques de rendiment obtingudes en cada una de les 5 iteracions es promedian per obtenir una estimació més robusta i menys susceptible a la particularitat d'una única partició de les dades.

Per avaluar el rendiment i les característiques dels clústers formats, s'empraran diverses mètriques quantitatives, els valors de les quals seran, per tant, la mitjana obtinguda després del procés de validació creuada.

Primerament, per avaluar la precisió de les prediccions de valoracions generades pel sistema, s'utilitzaran l'Error Absolut Mitjà Normalitzat (NMAE) i l'Error Quadràtic Mitjà Normalitzat (NRMSE). Aquestes mètriques es calculen de la següent manera:
\[
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} | \hat{r}_{u,i} - r_{u,i} |
\]
\[
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\hat{r}_{u,i} - r_{u,i})^2}
\]
on \(N\) és el nombre total de valoracions predites en un \textit{fold} de prova, \( \hat{r}_{u,i} \) és la valoració predita per a l'usuari \(u\) sobre l'ítem \(i\), i \( r_{u,i} \) és la valoració real.
Per obtenir les versions normalitzades, NMAE i NRMSE, aquests valors es divideixen pel rang de les valoracions presents al conjunt de dades (\( \text{rating}_{\text{max}} - \text{rating}_{\text{min}} \)):
\[
\text{NMAE} = \frac{\text{MAE}}{\text{rating}_{\text{max}} - \text{rating}_{\text{min}}}
\]
\[
\text{NRMSE} = \frac{\text{RMSE}}{\text{rating}_{\text{max}} - \text{rating}_{\text{min}}}
\]
La normalització d'aquestes mètriques d'error és particularment útil en aquest estudi, ja que permet comparar de manera més equitativa el rendiment dels algorismes a través de diferents conjunts de dades que puguin tenir escales de valoració distintes. Valors més baixos de la mitjana d'NMAE i NRMSE obtinguts a través de la validació creuada indiquen una major precisió generalitzada en les prediccions.

En segon lloc, per avaluar la qualitat de la distribució dels usuaris entre els clústers formats, s'utilitzarà l'entropia normalitzada dels clústers. Aquesta mètrica mesura el grau de desequilibri en l'assignació dels usuaris als diferents grups. Una situació ideal és aquella on els usuaris es distribueixen de manera relativament uniforme entre els clústers disponibles, evitant que la majoria dels usuaris siguin assignats a un sol clúster o a un nombre molt reduït de clústers. Si la majoria d'usuaris són agrupats en un únic clúster, l'estratègia de clustering no aporta un valor significatiu, ja que el sistema es comportaria de manera similar a un enfocament sense clustering. L'entropia normalitzada es calcula com:
\[
H_{\text{norm}} = \frac{-\sum_{j=1}^{K} p_j \log_2(p_j)}{\log_2(K)}
\]
on \(K\) és el nombre de clústers i \(p_j\) és la proporció d'usuaris assignats al clúster \(j\) en un \textit{fold} particular. Un valor de la mitjana d'entropia normalitzada (calculada a partir dels valors obtinguts en cada \textit{fold}) proper a 1 indica una distribució equilibrada dels usuaris de manera consistent, mentre que un valor proper a 0 suggereix que la majoria dels usuaris estan concentrats en pocs clústers de forma recurrent. Per tant, es cercaran valors mitjans d'entropia més elevats, ja que reflecteixen una millor utilització de la capacitat de segmentació del mètode de clustering en diferents particions de les dades.

\section{Resultats clustering dur}

En aquesta secció s'analitzen els resultats obtinguts amb l'algorisme de clustering dur K-means, utilitzant dues mètriques de distància diferents: la distància Euclidiana i la distància basada en la correlació de Pearson, tal com es mostra a la Figura~\ref{fig:hard-clustering-results}. S'avalua el rendiment en termes de NMAE, NRMSE i entropia normalitzada per als conjunts de dades \texttt{ml-small}, \texttt{ml-1m}, \texttt{books} i \texttt{jester}, en funció del nombre de clústers (K).

A la primera columna de la figura es mostren els resultats d'utilitzar la distància Euclidiana, mentre que a la segona columna es presenten els resultats utilitzant la distància de Pearson.
La primera fila mostra els valors de NMAE (Figures~\ref{fig:hard-clustering-results-a} i~\ref{fig:hard-clustering-results-b}), la segona fila mostra els valors de NRMSE (Figures~\ref{fig:hard-clustering-results-c} i~\ref{fig:hard-clustering-results-d}), i la tercera fila presenta els valors d'entropia normalitzada (Figures~\ref{fig:hard-clustering-results-e} i~\ref{fig:hard-clustering-results-f}).

L'anàlisi dels resultats del clustering dur amb K-means revela una interacció complexa entre l'algorisme, les característiques intrínseques de cada conjunt de dades i la mètrica de distància emprada. No emergeix una configuració universalment òptima, sinó que el rendiment depèn de l'equilibri entre la precisió de les prediccions (NMAE i NRMSE) i la qualitat de la segmentació dels usuaris (entropia normalitzada).

En termes de precisió, el conjunt de dades \texttt{books} destaca per obtenir generalment els errors més baixos quan s'utilitza la distància Euclidiana (Figures~\ref{fig:hard-clustering-results-a} i~\ref{fig:hard-clustering-results-c}, línia verda). En aquest cas, s'observa una millora de la precisió a mesura que el nombre de clústers (K) augmenta fins a un cert punt (aproximadament entre 60 i 80), suggerint que per a aquest dataset, amb la seva baixa densitat (0,33\,\%) i escala de valoració àmplia (1-10), una segmentació més fina pot ajudar a identificar grups d'usuaris amb preferències més homogènies. No obstant això, quan s'utilitza la distància de Pearson per al clustering en el dataset \texttt{books} (Figures~\ref{fig:hard-clustering-results-b} i~\ref{fig:hard-clustering-results-d}, línia verda), la precisió empitjora lleugerament en comparació amb la distància Euclidiana, especialment per a valors de K més alts. Això és contraintuïtiu, ja que s'esperaria que una mètrica de distància alineada amb la de similitud de la predicció (Pearson amb Pearson) oferís millors resultats. Aquest fenomen podria indicar que els clústers formats per Pearson, tot i agrupar usuaris amb patrons de valoració correlacionats, no són tan efectius per a la predicció específica amb desviació de la mitjana com els formats per Euclidiana en aquest cas particular, o que la interacció amb la baixa entropia resultant (explicada més endavant) impacta negativament.

Aquesta observació d'un empitjorament de la precisió amb la distància de Pearson es generalitza a la majoria dels altres conjunts de dades. Per exemple, per a \texttt{ml-1m} (línia taronja), l'increment de l'error amb K és més pronunciat amb Pearson que amb Euclidiana. De manera similar, per a \texttt{jester} (línia vermella), la precisió també es degrada més amb la distància de Pearson a mesura que K augmenta. Per a \texttt{ml-small} (línia blava), les diferències entre les dues distàncies són menys marcades, però no s'observa un benefici clar de Pearson sobre Euclidiana en termes de precisió. Aquesta tendència general suggereix que, tot i que la distància de Pearson captura la similitud de patrons, la seva aplicació directa en l'algorisme K-means, que intrínsecament busca minimitzar la variància espacial (més afí a Euclidiana), pot no traduir-se sempre en una millora de la precisió del sistema de recomanació global. La geometria de l'espai definida per Pearson pot no ser tan ben explotada per K-means com la geometria Euclidiana.

Pel que fa a la precisió en funció de K, per al conjunt \texttt{ml-1m}, augmentar el nombre de clústers K tendeix a empitjorar la precisió amb ambdues distàncies. Això suggereix que per a \texttt{ml-1m} (densitat 4,5\,\%, valoracions enteres 1-5), els patrons de preferència podrien ser més amplis o que els usuaris són intrínsecament més similars. Forçar una segmentació en massa clústers podria estar dividint "veïnats naturals", resultant en clústers més petits i menys informatius.

Per als conjunts \texttt{ml-small} i \texttt{jester}, quan s'utilitza la distància Euclidiana (Figures~\ref{fig:hard-clustering-results-a} i~\ref{fig:hard-clustering-results-c}, línies blava i vermella respectivament), l'empitjorament de la precisió a mesura que K augmenta és relativament petit, especialment després d'un increment inicial. Per a \texttt{jester}, els valors de NMAE i NRMSE es mantenen bastant estables. Per a \texttt{ml-small}, després de l'augment fins a K al voltant de 40, la precisió fins i tot mostra una lleugera tendència a millorar o estabilitzar-se. Aquesta estabilitat o petita pèrdua de precisió amb un nombre elevat de clústers podria permetre dividir la complexitat del problema en subproblemes més petits (un per clúster) sense una penalització significativa en el rendiment predictiu. Això podria ser avantatjós des d'un punt de vista computacional o per a anàlisis més granulars, sempre que l'entropia es mantingui en nivells acceptables.

Un aspecte crític és l'entropia normalitzada. Per al dataset \texttt{books} amb distància de Pearson (Figura~\ref{fig:hard-clustering-results-f}, línia verda), l'entropia és extremadament baixa, especialment per a valors baixos de K, indicant que gairebé tots els usuaris s'assignen a un únic clúster. Això podria explicar en part per què la precisió no millora: si gairebé tots els usuaris estan en un sol clúster, el sistema es comporta de manera similar a no tenir clustering, i els petits clústers residuals poden no ser representatius. Aquest fenomen de baixa entropia amb Pearson per a \texttt{books} suggereix que, tot i el filtratge, hi ha un nucli molt gran d'usuaris altament correlacionats segons Pearson, que dominen l'agrupament. En general, per a la resta de casos, l'entropia tendeix a disminuir a mesura que K augmenta (amb excepcions puntuals com el pic inicial de \texttt{ml-small} amb Pearson), la qual cosa és esperable, ja que aconseguir una distribució perfectament equilibrada és més difícil amb més clústers.

En conclusió, l'eficàcia del clustering dur amb K-means depèn críticament de l'harmonia entre les característiques del conjunt de dades, la mètrica de distància escollida i la seva interacció amb l'algorisme K-means i el mètode de predicció. Contrari al que es podria esperar inicialment, l'ús de la distància de Pearson per al clustering no ha resultat en una millora generalitzada de la precisió i, en molts casos, l'ha empitjorat en comparació amb la distància Euclidiana. Per a certs datasets com \texttt{ml-small} i \texttt{jester}, l'ús de la distància Euclidiana permet incrementar el nombre de clústers sense una pèrdua dràstica de precisió, la qual cosa podria ser útil per a la gestió de la complexitat. No obstant això, l'alineació conceptual entre la mètrica de clustering i la de similitud de predicció no garanteix un millor rendiment global, i altres factors com la geometria de l'espai, la capacitat de K-means per explotar-la, i la distribució resultant dels usuaris (entropia) són determinants. La selecció de K i la mètrica de distància requereix una anàlisi curosa, considerant el compromís entre la millora predictiva (que pot ser esquiva) i la qualitat estructural dels clústers.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-hard-all.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:hard-clustering-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-hard_pearson-all.png}
        \caption{NMAE, distància Pearson}
        \label{fig:hard-clustering-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hard-all.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:hard-clustering-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hard_pearson-all.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:hard-clustering-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hard-all.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:hard-clustering-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hard_pearson-all.png}
        \caption{Entropia, distància Pearson}
        \label{fig:hard-clustering-results-f}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens Small} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens 1M} \\
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Book-Crossing} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Jester} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering dur}
    \label{fig:hard-clustering-results}
\end{figure}

L'anàlisi dels resultats del clustering dur K-means sobre els conjunts de dades sintètiques (Figura~\ref{fig:hard-clustering-sin-results}) ofereix una perspectiva interessant sobre com les característiques extremes de densitat i, fonamentalment, la \textbf{naturalesa artificial d'aquestes dades}, influeixen en el rendiment. És crucial recordar que, tot i que aquests conjunts es basen en un dataset real (MovieLens Latest Small), el procés generatiu simplifica i pot no capturar tota la complexitat i els matisos del comportament humà real en la valoració. Les conclusions extretes d'aquests conjunts sintètics serveixen principalment per entendre el comportament dels algorismes en condicions controlades i extremes, però la seva extrapolació directa a escenaris reals ha de fer-se amb cautela.

Començant per la precisió (NMAE i NRMSE), el conjunt sintètic de baixa densitat (\texttt{syn03}, línia marró) mostra un comportament notablement estable i, de fet, aconsegueix errors (NMAE al voltant de 0.115, NRMSE al voltant de 0.18) inferiors als obtinguts amb els conjunts de dades reals i amb el conjunt sintètic base. Tant amb la distància Euclidiana (Figures~\ref{fig:hard-clustering-sin-results-a} i~\ref{fig:hard-clustering-sin-results-c}) com amb la distància de Pearson (Figures~\ref{fig:hard-clustering-sin-results-b} i~\ref{fig:hard-clustering-sin-results-d}), els valors de NMAE i NRMSE es mantenen pràcticament constants independentment del nombre de clústers K. Això suggereix que, en un escenari de densitat extremadament baixa (0,29\,\%) i amb dades generades que poden tenir una estructura de preferències més simple o menys sorollosa que les reals, l'algorisme de clustering té dificultats per trobar una estructura addicional que pugui alterar significativament la predicció. La informació és tan escassa que la segmentació addicional té un impacte mínim; les prediccions es basen en molt pocs veïns, i la divisió en clústers no canvia fonamentalment aquesta situació.

Per al conjunt sintètic base (\texttt{syn}, línia porpra), que intenta replicar les característiques del MovieLens Small (densitat 2,38\,\%), s'observa un augment de l'error (tant NMAE com NRMSE) amb un nombre baix de clústers (K de 2 a aproximadament 20-30), de manera similar al que succeïa amb el conjunt de dades original MovieLens Small. Després d'aquest augment inicial, l'error s'estabilitza en un nivell comparable al que s'observava amb el dataset real original. Aquest comportament és consistent amb ambdues mètriques de distància. Aquesta similitud en la tendència de l'error amb el seu homòleg real suggereix que el procés de generació, tot i ser una simplificació, aconsegueix capturar algunes de les dinàmiques fonamentals respecte a l'impacte del clustering.

El cas més distintiu és el del conjunt sintètic d'alta densitat (\texttt{syn80}, línia rosa), amb una densitat del 83,9\,\%. Aquí, els valors de NMAE i NRMSE són extremadament baixos (NMAE començant al voltant de 0.02, NRMSE al voltant de 0.05). Aquesta alta precisió és una conseqüència directa de l'abundància d'informació de valoracions. Curiosament, a mesura que el nombre de clústers K augmenta, s'observa un increment gradual però constant de l'error. Això suggereix que, quan les dades són molt denses (encara que artificialment), el sistema de recomanació base ja funciona extremadament bé. Intentar dividir els usuaris en més clústers, en aquest context, sembla introduir una segmentació que no millora, sinó que degrada lleugerament la precisió.

Pel que fa a l'entropia normalitzada (Figures~\ref{fig:hard-clustering-sin-results-e} i~\ref{fig:hard-clustering-sin-results-f}), els patrons reflecteixen com l'algorisme distribueix aquests usuaris sintètics. Amb la distància Euclidiana (Figura~\ref{fig:hard-clustering-sin-results-e}), \texttt{syn} i \texttt{syn03} mostren una disminució de l'entropia amb K. El conjunt \texttt{syn80} mostra una caiguda inicial de l'entropia (K=5), seguida d'una recuperació i fluctuacions, mantenint-se generalment alta.

Amb la distància de Pearson (Figura~\ref{fig:hard-clustering-sin-results-f}), l'entropia per a \texttt{syn} es manté relativament alta i estable. En canvi, per a \texttt{syn03} (línia marró), l'entropia és extremadament baixa i augmenta molt lentament amb K, sense superar mai 0.22. Això indica que, per al conjunt de baixa densitat, la distància de Pearson porta a una situació on gairebé tots els usuaris s'agrupen en un o molt pocs clústers. Aquest fenomen de baixa entropia amb Pearson en un context de baixa densitat és similar al que es va observar amb el conjunt de dades real \texttt{books}, la qual cosa podria suggerir una correlació entre la baixa densitat de dades i la tendència de la distància de Pearson a produir agrupaments molt desequilibrats quan s'utilitza amb K-means. Amb molt poques valoracions, les correlacions de Pearson poden ser o bé molt fortes entre un gran grup (si valoren els pocs ítems comuns de manera similar) o molt dèbils/indefinides, portant K-means a agrupar la majoria d'usuaris junts. Per a \texttt{syn80}, l'entropia amb Pearson comença baixa, però augmenta amb K fins a estabilitzar-se al voltant de 0.8.

En resum, els resultats amb dades sintètiques il·lustren com els algorismes de clustering responen a variacions extremes de densitat i a dades que, per la seva naturalesa generada, poden mancar de la riquesa i complexitat de les interaccions reals. En baixa densitat sintètica, el clustering té poc impacte sobre la precisió, i els errors obtinguts són fins i tot menors que en altres escenaris. En alta densitat sintètica, tendeix a empitjorar una precisió ja artificialment alta. La interpretació d'aquests resultats ha de tenir sempre present que són una aproximació i que les dinàmiques observades podrien no traduir-se directament al comportament amb dades reals.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-hard-sin.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:hard-clustering-sin-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-hard_pearson-sin.png}
        \caption{NMAE, distància Pearson}
        \label{fig:hard-clustering-sin-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hard-sin.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:hard-clustering-sin-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hard_pearson-sin.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:hard-clustering-sin-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hard-sin.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:hard-clustering-sin-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hard_pearson-sin.png}
        \caption{Entropia, distància Pearson}
        \label{fig:hard-clustering-sin-results-f}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customPurple, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (base)} \\
            \tikz{\draw[customBrown, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (baixa densitat)} \\
            \tikz{\draw[customPink, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (alta densitat)} \\

        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering dur dades sintètiques}
    \label{fig:hard-clustering-sin-results}
\end{figure}

\section{Resultats clustering difús}

L'aplicació del clustering difús mitjançant l'algorisme Fuzzy C-Means (FCM) als conjunts de dades reals, seguida d'una assignació dura mitjançant el mètode del Centre de Gravetat (COG), presenta una sèrie de resultats particulars que es detallen a la Figura~\ref{fig:fuzzy-clustering-results}. Aquesta aproximació, tot i la seva naturalesa difusa inicial, finalment assigna cada usuari a un únic clúster per a la fase de predicció, mantenint així la coherència amb l'arquitectura de recomanació emprada.

Observant la precisió (NMAE i NRMSE) amb la distància Euclidiana (Figures~\ref{fig:fuzzy-clustering-results-a} i~\ref{fig:fuzzy-clustering-results-c}), el comportament dels diferents conjunts de dades és força heterogeni. Per al conjunt \texttt{ml-small} (línia blava), l'error augmenta significativament a mesura que K s'incrementa, especialment fins a K al voltant de 60, estabilitzant-se després en un nivell d'error superior al dels valors baixos de K. Això suggereix que la flexibilitat inherent de FCM, combinada amb una posterior assignació dura, no necessàriament es tradueix en una millora de la precisió per a aquest dataset, i una segmentació més fina resulta perjudicial. El conjunt \texttt{ml-1m} (línia taronja) mostra un lleuger increment de l'error amb K, però amb menys variabilitat que \texttt{ml-small}, mantenint-se relativament estable. El conjunt \texttt{jester} (línia vermella) presenta una precisió notablement constant al llarg de tot el rang de K, amb variacions mínimes. El cas més favorable amb distància Euclidiana és el del conjunt \texttt{books} (línia verda), on tant el NMAE com el NRMSE disminueixen de manera consistent a mesura que K augmenta, assolint els valors més baixos de precisió entre tots els datasets amb aquesta configuració. Això indica que, per a \texttt{books}, la capacitat de FCM per modelar pertinences parcials, fins i tot després de l'assignació dura, podria estar identificant agrupacions més coherents que beneficien la predicció.

Quan s'utilitza la distància de Pearson amb FCM (Figures~\ref{fig:fuzzy-clustering-results-b} i~\ref{fig:fuzzy-clustering-results-d}), els resultats de precisió canvien dràsticament i mostren patrons molt distintius. Per a \texttt{ml-1m} i \texttt{jester} (línies taronja i vermella), la precisió esdevé pràcticament constant al llarg de tot el rang de K, amb valors d'error superiors als obtinguts amb distància Euclidiana. Però el comportament més sorprenent es dóna en \texttt{ml-small} i \texttt{books} (línies blava i verda). Ambdós conjunts mostren un patró cíclic o oscil·latori en els valors de NMAE i NRMSE. Per exemple, per a \texttt{ml-small}, l'error fluctua considerablement, amb pics i valls a mesura que K canvia. Per a \texttt{books}, tot i que les oscil·lacions són menys pronunciades, també s'observa una manca de tendència clara. Aquest comportament oscil·latori és inusual i podria ser un artefacte de la interacció entre la distància de Pearson (que opera sobre correlacions), l'algorisme FCM (que calcula centroids ponderats i graus de pertinença), i el mètode d'assignació dura COG. La mètrica de Pearson pot ser molt sensible a petits canvis en les dades o en la inicialització dels centroids, especialment quan es combina amb els càlculs iteratius de FCM. Les oscil·lacions podrien indicar una inestabilitat en la convergència dels clústers o que l'assignació final mitjançant COG és molt sensible al nombre de clústers quan s'utilitza Pearson en aquest context difús.

L'anàlisi de l'entropia normalitzada (Figures~\ref{fig:fuzzy-clustering-results-e} i~\ref{fig:fuzzy-clustering-results-f}) revela aspectes crítics sobre la distribució dels usuaris. Amb la distància Euclidiana (Figura~\ref{fig:fuzzy-clustering-results-e}), \texttt{ml-small} i \texttt{books} mantenen una entropia relativament alta i estable (al voltant de 0.7-0.9), indicant una bona distribució dels usuaris entre els clústers. En canvi, \texttt{ml-1m} mostra una entropia que disminueix amb K, i \texttt{jester} presenta l'entropia més baixa i més variable, amb caigudes significatives per a certs valors de K (per exemple, K=30 i K=85), la qual cosa suggereix que per a \texttt{jester}, la combinació FCM-Euclidiana-COG tendeix a concentrar els usuaris en pocs clústers.

La situació de l'entropia amb la distància de Pearson és encara més extrema (Figura~\ref{fig:fuzzy-clustering-results-f}). Per a \texttt{ml-small}, \texttt{ml-1m} i \texttt{jester}, l'entropia cau a valors propers a zero per a $K>2$ i es manté allà, amb lleugeres fluctuacions gairebé imperceptibles. Això indica que, amb la distància de Pearson, el mètode FCM seguit de l'assignació COG resulta en què pràcticament tots els usuaris són assignats a un únic clúster. Aquesta és una troballa crítica, ja que invalida en gran manera el propòsit del clustering per a aquests casos: si tots els usuaris estan en un sol grup, el sistema es comporta com si no hi hagués clustering. Aquest fenomen podria explicar els resultats de precisió constants o oscil·latoris observats per a aquests datasets amb Pearson, ja que les prediccions es basarien gairebé sempre en el conjunt complet d'usuaris (o en un grup dominant i altres residuals molt petits). L'única excepció és el conjunt \texttt{books}, on l'entropia, tot i començar baixa, augmenta amb K fins a assolir valors més respectables (al voltant de 0.7), indicant una millor distribució, encara que no perfecta. Aquesta millora en l'entropia per a \texttt{books} amb Pearson podria estar relacionada amb la seva millor precisió relativa en aquesta configuració, tot i les oscil·lacions. La sensibilitat de la distància de Pearson i la seva possible interacció amb el paràmetre de difusió \(m\) de FCM (tot i que es va mantenir a \(m=2\)) podrien estar darrere d'aquesta concentració extrema d'usuaris.

En conclusió, el clustering difús amb FCM i posterior assignació dura mostra una gran sensibilitat a l'elecció de la mètrica de distància. Mentre que amb la distància Euclidiana s'observen comportaments diversos en precisió i una entropia generalment acceptable (excepte per a \texttt{jester}), l'ús de la distància de Pearson condueix a resultats de precisió inestables o plans i, de manera més preocupant, a una entropia extremadament baixa per a la majoria dels datasets. Aquesta concentració d'usuaris en un sol clúster amb Pearson qüestiona la utilitat pràctica d'aquesta combinació específica (FCM-Pearson-COG) per a la segmentació efectiva d'usuaris en aquests sistemes de recomanació, ja que la naturalesa "difusa" no sembla traduir-se en una partició dura útil quan s'empra Pearson. El dataset \texttt{books} emergeix com el cas més consistentment positiu, especialment amb distància Euclidiana, on la precisió millora amb K i l'entropia es manté alta.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-fuzzy-all.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:fuzzy-clustering-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-fuzzy_pearson-all.png}
        \caption{NMAE, distància Pearson}
        \label{fig:fuzzy-clustering-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-fuzzy-all.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:fuzzy-clustering-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-fuzzy_pearson-all.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:fuzzy-clustering-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-fuzzy-all.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:fuzzy-clustering-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-fuzzy_pearson-all.png}
        \caption{Entropia, distància Pearson}
        \label{fig:fuzzy-clustering-results-f}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens Small} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens 1M} \\
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Book-Crossing} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Jester} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering difús}
    \label{fig:fuzzy-clustering-results}
\end{figure}

L'avaluació del clustering difús (FCM amb assignació COG) sobre els conjunts de dades sintètiques, com es mostra a la Figura~\ref{fig:fuzzy-clustering-sin-results}, proporciona informació addicional sobre la interacció entre la naturalesa de les dades generades, la densitat, i el comportament de l'algorisme. Com en el cas anterior, és important subratllar que aquestes dades sintètiques són una simplificació de la realitat i els resultats han d'interpretar-se en aquest context.

Analitzant la precisió (NMAE i NRMSE) amb la distància Euclidiana (Figures~\ref{fig:fuzzy-clustering-sin-results-a} i~\ref{fig:fuzzy-clustering-sin-results-c}), observem que el conjunt sintètic de baixa densitat (\texttt{syn03}, línia marró) i el d'alta densitat (\texttt{syn80}, línia rosa) mostren una precisió extremadament estable al llarg de tot el rang de K. Els valors d'error són pràcticament plans, indicant que, per a aquestes densitats extremes i amb dades sintètiques, la variació del nombre de clústers en FCM té un impacte negligible en la precisió final. Per a \texttt{syn03}, l'escassetat d'informació limita la capacitat del clustering per millorar o empitjorar les prediccions. Per a \texttt{syn80}, l'abundància d'informació fa que el sistema base ja sigui molt precís, i la segmentació addicional no aporta canvis significatius. El conjunt sintètic base (\texttt{syn}, línia porpra) presenta un comportament diferent: l'error augmenta amb K fins a estabilitzar-se, de manera similar al que s'observava amb el clustering dur per a aquest mateix conjunt sintètic i el seu homòleg real, suggerint que la segmentació addicional és perjudicial.

Quan s'utilitza la distància de Pearson (Figures~\ref{fig:fuzzy-clustering-sin-results-b} i~\ref{fig:fuzzy-clustering-sin-results-d}), els resultats de precisió canvien notablement. El conjunt \texttt{syn03} manté la seva precisió gairebé constant, tot i que a un nivell d'error lleugerament superior que amb Euclidiana. El conjunt \texttt{syn80} continua mostrant una precisió molt alta, però amb un lleuger augment de l'error a mesura que K augmenta, similar al que passava amb el clustering dur. El canvi més dràstic es produeix amb el conjunt sintètic base (\texttt{syn}), que ara exhibeix un patró fortament oscil·latori en els valors de NMAE i NRMSE. Aquestes fluctuacions pronunciades són similars a les observades amb alguns conjunts de dades reals quan s'utilitzava FCM amb Pearson, i podrien atribuir-se a inestabilitats en el procés de convergència de FCM amb aquesta mètrica o a la sensibilitat de l'assignació COG a les subtils variacions en els graus de pertinença generats per Pearson en un context de dades sintètiques.

L'entropia normalitzada (Figures~\ref{fig:fuzzy-clustering-sin-results-e} i~\ref{fig:fuzzy-clustering-sin-results-f}) revela aspectes crucials sobre la distribució dels usuaris sintètics. Amb la distància Euclidiana (Figura~\ref{fig:fuzzy-clustering-sin-results-e}), \texttt{syn} i \texttt{syn03} mostren una entropia que comença alta i disminueix a mesura que K augmenta, indicant una distribució progressivament menys equilibrada. El conjunt \texttt{syn80} presenta un comportament d'entropia molt particular: comença alta, cau a zero per a K=5, i després oscil·la entre zero i valors baixos (aproximadament 0.25) per a la resta de K. Aquesta tendència a una entropia zero o molt baixa per a \texttt{syn80} amb Euclidiana significa que, malgrat l'alta densitat, l'algorisme tendeix a agrupar gairebé tots els usuaris en un sol clúster o en un nombre molt reduït de clústers. Això és sorprenent i podria ser un artefacte de com FCM amb Euclidiana gestiona la variància en un espai de dades sintètiques tan densament i uniformement poblat.

Amb la distància de Pearson (Figura~\ref{fig:fuzzy-clustering-sin-results-f}), els patrons d'entropia són encara més extrems. Per al conjunt sintètic base (\texttt{syn}), l'entropia és pràcticament zero per a gairebé tots els valors de K majors que 2, amb algunes oscil·lacions puntuals on arriba a valors més alts (per exemple, K=10, K=20). Aquesta és una indicació clara que la combinació FCM-Pearson-COG porta a una concentració gairebé total dels usuaris en un sol clúster per a aquest conjunt sintètic. Això explicaria les oscil·lacions en la precisió, ja que petits canvis en K podrien moure uns pocs usuaris residuals, mentre la majoria romanen agrupats. Per al conjunt \texttt{syn03}, l'entropia també és molt baixa, començant al voltant de 0.5 i disminuint fins a aproximadament 0.3, indicant una distribució molt desequilibrada, similar al que passava amb el clustering dur per a aquest dataset amb Pearson. El conjunt \texttt{syn80}, en canvi, manté una entropia relativament més alta i estable amb Pearson (entre 0.55 i 0.8), suggerint que per a aquestes dades sintètiques d'alta densitat, Pearson permet una segmentació més equilibrada que Euclidiana.

En conclusió, l'aplicació de FCM a dades sintètiques mostra que els resultats són altament dependents de la densitat de les dades generades i de la mètrica de distància. La precisió tendeix a ser estable o a empitjorar amb l'augment de K, i les oscil·lacions observades amb Pearson per al conjunt base sintètic són notables. L'entropia revela problemes significatius de concentració d'usuaris, especialment quan s'utilitza la distància Euclidiana amb el conjunt d'alta densitat (\texttt{syn80}) o la distància de Pearson amb el conjunt base (\texttt{syn}) i el de baixa densitat (\texttt{syn03}). Aquests resultats amb dades sintètiques, tot i que útils per explorar comportaments en condicions extremes, han de ser interpretats amb la deguda consideració de les limitacions inherents a la generació de dades, que pot no capturar tota la variabilitat i complexitat dels patrons de preferència humans reals. La tendència a l'entropia zero o molt baixa en diverses configuracions sintètiques quan s'utilitza FCM-Pearson suggereix una possible inestabilitat o una particularitat d'aquesta combinació metodològica amb la naturalesa de les dades generades.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-fuzzy-sin.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:fuzzy-clustering-sin-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-fuzzy_pearson-sin.png}
        \caption{NMAE, distància Pearson}
        \label{fig:fuzzy-clustering-sin-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-fuzzy-sin.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:fuzzy-clustering-sin-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-fuzzy_pearson-sin.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:fuzzy-clustering-sin-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-fuzzy-sin.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:fuzzy-clustering-sin-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-fuzzy_pearson-sin.png}
        \caption{Entropia, distància Pearson}
        \label{fig:fuzzy-clustering-sin-results-f}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customPurple, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (base)} \\
            \tikz{\draw[customBrown, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (baixa densitat)} \\
            \tikz{\draw[customPink, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (alta densitat)} \\

        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering difús dades sintètiques}
    \label{fig:fuzzy-clustering-sin-results}
\end{figure}

\section{Resultats clustering jeràrquic}

L'aplicació del clustering jeràrquic aglomeratiu (HAC), utilitzant el mètode d'enllaç promig (average linkage) i tallant el dendrograma per obtenir K clústers, presenta una sèrie de dinàmiques interessants quan s'aplica als conjunts de dades reals, tal com es mostra a la Figura~\ref{fig:hac-clustering-results}.

En termes de precisió (NMAE i NRMSE) amb la distància Euclidiana (Figures~\ref{fig:hac-clustering-results-a} i~\ref{fig:hac-clustering-results-c}), el comportament dels datasets és variat. Per a \texttt{ml-small} (línia blava), l'error augmenta de manera notable a mesura que K s'incrementa, especialment fins a K al voltant de 40, per després estabilitzar-se en un nivell d'error significativament més alt que amb pocs clústers. Aquest patró suggereix que la segmentació progressiva que realitza HAC, quan es basa en la distància Euclidiana, no aconsegueix crear agrupacions que millorin la predicció per a \texttt{ml-small}, sinó que la fragmentació resulta perjudicial. El conjunt \texttt{ml-1m} (línia taronja) mostra un increment constant i més gradual de l'error amb K, indicant que per a aquest dataset més gran, una major granularitat en els clústers també redueix la precisió. El conjunt \texttt{jester} (línia vermella) presenta una precisió molt estable amb la distància Euclidiana, amb un lleuger augment de l'error a l'inici que després es manté pràcticament constant, suggerint que la naturalesa densa de \texttt{jester} fa que la predicció sigui menys sensible al nombre de clústers formats per HAC amb Euclidiana. El cas més favorable, similar al que s'observava amb altres mètodes de clustering, és el del conjunt \texttt{books} (línia verda). Aquí, el NMAE i el NRMSE disminueixen de manera consistent a mesura que K augmenta, assolint els errors més baixos. Això indica que per a \texttt{books}, l'estructura jeràrquica descoberta per HAC amb Euclidiana i el mètode d'enllaç promig, condueix a agrupacions progressivament més útils per a la predicció.

Quan s'utilitza la distància de Pearson amb HAC (Figures~\ref{fig:hac-clustering-results-b} i~\ref{fig:hac-clustering-results-d}), els resultats de precisió mostren canvis respecte a la distància Euclidiana. Per a \texttt{ml-small}, l'error encara augmenta amb K, però l'increment és menys pronunciat i l'estabilització es produeix a un nivell d'error lleugerament inferior que amb Euclidiana per a valors alts de K. Per al conjunt \texttt{ml-1m}, la tendència de l'error amb la distància de Pearson és molt similar a la observada amb la distància Euclidiana, mostrant un increment gradual i constant de l'error a mesura que K augmenta. En el cas de \texttt{jester}, l'ús de la distància de Pearson resulta en un lleuger increment de l'error amb K, a diferència del comportament pràcticament constant observat amb la distància Euclidiana; per tant, per a \texttt{jester}, la precisió empitjora de manera més gradual però consistent amb Pearson. Per al conjunt \texttt{books}, la distància de Pearson també condueix a una millora de la precisió a mesura que K augmenta, però la corba és més plana i l'error es manté gairebé constant després d'una petita millora inicial. En aquest cas, la distància Euclidiana semblava oferir una millora més sostinguda per a \texttt{books}. Aquests resultats suggereixen que la naturalesa de l'enllaç promig, que considera la mitjana de les distàncies entre tots els parells d'usuaris dels clústers a fusionar, interactua de manera diferent amb les dues mètriques de distància. La distància de Pearson, basada en correlacions, pot ser més sensible a valors atípics o a usuaris amb pocs ítems en comú, la qual cosa podria influir en la mitjana de les distàncies i, per tant, en l'estructura del dendrograma resultant.

L'anàlisi de l'entropia normalitzada (Figures~\ref{fig:hac-clustering-results-e} i~\ref{fig:hac-clustering-results-f}) és particularment reveladora per al clustering jeràrquic. Amb la distància Euclidiana (Figura~\ref{fig:hac-clustering-results-e}), tots els conjunts de dades, excepte \texttt{books}, mostren una entropia molt baixa per a un nombre reduït de clústers ($K<10$), que després augmenta i s'estabilitza a mesura que K creix. Per exemple, \texttt{ml-small}, \texttt{ml-1m} i \texttt{jester} tenen entropies properes a zero per a K=2, indicant que el tall del dendrograma en dos clústers resulta en un clúster que conté gairebé tots els usuaris i un altre molt petit. Això és una característica coneguda del clustering jeràrquic aglomeratiu quan es talla a pocs nivells: sovint es produeixen clústers de mides molt desiguals. A mesura que K augmenta, es van separant més branques del dendrograma, permetent una distribució més equilibrada. El conjunt \texttt{books} és una excepció notable, ja que la seva entropia comença relativament alta i té un pic al voltant de K=15, per després disminuir lleugerament i estabilitzar-se, suggerint una estructura jeràrquica més equilibrada des de l'inici per a aquest dataset amb Euclidiana.

Amb la distància de Pearson (Figura~\ref{fig:hac-clustering-results-f}), l'entropia mostra un comportament molt diferent i generalment més favorable en termes de distribució equilibrada. Per a \texttt{ml-small}, \texttt{ml-1m} i \texttt{jester}, l'entropia augmenta de manera molt més ràpida i constant amb K, assolint valors significativament més alts que amb la distància Euclidiana, especialment per a \texttt{ml-small} que arriba a entropies superiors a 0.8. Això indica que l'estructura jeràrquica generada amb la distància de Pearson, quan es talla per obtenir K clústers, tendeix a produir agrupacions de mides més equilibrades des de valors baixos de K. El conjunt \texttt{books} també mostra un patró interessant: comença amb una entropia moderada, té un pic molt alt (superior a 0.8) per a K=5, i després disminueix gradualment. Aquesta capacitat de la distància de Pearson per generar dendrogrames que, en ser tallats, produeixen particions més equilibrades podria estar relacionada amb la seva naturalesa normalitzada, que pot donar lloc a una estructura de similituds més distribuïda.

En conclusió, el clustering jeràrquic aglomeratiu mostra una forta dependència de la mètrica de distància tant en termes de precisió com, de manera molt destacada, en la distribució dels usuaris (entropia). Tot i que la distància Euclidiana pot oferir millores de precisió per a certs datasets com \texttt{books}, sovint condueix a entropies molt baixes per a pocs clústers. La distància de Pearson, per contra, tendeix a generar clústers més equilibrats (major entropia) de manera més consistent a mesura que K augmenta, la qual cosa és desitjable des del punt de vista de la segmentació. No obstant això, aquesta millor distribució no sempre es tradueix en una millor precisió. Per exemple, mentre \texttt{ml-1m} presenta un comportament de precisió similar amb ambdues distàncies, per a \texttt{jester}, l'ús de Pearson condueix a un lleuger empitjorament de la precisió. Això subratlla el compromís entre obtenir clústers ben distribuïts i clústers que siguin efectivament útils per a la tasca de predicció. L'elecció de la mètrica i el nombre de clústers K en HAC requereix una consideració acurada de les propietats estructurals dels clústers formats (reflectides per l'entropia) juntament amb el rendiment predictiu.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-hac-all.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:hac-clustering-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-hac_pearson-all.png}
        \caption{NMAE, distància Pearson}
        \label{fig:hac-clustering-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hac-all.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:hac-clustering-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hac_pearson-all.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:hac-clustering-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hac-all.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:hac-clustering-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hac_pearson-all.png}
        \caption{Entropia, distància Pearson}
        \label{fig:hac-clustering-results-f}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens Small} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens 1M} \\
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Book-Crossing} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Jester} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering jeràrquic}
    \label{fig:hac-clustering-results}
\end{figure}

L'aplicació del clustering jeràrquic aglomeratiu (HAC) als conjunts de dades sintètiques, visible a la Figura~\ref{fig:hac-clustering-sin-results}, continua il·lustrant la influència de la densitat de les dades i la mètrica de distància, tot recordant la naturalesa artificial d'aquests conjunts.

En termes de precisió (NMAE i NRMSE) amb la distància Euclidiana (Figures~\ref{fig:hac-clustering-sin-results-a} i~\ref{fig:hac-clustering-sin-results-c}), el conjunt sintètic de baixa densitat (\texttt{syn03}, línia marró) manté una precisió pràcticament constant i amb errors baixos, similar al que s'observava amb altres mètodes de clustering per a aquest dataset. El conjunt sintètic d'alta densitat (\texttt{syn80}, línia rosa) mostra un increment gradual de l'error a mesura que K augmenta, indicant que la segmentació addicional empitjora una precisió que ja és molt bona de partida. El conjunt sintètic base (\texttt{syn}, línia porpra) exhibeix un augment de l'error amb K, especialment per a valors baixos, estabilitzant-se posteriorment, un patró consistent amb el seu comportament en altres experiments i amb el seu homòleg real.

Quan s'utilitza la distància de Pearson (Figures~\ref{fig:hac-clustering-sin-results-b} i~\ref{fig:hac-clustering-sin-results-d}), el comportament de \texttt{syn03} i \texttt{syn80} en termes de precisió és molt similar al que presenten amb la distància Euclidiana: \texttt{syn03} es manté estable i \texttt{syn80} mostra un augment gradual de l'error. Per al conjunt sintètic base (\texttt{syn}, línia porpra), l'ús de la distància de Pearson resulta en un comportament de l'error que, si bé presenta algunes fluctuacions, es manté generalment en un nivell inferior al que s'obté amb la distància Euclidiana per a aquest mateix conjunt, especialment per a valors de K més elevats.

L'anàlisi de l'entropia normalitzada (Figures~\ref{fig:hac-clustering-sin-results-e} i~\ref{fig:hac-clustering-sin-results-f}) és on es veuen les diferències més significatives induïdes per la naturalesa jeràrquica de l'algorisme. Amb la distància Euclidiana (Figura~\ref{fig:hac-clustering-sin-results-e}), els tres conjunts de dades sintètiques comencen amb entropies molt baixes per a K=2 (properes a 0 per a \texttt{syn80} i \texttt{syn}, i al voltant de 0.2 per a \texttt{syn03}). Això reflecteix la tendència de HAC a crear un clúster molt gran i altres de molt petits quan es talla el dendrograma a pocs nivells. A mesura que K augmenta, l'entropia s'incrementa per a tots tres. \texttt{syn03} assoleix l'entropia més alta i més estable (al voltant de 0.7-0.85). \texttt{syn} arriba a valors al voltant de 0.6-0.65. \texttt{syn80} mostra l'increment més lent, assolint només al voltant de 0.45-0.5 per a K alt. Aquest comportament suggereix que la naturalesa de les dades sintètiques d'alta densitat (\texttt{syn80}) pot portar a una estructura jeràrquica on, fins i tot amb molts clústers, la distribució dels usuaris roman relativament desequilibrada quan s'usa Euclidiana.

Amb la distància de Pearson (Figura~\ref{fig:hac-clustering-sin-results-f}), els patrons d'entropia canvien dràsticament. El conjunt sintètic base (\texttt{syn}, línia porpra) mostra un increment molt ràpid i sostingut de l'entropia, arribant a valors propers a 1 per a K alt. Això indica que, per a aquest conjunt, la distància de Pearson genera una estructura jeràrquica que permet obtenir particions molt equilibrades a mesura que augmenta el nombre de talls. El conjunt \texttt{syn80} (línia rosa) també mostra un increment de l'entropia amb K, tot i que més moderat, estabilitzant-se al voltant de 0.5. El comportament més particular és el de \texttt{syn03} (línia marró): comença amb una entropia relativament alta (al voltant de 0.75 per K=2), però després disminueix de manera constant a mesura que K augmenta, arribant a valors propers a 0.2 per a K alt. Aquesta inversió de la tendència (disminució de l'entropia amb K) és inusual i podria indicar que per a dades sintètiques de molt baixa densitat, la combinació de Pearson amb HAC podria portar a una situació on, en intentar definir més clústers, aquests esdevenen progressivament més desequilibrats o dominats per uns pocs grups més coherents segons Pearson, mentre que la resta es fragmenten de manera desigual.

En conclusió, el clustering jeràrquic aplicat a dades sintètiques mostra que la precisió és relativament insensible al nombre de clústers per a \texttt{syn03}, mentre que per a \texttt{syn} i \texttt{syn80} tendeix a empitjorar amb K quan s'usa Euclidiana. Amb Pearson, \texttt{syn} mostra una millora en la precisió respecte a Euclidiana. L'aspecte més destacable és l'impacte sobre l'entropia. Amb la distància Euclidiana, les entropies inicials són baixes, augmentant amb K, però \texttt{syn80} manté una distribució menys equilibrada. Amb la distància de Pearson, \texttt{syn} aconsegueix distribucions molt equilibrades per a K alt, mentre que \texttt{syn03} mostra un comportament atípic de disminució de l'entropia amb K. Aquests resultats subratllen com l'estructura jeràrquica inherent de l'algorisme interactua amb les mètriques de distància i les propietats extremes de les dades sintètiques, generant patrons de distribució d'usuaris complexos i de vegades contraintuïtius. La interpretació d'aquests patrons ha de considerar sempre les simplificacions inherents als conjunts de dades generats.


\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-hac-sin.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:hac-clustering-sin-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-fuzzy_pearson-sin.png}
        \caption{NMAE, distància Pearson}
        \label{fig:hac-clustering-sin-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hac-sin.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:hac-clustering-sin-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-hac_pearson-sin.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:hac-clustering-sin-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hac-sin.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:hac-clustering-sin-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-hac_pearson-sin.png}
        \caption{Entropia, distància Pearson}
        \label{fig:hac-clustering-sin-results-f}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customPurple, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (base)} \\
            \tikz{\draw[customBrown, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (baixa densitat)} \\
            \tikz{\draw[customPink, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (alta densitat)} \\

        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering jeràrquic dades sintètiques}
    \label{fig:hac-clustering-sin-results}
\end{figure}
\newpage
Per tal de justificar l'elecció del mètode d'enllaç promig (\textit{average linkage}) utilitzat en les seccions anteriors del clustering jeràrquic, s'ha realitzat una anàlisi comparativa amb altres mètodes d'enllaç comuns. Aquests mètodes defineixen com es calcula la distància entre dos clústers durant el procés d'aglomeració:
\begin{itemize}
    \item \textbf{Enllaç simple (Single Linkage):} La distància entre dos clústers es defineix com la distància mínima entre qualsevol parell de punts dels dos clústers. Tendeix a formar clústers allargats i pot patir l'efecte "encadenament".
    \item \textbf{Enllaç complet (Complete Linkage):} La distància entre dos clústers és la distància màxima entre qualsevol parell de punts dels dos clústers. Tendeix a produir clústers més compactes i de mida similar.
    \item \textbf{Enllaç per centroides (Centroid Linkage):} La distància entre dos clústers es calcula com la distància entre els seus respectius centroides.
    \item \textbf{Enllaç ponderat (Weighted Linkage, WPGMA):} Similar a l'enllaç promig, però pondera les contribucions dels clústers originals de manera diferent quan es fusionen, especialment útil si els clústers tenen mides molt diferents.
\end{itemize}
Aquesta anàlisi comparativa s'ha realitzat exclusivament sobre el conjunt de dades \texttt{ml-small}, com es pot veure a la Figura~\ref{fig:hac-clustering-e-results}. La decisió de centrar-se en aquest conjunt es basa en els resultats previs: \texttt{ml-small} va mostrar una sensibilitat intermèdia als paràmetres de clustering i a les mètriques de distància, a diferència de \texttt{ml-1m} (on la precisió generalment empitjorava amb K), \texttt{jester} (molt estable o empitjorant amb Pearson) o \texttt{books} (que ja mostrava bons resultats amb \textit{average linkage} però amb problemes d'entropia en algunes configuracions). \texttt{ml-small} ofereix, per tant, un escenari més matisat per avaluar les diferències subtils entre mètodes d'enllaç.

Observant els resultats de precisió (NMAE i NRMSE) per a \texttt{ml-small} amb distància Euclidiana (Figures~\ref{fig:hac-clustering-e-results-a} i~\ref{fig:hac-clustering-e-results-c}), el mètode d'enllaç promig (\textit{average}, línia blava) i l'enllaç per centroides (\textit{centroid}, línia verda) mostren un comportament molt similar, amb un augment de l'error a mesura que K creix, estabilitzant-se a un nivell relativament alt. L'enllaç ponderat (\textit{weighted}, línia taronja) segueix una tendència similar però amb un error lleugerament superior. L'enllaç simple (\textit{single}, línia vermella) és el que presenta l'error més baix de manera consistent, especialment per a K baixos, tot i que també augmenta amb K. L'enllaç complet (\textit{complete}, línia porpra) mostra l'error més alt entre tots els mètodes.

Amb la distància de Pearson (Figures~\ref{fig:hac-clustering-e-results-b} i~\ref{fig:hac-clustering-e-results-d}), el mètode d'enllaç promig (\textit{average}) continua mostrant un augment de l'error amb K, però assoleix un nivell d'estabilització inferior al que tenia amb Euclidiana. L'enllaç ponderat i el complet segueixen patrons similars, amb el complet obtenint els errors més alts. L'enllaç per centroides i l'enllaç simple mostren els errors més baixos i un comportament molt estable al llarg de K, amb el simple sent lleugerament millor.

Pel que fa a l'entropia amb distància Euclidiana (Figura~\ref{fig:hac-clustering-e-results-e}), l'enllaç simple és el que presenta l'entropia més baixa de manera consistent, indicant clústers molt desequilibrats. L'enllaç complet i el ponderat aconsegueixen les entropies més altes i estables. L'enllaç promig i el de centroides es troben en una posició intermèdia. Amb la distància de Pearson (Figura~\ref{fig:hac-clustering-e-results-f}), la situació canvia: l'enllaç complet assoleix l'entropia més alta, propera a 1, indicant una distribució gairebé perfecta. L'enllaç promig, ponderat i per centroides també aconsegueixen entropies altes i creixents amb K. Novament, l'enllaç simple és el que presenta l'entropia més baixa.

La justificació per utilitzar l'enllaç promig (\textit{average linkage}) en els experiments principals es basa en un compromís entre la precisió i la qualitat de la distribució dels clústers. Tot i que l'enllaç simple sovint ofereix la millor precisió, especialment amb la distància Euclidiana, la seva tendència a produir entropies molt baixes (clústers desequilibrats i efecte "encadenament") el fa menys desitjable si es busca una segmentació significativa. L'enllaç complet, tot i que pot generar entropies altes (especialment amb Pearson), tendeix a produir errors de predicció més elevats. L'enllaç promig, tot i no ser el millor en cap mètrica individual de manera consistent, ofereix un balanç raonable: la seva precisió és competitiva, especialment amb la distància de Pearson per a \texttt{ml-small}, i aconsegueix entropies significativament millors que l'enllaç simple. A més, és conegut per ser menys sensible a valors atípics que l'enllaç simple o complet, i per generar clústers amb formes més equilibrades, la qual cosa s'alinea amb l'objectiu de trobar grups d'usuaris coherents. Per tant, l'elecció de l'enllaç promig es va considerar un punt de partida robust i equilibrat per a l'anàlisi principal del clustering jeràrquic.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-euc-ml-small.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:hac-clustering-e-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-pear-ml-small.png}
        \caption{NMAE, distància Pearson}
        \label{fig:hac-clustering-e-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-euc-ml-small.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:hac-clustering-e-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-pear-ml-small.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:hac-clustering-e-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-euc-ml-small.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:hac-clustering-e-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-pear-ml-small.png}
        \caption{Entropia, distància Pearson}
        \label{fig:hac-clustering-e-results-f}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Average} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Weighted} \\
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Centroid} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Single} \\
            \tikz{\draw[customPurple, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Complete} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering jeràrquic diferents mètodes d'enllaç (MovieLens Small)}
    \label{fig:hac-clustering-e-results}
\end{figure}

\section{Resultats clustering per densitat}

Per a l'avaluació del clustering basat en densitat, s'ha utilitzat l'algorisme DBSCAN. Aquest algorisme requereix la definició de dos paràmetres clau: \(\varepsilon\) (el radi del veïnat) i \(\mathit{MinPts}\) (el nombre mínim de punts dins d'aquest radi per considerar una regió com a densa).

En aquest estudi, s'ha decidit mantenir el paràmetre \(\mathit{MinPts}\) fix en un valor de 5. Aquesta elecció es basa en la pràctica comuna i les recomanacions trobades a la literatura \cite{satsiou2017hybrid}, on sovint s'utilitzen valors petits per a \(\mathit{MinPts}\). Un valor de 5 es considera un bon compromís que permet identificar regions raonablement denses sense ser excessivament restrictiu ni massa permissiu amb el soroll, especialment considerant que el nostre espai de característiques (vectors de valoracions d'usuaris) pot ser d'alta dimensionalitat.

Amb \(\mathit{MinPts}\) fixat, l'atenció es centra en la selecció del paràmetre \(\varepsilon\). Per determinar un rang adequat de valors per a \(\varepsilon\), s'ha emprat la tècnica del gràfic de k-distància (k-distance graph). Aquesta tècnica consisteix a calcular la distància de cada punt al seu k-èssim veí més proper, on k en aquest context correspon directament al valor de \(\mathit{MinPts}\) (és a dir, la distància al 5è veí més proper, ja que \(\mathit{MinPts}=5\)). Després, aquestes distàncies s'ordenen de manera ascendent i es representen gràficament. El punt on la corba mostra un ``colze'' (elbow) o un canvi brusc de pendent suggereix un valor òptim o un llindar natural per a \(\varepsilon\). Els valors de \(\varepsilon\) per sota d'aquest colze podrien resultar en massa clústers petits o considerar molts punts com a soroll, mentre que valors molt per sobre podrien fusionar clústers diferents.

A la Figura~\ref{fig:k-distance-example} es mostra un exemple d'un gràfic de k-distància (específicament, la distància al 5è veí, assumint \(\mathit{MinPts}=5\)) per al conjunt de dades \texttt{ml-small} utilitzant la distància Euclidiana. En aquest exemple il·lustratiu, s'observa que la corba comença a mostrar un canvi significatiu de pendent en un cert rang. Basant-nos en l'anàlisi d'aquests gràfics per a cada combinació de dataset i mètrica de distància, s'ha seleccionat un rang de valors d'\(\varepsilon\) per explorar el comportament de DBSCAN. Per exemple, en el cas il·lustrat a la figura, un rang raonable per a \(\varepsilon\) podria ser entre 25 i 75.

Per tant, en els resultats que es presenten a continuació per al clustering per densitat, l'eix horitzontal no representarà el nombre de clústers K (ja que DBSCAN determina el nombre de clústers automàticament), sinó que mostrarà la variació del paràmetre \(\varepsilon\) dins del rang seleccionat per a cada cas. S'analitzarà com canvien el NMAE, el NRMSE i l'entropia normalitzada en funció d'\(\varepsilon\). És important notar que els usuaris identificats com a soroll per DBSCAN (que no pertanyen a cap clúster) seran tractats com un grup separat.

A causa de les diferències significatives en els rangs òptims d'\(\varepsilon\) trobats per als diferents conjunts de dades, els resultats es presentaran en dos blocs separats. El primer bloc (Figura~\ref{fig:dbscan-clustering-results}) se centra en els conjunts \texttt{ml-small} i \texttt{ml-1m}, que comparteixen rangs d'\(\varepsilon\) relativament similars. Posteriorment, s'analitzaran els resultats per als conjunts \texttt{books} i \texttt{jester} (Figura \ref{fig:dbscan2-clustering-results}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figuras/DBSCAN/k-distancia-ml-small-5.png}
    \caption{Exemple de gràfic de k-distància (MovieLens Small, \(\mathit{MinPts}=5\))}
    \label{fig:k-distance-example}
\end{figure}

Observant els resultats per a \texttt{ml-small} i \texttt{ml-1m} a la Figura~\ref{fig:dbscan-clustering-results}, es poden extreure diverses conclusions importants.

Amb la distància Euclidiana (columna esquerra de la Figura~\ref{fig:dbscan-clustering-results}), la precisió (NMAE i NRMSE, Figures~\ref{fig:dbscan-clustering-results-a} i~\ref{fig:dbscan-clustering-results-c}) per a \texttt{ml-small} (línia blava) mostra una lleugera millora (disminució de l'error) a mesura que \(\varepsilon\) augmenta, assolint un mínim al voltant d'\(\varepsilon=75\). Per a \texttt{ml-1m} (línia taronja), la precisió es manté pràcticament constant, amb un error significativament més alt que \texttt{ml-small}, independentment del valor d'\(\varepsilon\) dins del rang explorat. Aquesta diferència en precisió podria atribuir-se a la major densitat i mida de \texttt{ml-1m}, que podria dificultar la formació de clústers ben definits i útils per DBSCAN.

El nombre de clústers trobats amb la distància Euclidiana (Figura~\ref{fig:dbscan-clustering-results-g}) és particularment revelador. Per a ambdós datasets, quan \(\varepsilon\) és petit, es troben inicialment alguns clústers (per exemple, \texttt{ml-1m} troba al voltant de 2.6 clústers de mitjana per a \(\varepsilon \approx 25\), mentre que \texttt{ml-small} troba poc més de 2). No obstant això, a mesura que \(\varepsilon\) augmenta lleugerament (al voltant de 30-35), el nombre de clústers cau dràsticament a 2 per a ambdós (un clúster principal que engloba gairebé tots els punts i el clúster de soroll). Per a \texttt{ml-small}, hi ha un petit repunt on es tornen a identificar més de 2 clústers (2.2) per a \(\varepsilon\) entre 38 i 52, abans de tornar a caure a 2. Aquesta tendència a formar només un clúster principal més el soroll per a la majoria dels valors d'\(\varepsilon\) és un indicador clau que DBSCAN, amb aquests paràmetres i distància Euclidiana, està tenint serioses dificultats per segmentar els usuaris de manera efectiva. Si gairebé tots els usuaris acaben en un únic gran clúster, l'estratègia de clustering no està aportant el valor de segmentació desitjat.

L'entropia amb distància Euclidiana (Figura~\ref{fig:dbscan-clustering-results-e}) reflecteix aquesta situació. Comença relativament alta quan hi ha més clústers (i possiblement més soroll distribuït), però disminueix ràpidament a mesura que el nombre de clústers es redueix a 2, estabilitzant-se en valors molt baixos. Una entropia baixa quan només hi ha 2 agrupacions (el clúster principal i el soroll) indica que la gran majoria dels usuaris pertanyen al clúster principal.

Quan s'utilitza la distància de Pearson (columna dreta de la Figura~\ref{fig:dbscan-clustering-results}), els resultats canvien. La precisió per a \texttt{ml-small} (Figures~\ref{fig:dbscan-clustering-results-b} i~\ref{fig:dbscan-clustering-results-d}, línia blava) es manté estable per a valors baixos d'\(\varepsilon\) (fins a 0.400), després mostra un pic d'empitjorament (augment de l'error) al voltant d'\(\varepsilon=0.425\), per després millorar de nou. Per a \texttt{ml-1m} (línia taronja), la precisió es manté gairebé constant, similar al cas Euclidià.

El nombre de clústers trobats amb la distància de Pearson (Figura~\ref{fig:dbscan-clustering-results-h}) és molt més dinàmic, especialment per a \texttt{ml-1m}. Aquest dataset arriba a formar una mitjana de més de 30 clústers per a \(\varepsilon \approx 0.360\), abans de disminuir ràpidament. \texttt{ml-small} també forma un nombre variable de clústers, arribant a un màxim de 4, però en general troba menys clústers que \texttt{ml-1m} amb Pearson. No obstant això, per a valors d'\(\varepsilon\) alts (superiors a 0.425), ambdós datasets tendeixen a formar només 2 clústers (el principal i el soroll).

L’entropia amb distància de Pearson (Figura~\ref{fig:dbscan-clustering-results-f}) mostra un comportament desalineat respecte al nombre de clústers. De fet, el nombre de clústers per a \texttt{ml-small} arriba a un màxim al voltant d’\(\varepsilon=0.360\) (fins a 4 clústers), mentre que el pic d’entropia es produeix més tard, prop d’\(\varepsilon=0.410\), on l’entropia supera 0,7. Aquesta desalineació significa que, quan realment hi ha més clústers, la distribució dels usuaris entre ells no arriba a l’equilibri òptim, i quan l’entropia és màxima, el nombre de clústers ja ha començat a disminuir. Per a \texttt{ml-1m}, es dona un fenomen semblant: el nombre de clústers és més alt cap a \(\varepsilon=0.360\) (més de 30 clústers), però l’entropia no arriba al seu punt màxim fins a valors d’\(\varepsilon\) superiors, on molts clústers ja han desaparegut. En altres paraules, el moment de màxima dispersió dels usuaris (entropia) no coincideix amb el moment de màxima fragmentació en clústers, fet que indica que no es troba una segmentació bona ni estable.

En general, tant per a \texttt{ml-small} com per a \texttt{ml-1m}, l’algorisme DBSCAN amb distància Euclidiana i amb distància Pearson lluita per generar una segmentació consistent a causa d’aquesta desalineació entre els pics de nombre de clústers i els pics d’entropia. Amb la distància Euclidiana, la tendència és formar un únic gran clúster més el soroll, la qual cosa limita considerablement la utilitat del clustering. Amb la distància de Pearson, tot i que existeixen fines finestres d’\(\varepsilon\) on es formen diversos clústers i l’entropia és relativament elevada, aquestes finestres són molt estretes i no coincideixen amb el punt on realment hi ha més clústers. Això dificulta l’elecció d’un \(\varepsilon\) òptim que proporcioni alhora una bona fragmentació (més clústers) i una bona distribució dels usuaris entre ells (alta entropia). Per tant, la dificultat per obtenir una segmentació robusta suggereix que les estructures de densitat que DBSCAN identifica basant-se en la similitud de Pearson no són prou pronunciades o que aquesta mètrica no s’alinea bé amb les característiques reals dels usuaris per generar clústers útils.


\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_nmae_vs_eps.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:dbscan-clustering-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_pearson_nmae_vs_eps.png}
        \caption{NMAE, distància Pearson}
        \label{fig:dbscan-clustering-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_nrmse_vs_eps.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:dbscan-clustering-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_pearson_nrmse_vs_eps.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:dbscan-clustering-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_entropy_vs_eps.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:dbscan-clustering-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_pearson_entropy_vs_eps.png}
        \caption{Entropia, distància Pearson}
        \label{fig:dbscan-clustering-results-f}
    \end{subfigure}

    %---- CUARTA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_n_clusters_found_vs_eps.png}
        \caption{Mitjana de clústers, distància Euclidiana}
        \label{fig:dbscan-clustering-results-g}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/ml-small_ml-1m_density_pearson_n_clusters_found_vs_eps.png}
        \caption{Mitjana de clústers, distància Pearson}
        \label{fig:dbscan-clustering-results-h}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens Small} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens 1M} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering per densitat (MovieLens)}
    \label{fig:dbscan-clustering-results}
\end{figure}

Continuant amb l'anàlisi del clustering per densitat, la Figura~\ref{fig:dbscan2-clustering-results} presenta els resultats per als conjunts de dades \texttt{books} i \texttt{jester}. Aquests dos conjunts requereixen rangs d'\(\varepsilon\) diferents dels utilitzats per a MovieLens, d'aquí la seva presentació separada.

Amb la distància Euclidiana (columna esquerra de la Figura~\ref{fig:dbscan2-clustering-results}), el conjunt \texttt{books} (línia verda) mostra una lleugera millora en la precisió (NMAE i NRMSE, Figures~\ref{fig:dbscan2-clustering-results-a} i~\ref{fig:dbscan2-clustering-results-c}) a mesura que \(\varepsilon\) augmenta dins del seu rang òptim, assolint els seus millors valors d'error cap al final del rang explorat. Per al conjunt \texttt{jester} (línia vermella), la precisió es manté extremadament estable, amb un error NMAE al voltant de 0.168 i NRMSE al voltant de 0.216, pràcticament independent del valor d'\(\varepsilon\).

El nombre de clústers trobats amb la distància Euclidiana (Figura~\ref{fig:dbscan2-clustering-results-g}) és, novament, un indicador crític. Per a \texttt{jester}, DBSCAN troba consistentment només 2 clústers (el clúster principal i el soroll) per a tots els valors d'\(\varepsilon\) provats. Aquesta incapacitat per segmentar \texttt{jester} en més de dos grups amb Euclidiana explica la seva precisió constant: el sistema es comporta essencialment com si no hi hagués clustering o com si tots els usuaris estiguessin en un gran clúster. Per a \texttt{books}, es troba un nombre més variable de clústers, començant amb més de 8 per a valors petits d'\(\varepsilon\) i disminuint gradualment fins a estabilitzar-se al voltant de 3.8 clústers per a \(\varepsilon\) més alts. Tot i que es formen més de dos clústers, aquest nombre encara és relativament baix.

L'entropia amb distància Euclidiana (Figura~\ref{fig:dbscan2-clustering-results-e}) reflecteix aquesta situació. Per a \texttt{jester}, l'entropia comença molt alta (propera a 1) per a valors petits d'\(\varepsilon\) (quan el soroll podria ser més significatiu o distribuït), però cau dràsticament a mesura que \(\varepsilon\) augmenta, arribant a valors propers a zero. Això confirma que la majoria dels usuaris de \texttt{jester} s'agrupen en el clúster principal. Per a \texttt{books}, l'entropia és generalment baixa, començant al voltant de 0.3 i disminuint a mesura que es formen menys clústers, indicant una distribució desigual dels usuaris fins i tot quan es troben més de dos clústers.

Quan s'utilitza la distància de Pearson (columna dreta de la Figura~\ref{fig:dbscan2-clustering-results}), la precisió per a \texttt{jester} (línia vermella, Figures~\ref{fig:dbscan2-clustering-results-b} i~\ref{fig:dbscan2-clustering-results-d}) continua sent molt estable, amb un lleuger empitjorament cap a la meitat del rang d'\(\varepsilon\). Per a \texttt{books} (línia verda), la precisió es manté força constant per a la major part del rang d'\(\varepsilon\), amb un notable empitjorament (augment de l'error) quan \(\varepsilon\) s'acosta a 0.35, seguit d'una recuperació.

El nombre de clústers trobats amb la distància de Pearson (Figura~\ref{fig:dbscan2-clustering-results-h}) és on s'observen els canvis més dramàtics. El conjunt \texttt{books} arriba a formar un nombre molt elevat de clústers, amb un pic de 120 clústers de mitjana per a \(\varepsilon \approx 0.3\), abans de caure abruptament. Aquesta capacitat de formar tants clústers és única entre totes les combinacions de dataset i mètrica provades amb DBSCAN. Per a \texttt{jester}, amb Pearson, també es forma un nombre variable de clústers, arribant a un màxim d'aproximadament 12 clústers, abans de caure a 2 per a valors més alts d'\(\varepsilon\).

L’entropia amb distància de Pearson (Figura~\ref{fig:dbscan2-clustering-results-f}) per a \texttt{books} no coincideix exactament amb el nombre de clústers. De fet, el nombre de clústers per a \texttt{books} arriba al seu màxim en un valor d’\(\varepsilon\) inferior, mentre que el pic d’entropia (amb valors superiors a 0,7) es produeix posteriorment, a un \(\varepsilon\) més alt. Això indica que quan realment hi ha més clústers, la distribució dels usuaris entre ells encara no és òptima, i quan l’entropia és màxima, el nombre de clústers ja ha començat a disminuir. Per a \texttt{jester}, passa un fenomen semblant: el nombre de clústers arriba al seu màxim en un cert \(\varepsilon\), però el pic d’entropia (al voltant de 0,68) es produeix quan el nombre de clústers ja està en descens. El moment de màxima dispersió (entropia) no coincideix amb el moment de màxima fragmentació en clústers, fet que indica que no es troba una segmentació bona ni estable en aquests punts.

En resum, l’algorisme DBSCAN mostra una gran dificultat per aconseguir una segmentació efectiva i equilibrada dels usuaris en els conjunts de dades reals analitzats. Amb la distància Euclidiana, la tendència predominant és la formació d’un únic gran clúster més el soroll, especialment per a \texttt{jester} i, en gran mesura, per als conjunts MovieLens. Això resulta en una entropia baixa i poc impacte en la precisió, ja que el sistema opera de manera similar a no tenir clustering. Tot i que \texttt{books} amb Euclidiana forma alguns clústers addicionals, la seva distribució continua sent desequilibrada. Amb la distància de Pearson, tot i que es poden arribar a identificar un nombre significativament més gran de clústers en rangs específics d’\(\varepsilon\) (especialment per a \texttt{books} i \texttt{ml-1m}), aquesta segmentació és altament sensible al valor d’\(\varepsilon\) i no ve acompañada en ningún cas de una bona distribució dels usuaris. L’objectiu fonamental del clustering, que és dividir els usuaris en grups significatius i ben distribuïts per millorar les recomanacions, no sembla assolir-se de manera satisfactòria amb DBSCAN en les configuracions explorades. La formació de pocs clústers (sovint només 2, incloent el soroll) o la inestabilitat en el nombre de clústers i la seva distribució (entropia) són indicadors clars d’aquestes limitacions.

Donada la manca de resultats clarament afavoridors (combinació d'un nombre elevat de clústers i una entropia alta que suggereixi una bona segmentació) amb els conjunts de dades reals, s'ha decidit no estendre l'anàlisi del clustering per densitat als conjunts de dades sintètiques. És poc probable que aquests, essent simplificacions, ofereixin un escenari més propici per a DBSCAN si ja amb les dades reals no s'han observat les condicions desitjades per a una aplicació efectiva del clustering en aquest context de sistemes de recomanació.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_nmae_vs_eps.png}
        \caption{NMAE, distància Euclidiana}
        \label{fig:dbscan2-clustering-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_pearson_nmae_vs_eps.png}
        \caption{NMAE, distància Pearson}
        \label{fig:dbscan2-clustering-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_nrmse_vs_eps.png}
        \caption{NRMSE, distància Euclidiana}
        \label{fig:dbscan2-clustering-results-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_pearson_nrmse_vs_eps.png}
        \caption{NRMSE, distància Pearson}
        \label{fig:dbscan2-clustering-results-d}
    \end{subfigure}

    \vspace{1em}

    %---- TERCERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_entropy_vs_eps.png}
        \caption{Entropia, distància Euclidiana}
        \label{fig:dbscan2-clustering-results-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_pearson_entropy_vs_eps.png}
        \caption{Entropia, distància Pearson}
        \label{fig:dbscan2-clustering-results-f}
    \end{subfigure}

    %---- CUARTA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_n_clusters_found_vs_eps.png}
        \caption{Mitjana de clústers, distància Euclidiana}
        \label{fig:dbscan2-clustering-results-g}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/DBSCAN/res/books_jester_density_pearson_n_clusters_found_vs_eps.png}
        \caption{Mitjana de clústers, distància Pearson}
        \label{fig:dbscan2-clustering-results-h}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Book-Crossing} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Jester} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering per densitat (Book-Crossing i Jester)}
    \label{fig:dbscan2-clustering-results}
\end{figure}

\section{Resultats clustering per model}

L'últim enfocament de clustering avaluat és el clustering basat en models, utilitzant específicament el Gaussian Mixture Model (GMM). A diferència dels mètodes anteriors que depenen d'una mètrica de distància explícita (com Euclidiana o Pearson) per a l'agrupació, GMM assumeix que les dades provenen d'una barreja de K distribucions gaussianes i utilitza l'algorisme d'Expectation-Maximization (EM) per estimar els paràmetres d'aquestes distribucions (mitjanes, covariàncies i pesos de barreja). L'assignació final d'un usuari a un clúster es basa en la component gaussiana a la qual té la màxima probabilitat de pertinença (responsabilitat). Per aquest motiu, en els resultats presentats per a GMM (Figura~\ref{fig:gmm-clustering-results}), no hi ha una distinció entre diferents mètriques de distància com en els casos anteriors, ja que la "distància" o afinitat ve implícitament definida per la forma i posició de les gaussianes estimades. S'ha utilitzat una matriu de covariància diagonal per a cada component, una elecció comuna per equilibrar flexibilitat i complexitat del model.

Observant la precisió (NMAE i NRMSE, Figures~\ref{fig:gmm-clustering-results-a} i~\ref{fig:gmm-clustering-results-b}) en funció del nombre de components K, el comportament dels diferents conjunts de dades reals és, en línies generals, bastant similar al que es va observar amb el clustering dur (K-means), especialment quan K-means utilitzava la distància Euclidiana. Per al conjunt \texttt{ml-small} (línia blava), l'error augmenta amb K, especialment en el rang inicial, per després estabilitzar-se a un nivell superior. El conjunt \texttt{ml-1m} (línia taronja) mostra un increment constant de l'error a mesura que K augmenta. El conjunt \texttt{jester} (línia vermella) presenta una precisió molt estable al llarg de tot el rang de K, amb un lleuger augment inicial. Novament, el conjunt \texttt{books} (línia verda) és el que mostra el comportament més favorable en termes de precisió, amb una disminució constant de l'error NMAE i NRMSE a mesura que K augmenta, assolint els valors més baixos entre tots els datasets. Aquesta similitud en les tendències de precisió amb K-means suggereix que, tot i les diferències fonamentals en els algorismes (partició dura basada en distància vs. model probabilístic), la manera com la segmentació en K grups afecta la predicció final (que utilitza la mateixa fórmula basada en similitud de Pearson dins de cada grup) és comparable. Tant K-means com GMM busquen una partició de l'espai, i si les particions resultants són estructuralment similars, l'impacte en la predicció també ho serà.

L'anàlisi de l'entropia normalitzada (Figura~\ref{fig:gmm-clustering-results-c}) revela alguns matisos. Per a \texttt{ml-small}, l'entropia és relativament estable, al voltant de 0.8, indicant una bona distribució dels usuaris entre les components gaussianes. Per a \texttt{ml-1m}, l'entropia comença alta però disminueix gradualment amb K, suggerint que, a mesura que s'intenta modelar amb més components, la distribució dels usuaris es torna menys equilibrada. El conjunt \texttt{jester} mostra una entropia inicialment alta que fluctua però es manté en general per sobre de 0.85, la qual cosa és positiva. El conjunt \texttt{books} presenta un comportament interessant: comença amb una entropia molt alta (propera a 1), cau significativament al voltant de K=15, i després es recupera parcialment, mantenint-se en un nivell respectable però amb més variabilitat que \texttt{ml-small} o \texttt{jester}. Les fluctuacions en l'entropia per a GMM podrien deure's a la naturalesa de l'algorisme EM, que pot convergir a òptims locals, i la forma en què les components gaussianes s'ajusten a la distribució real de les dades pot variar amb K, afectant l'assignació final i, per tant, l'equilibri dels clústers.

En general, els resultats del clustering per model amb GMM, pel que fa a la precisió en funció de K, no ofereixen un avantatge clar respecte al clustering dur amb K-means. Les tendències són molt semblants, amb \texttt{books} sent el cas més positiu on augmentar K millora la precisió. L'entropia obtinguda amb GMM és, en molts casos, comparable o fins i tot lleugerament pitjor que la de K-means amb distància Euclidiana. Això podria indicar que la capacitat de GMM per modelar clústers de formes més flexibles (el·líptiques, donades les covariàncies diagonals) i la seva naturalesa probabilística podrien portar a una assignació d'usuaris una mica més desequilibrada en certs casos. No obstant això, aquest petit empitjorament en l'entropia es tradueix en una millora molt lleugera en la precisió. La complexitat addicional de GMM (estimació de més paràmetres) no sembla justificar-se en termes de rendiment predictiu en comparació amb l'enfocament més simple de K-means per a aquesta aplicació particular i amb l'arquitectura de recomanació utilitzada.


\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-gmm-all.png}
        \caption{NMAE}
        \label{fig:gmm-clustering-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-gmm-all.png}
        \caption{NRMSE}
        \label{fig:gmm-clustering-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-gmm-all.png}
        \caption{Entropia}
        \label{fig:gmm-clustering-results-c}
    \end{subfigure}
    \hfill

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens Small} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{MovieLens 1M} \\
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Book-Crossing} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Jester} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering per model}
    \label{fig:gmm-clustering-results}
\end{figure}

L'aplicació del Gaussian Mixture Model (GMM) als conjunts de dades sintètiques (Figura~\ref{fig:gmm-clustering-sin-results}) permet explorar com aquest mètode basat en models es comporta davant de dades amb característiques de densitat extremes i una naturalesa generada artificialment. Com en els casos anteriors amb GMM, no hi ha distinció per mètrica de distància.

Pel que fa a la precisió (NMAE i NRMSE, Figures~\ref{fig:gmm-clustering-sin-results-a} i~\ref{fig:gmm-clustering-sin-results-b}), els resultats són força consistents amb el que s'ha observat amb altres mètodes de clustering per a aquests mateixos conjunts sintètics. El conjunt de baixa densitat (\texttt{syn03}, línia marró) manté una precisió pràcticament constant i amb errors relativament baixos, independentment del nombre de components K. El conjunt d'alta densitat (\texttt{syn80}, línia rosa) mostra un lleuger però constant increment de l'error a mesura que K augmenta, partint d'un nivell d'error inicial molt baix. El conjunt sintètic base (\texttt{syn}, línia porpra) exhibeix un augment de l'error amb K, especialment per a valors baixos, estabilitzant-se posteriorment en un nivell d'error superior. Aquestes tendències de precisió són notablement similars a les obtingudes amb K-means sobre les mateixes dades sintètiques, reforçant la idea que, per a aquests conjunts generats, la manera com la segmentació en K grups impacta la predicció és bastant robusta entre els dos mètodes, malgrat les seves diferències algorísmiques. La naturalesa potencialment més homogènia o amb menys patrons complexos de les dades sintètiques podria fer que els dos mètodes convergeixin a particions efectivament similars en termes del seu impacte en la predicció.

L'entropia normalitzada (Figura~\ref{fig:gmm-clustering-sin-results-c}) presenta alguns patrons interessants i diferents dels observats amb K-means per a aquests mateixos conjunts sintètics. Per al conjunt sintètic base (\texttt{syn}), l'entropia comença molt alta (propera a 1), però disminueix gradualment a mesura que K augmenta, estabilitzant-se al voltant de 0.77. Això suggereix que GMM, per a aquest conjunt, aconsegueix una distribució inicial molt equilibrada que es torna lleugerament menys equilibrada amb més components. El conjunt de baixa densitat (\texttt{syn03}) mostra un comportament més erràtic: comença amb una entropia baixa (al voltant de 0.4), augmenta ràpidament fins a un pic proper a 0.85 per a K=5, per després disminuir de manera més o menys constant fins a valors al voltant de 0.65. Aquesta variabilitat podria indicar dificultats de l'algorisme EM per ajustar de manera estable les components gaussianes quan les dades són molt disperses i sintètiques. El conjunt d'alta densitat (\texttt{syn80}) és el que manté l'entropia més alta i estable, gairebé sempre per sobre de 0.82, la qual cosa indica que GMM aconsegueix una molt bona distribució dels usuaris entre les components per a aquest conjunt. Això contrasta amb alguns resultats de K-means on l'entropia podia ser més baixa per a \texttt{syn80} amb certes mètriques. La capacitat de GMM per modelar la densitat de probabilitat podria ser particularment efectiva quan les dades són molt denses, permetent un ajust més natural i equilibrat de les components.

En conclusió, l'aplicació de GMM a les dades sintètiques mostra que, en termes de precisió, el seu comportament és molt similar al de K-means: poc impacte o empitjorament amb K, depenent de la densitat del conjunt sintètic. No obstant això, GMM demostra una capacitat generalment bona per mantenir una entropia elevada, especialment per al conjunt d'alta densitat (\texttt{syn80}) i, després d'un ajust inicial, per al conjunt base (\texttt{syn}). Això suggereix que l'enfocament basat en models pot ser més robust per aconseguir una distribució equilibrada dels usuaris en aquests escenaris sintètics, fins i tot si això no es tradueix en una millora de la precisió de les recomanacions. La naturalesa de les dades sintètiques, possiblement amb estructures menys complexes o més "netes" que les reals, podria afavorir l'ajust de models probabilístics com GMM en termes de la qualitat de la partició (entropia), encara que el benefici final per a la predicció romangui limitat per les mateixes raons que afecten altres mètodes de clustering en aquests conjunts.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nmae-gmm-sin.png}
        \caption{NMAE}
        \label{fig:gmm-clustering-sin-results-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/nrmse-gmm-sin.png}
        \caption{NRMSE}
        \label{fig:gmm-clustering-sin-results-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/entropy-gmm-sin.png}
        \caption{Entropia}
        \label{fig:gmm-clustering-sin-results-c}
    \end{subfigure}
    \hfill

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customPurple, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (base)} \\
            \tikz{\draw[customBrown, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (baixa densitat)} \\
            \tikz{\draw[customPink, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Sintètic (alta densitat)} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats clustering per model (dades sintètiques)}
    \label{fig:gmm-clustering-sin-results}
\end{figure}

\section{Síntesi i comparació entre mètodes}

Després d'analitzar individualment cada mètode de clustering, és útil realitzar una comparativa global per identificar fortaleses, debilitats i patrons comuns. La Taula~\ref{tab:resum-clustering-global} presenta un resum qualitatiu del comportament general de cada algorisme sobre els conjunts de dades reals, considerant la precisió (NMAE/NRMSE) i la qualitat de la segmentació (Entropia i Nombre de Clústers per a DBSCAN).

\begin{table}[H]
    \centering
    \small % Reduir la mida de la font per a millor ajust
    \begin{tabular}{|p{2.5cm}|p{3cm}|p{4.5cm}|p{4.5cm}|}
    \hline
    \textbf{Mètode de Clustering} & \textbf{Mètrica Dist./Config.} & \textbf{Precisió (NMAE/NRMSE)} & \textbf{Segmentació (Entropia / Nº Clústers)} \\ \hline
    
    \multirow{2}{=}{K-means (Dur)} & Euclidiana & Resultats mixts. Millora per \texttt{books} amb K alt. Empitjora per \texttt{ml-1m}. Poca variació per \texttt{jester}. Petita pèrdua per \texttt{ml-small} amb K alt. & Entropia generalment decreixent amb K, però acceptablement alta per a la majoria. \\ \cline{2-4}
     & Pearson & Empitjora la precisió en general vs Euclidiana. \texttt{books} encara és el millor però amb menys millora. & Problemes seriosos d'entropia per \texttt{books} (molt baixa).Entropia generalment més alta que amb Euclidiana per a la majoria de conjunts. \\ \hline
     
    \multirow{2}{=}{FCM + COG (Difús)} & Euclidiana & Resultats variats. Millora clara per \texttt{books} amb K. Error augmenta per \texttt{ml-small}. Estable per \texttt{jester}, \texttt{ml-1m}. & Bona entropia per \texttt{ml-small}, \texttt{books}. Baixa i variable per \texttt{jester}. Decreixent per \texttt{ml-1m}. \\ \cline{2-4}
     & Pearson & Precisió inestable/oscil·latòria per \texttt{ml-small}, \texttt{books}. Estable per \texttt{ml-1m}, \texttt{jester}. & Entropia extremadament baixa (propera a 0) per \texttt{ml-small}, \texttt{ml-1m}, \texttt{jester} (quasi tots a 1 clúster). Millor per \texttt{books} (augmenta amb K). \\ \hline
     
    \multirow{2}{=}{HAC (Jeràrquic - Average Linkage)} & Euclidiana & Millora per \texttt{books}. Augment d'error per \texttt{ml-small}, \texttt{ml-1m}. Estable per \texttt{jester}. & Entropia acceptable per \texttt{ml-small} i \texttt{books}. Entropia baixa per \texttt{ml-1m} i \texttt{jester}. \\ \cline{2-4}
     & Pearson & Precisió notablement millor per \texttt{ml-small} que Euclidiana. Similar per \texttt{ml-1m} i \texttt{books}. lleugerament pitjor per \texttt{jester}. & Entropia generalment molt millor que amb Euclidiana, augmenta amb K per a la majoria. Per a \texttt{jester} disminueix. \\ \hline
     
    \multirow{2}{=}{DBSCAN (Densitat)} & Euclidiana (\(\varepsilon\) variable) & Precisió bastant estable amb petites fluctuacions atribuïdes a variacions en el nombre de clústers i entropia. & Tendeix a formar molt pocs clústers (sovint 1 + soroll). Entropia generalment baixa reflectint això. \\ \cline{2-4}
     & Pearson (\(\varepsilon\) variable) & Comportament similar a Euclidiana. Precisió generalment estable amb oscil·lacions lleus degudes a canvis en segmentació. & Nombre de clústers i entropia molt sensibles a \(\varepsilon\). Pot formar molts clústers (\texttt{books}, \texttt{ml-1m}) però en finestres estretes i amb entropia no sempre òptima. \\ \hline
     
    GMM (Model) & N/A (basat en probabilitat) & Comportament de precisió molt similar a K-means amb Euclidiana. Millora per \texttt{books}, empitjora/estable per altres. & Entropia generalment bona i estable. \\ \hline
    
    \end{tabular}
    \caption{Resum qualitatiu comparatiu dels mètodes de clustering sobre dades reals.}
    \label{tab:resum-clustering-global}
\end{table}

\textbf{Anàlisi de la Taula Resum}
A continuació es presenten les principals conclusions extretes de la Taula~\ref{tab:resum-clustering-global}:

\begin{itemize}
  \item \textbf{Impacte de la mètrica de distància.}
    L’elecció entre la distància euclidiana i la de Pearson afecta de manera significativa els resultats, especialment en els algorismes K-means, FCM i HAC:
    \begin{itemize}
      \item \textbf{K-means:} l’ús de la distància de Pearson tendeix a empitjorar la precisió. En el cas del dataset \texttt{books}, va comportar una entropia molt baixa i va degradar la segmentació.
      \item \textbf{FCM+COG:} la distància de Pearson va resultar especialment problemàtica, produint entropies extremadament baixes en la majoria de datasets i invalidant la cohesió dels grups.
      \item \textbf{HAC:} a diferència dels altres mètodes, la distància de Pearson va millorar notablement la distribució dels elements entre clusters (entropia més alta) i, en el cas de \texttt{ml-small}, va augmentar també la precisió.
      \item \textbf{DBSCAN:} cap de les dues mètriques no va aconseguir una segmentació consistent; tanmateix, Pearson va permetre generar més clusters en intervals molt concrets de l’\(\varepsilon\).
    \end{itemize}

  \item \textbf{Comportament del dataset \texttt{books}.}
    Aquest conjunt de dades mostra característiques singulars respecte als altres:
    \begin{itemize}
      \item Increment de la precisió amb l’augment de \(K\) en K-means (euclidiana), FCM (euclidiana) i HAC (ambdues distàncies).
      \item GMM també registra una millora de la precisió.
    \end{itemize}
    Aquestes variacions indiquen que la baixa densitat, l’ampli rang de valoracions i la naturalesa dels ítems de \texttt{books} faciliten una segmentació més beneficiosa per aquests algorismes.

  \item \textbf{Precisió vs. qualitat de la segmentació.}
    Rarament es troba una combinació que maximitzi simultàniament la precisió i una segmentació de qualitat (entropia alta i nombre de clusters raonable). Sovint, augmentar la precisió comporta entropies més baixes o pocs grups ben definits.

  \item \textbf{Limitacions de DBSCAN.}
    DBSCAN presenta una gran sensibilitat al paràmetre \(\varepsilon\):
    \begin{itemize}
      \item Tendència a generar un únic cluster majoritari més soroll.
      \item Necessita un ajustament molt fi per a obtenir múltiples clusters estables.
    \end{itemize}
    Això el fa menys pràctic sense una calibració molt específica per a cada dataset.

  \item \textbf{Cap mètode universal.}
    No existeix cap tècnica de clustering que superi de forma consistent totes les mètriques en tots els datasets. La selecció del mètode i la seva configuració han de basar-se en:
    \begin{itemize}
      \item Les característiques concretes de cada conjunt de dades.
      \item Els objectius prioritaris (precisió vs. qualitat de la segmentació).
    \end{itemize}
\end{itemize}

En resum, tot i que el clustering pot ajudar a gestionar la complexitat i a identificar subgrups rellevants d’usuaris, la seva implementació pràctica en sistemes de recomanació resulta exigent. Els guanys en precisió acostumen a ser moderats i dependents del dataset, i aconseguir una segmentació equilibrada i útil requereix un ajustament acurat dels paràmetres i una avaluació específica per a cada cas. 

Després d'una anàlisi detallada de cada tècnica de clustering i les seves variants, aquesta secció presenta una comparativa final del seu rendiment en termes de precisió (NMAE) sobre els diferents conjunts de dades reals. Per a aquesta anàlisi final, s'han seleccionat només aquelles configuracions d'algorismes que van demostrar una capacitat mínima de segmentació, considerant com a llindar una entropia normalitzada mitjana superior a 0.5 en la majoria del rang de K (o \(\varepsilon\) per a DBSCAN). Aquest criteri busca assegurar que els mètodes comparats no només ofereixin una certa precisió, sinó que també realitzin una divisió mínimament equilibrada dels usuaris, ja que una entropia molt baixa indicaria que gairebé tots els usuaris acaben en un sol clúster, invalidant el propòsit del clustering. Per aquest motiu, s'han exclòs d'aquesta comparativa final les variants de K-means amb Pearson per a \texttt{books}, FCM+COG amb Pearson per a \texttt{ml-small}, \texttt{ml-1m} i \texttt{jester}, i l'algorisme DBSCAN en general, ja que van presentar problemes severs d'entropia o de formació de molt pocs clústers en la majoria de les seves configuracions.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/FinalResults/nmae-pear-ml-small.png}
        \caption{MovieLens Small}
        \label{fig:nmae-ml-small-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/FinalResults/nmae-pear-ml-1m.png}
        \caption{MovieLens 1M}
        \label{fig:nmae-ml-1m-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/FinalResults/nmae-pear-books.png}
        \caption{Book-Crossing}
        \label{fig:nmae-books-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/FinalResults/nmae-pear-jester.png}
        \caption{Jester}
        \label{fig:nmae-jester-d}
    \end{subfigure}

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{K-means Euclidiana} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Kmeans Pearson} \\
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Fuzzy C-means Euclidiana} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Fuzzy C-means Pearson} \\
            \tikz{\draw[customPurple, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{HAC Euclidiana} \\
            \tikz{\draw[customBrown, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{HAC Pearson} \\
            \tikz{\draw[customPink, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{GMM} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats finals de NMAE}
    \label{fig:nmae-final-results}
\end{figure}

La Figura~\ref{fig:nmae-final-results} mostra l'evolució del NMAE per als mètodes de clustering restants en cada un dels quatre conjunts de dades reals.

Per al conjunt de dades \textbf{\texttt{MovieLens Small}} (Figura~\ref{fig:nmae-ml-small-a}), s'observa que la majoria dels mètodes presenten un increment de l'error NMAE a mesura que augmenta el nombre de clústers K. K-means amb distància Euclidiana (línia blava) i GMM (línia rosa) tenen un comportament molt similar, amb un error que creix fins a estabilitzar-se al voltant de 0.17. FCM amb Euclidiana (línia verda) també segueix aquesta tendència, tot i que amb un error lleugerament superior. HAC amb distància Euclidiana (línia porpra) mostra un increment d'error més pronunciat. La variant més destacada en aquest conjunt és HAC amb distància de Pearson (línia marró), que, tot i començar amb un error similar als altres, aconsegueix mantenir un NMAE significativament més baix (al voltant de 0.156) a mesura que K augmenta, sent la millor opció en termes de precisió per a \texttt{ml-small} entre els mètodes que van superar el filtre d'entropia. K-means amb Pearson (línia taronja) també mostra un augment de l'error, assolint valors similars a les pitjors configuracions.

En el cas de \textbf{\texttt{MovieLens 1M}} (Figura~\ref{fig:nmae-ml-1m-b}), on menys mètodes van superar el filtre d'entropia, K-means amb Euclidiana (línia blava) i GMM (línia rosa) tornen a mostrar un comportament molt similar, amb un NMAE que augmenta amb K, arribant a valors propers a 0.191. FCM amb Euclidiana (línia verda) presenta un error inicial més alt però una tendència més plana, acabant amb un NMAE al voltant de 0.183, que és millor que K-means Euclidiana i GMM per a K elevats. K-means amb Pearson (línia taronja) és el que obté l'error més alt, superant 0.194. Per a aquest dataset, sembla que mantenir un nombre de clústers baix o utilitzar FCM amb Euclidiana podria ser preferible si es prioritza la precisió.

Per al conjunt de dades \textbf{\texttt{Book-Crossing}} (Figura~\ref{fig:nmae-books-c}), el panorama és més divers. K-means amb Euclidiana (línia blava) mostra una clara millora de la precisió a mesura que K augmenta, assolint el NMAE més baix de totes les combinacions, al voltant de 0.140. FCM amb Euclidiana (línia verda) i GMM (línia rosa) segueixen una tendència similar de millora, però amb errors lleugerament superiors, estabilitzant-se al voltant de 0.143. HAC amb Euclidiana (línia porpra) també millora amb K, quedant-se entre els millors. En canvi, FCM amb Pearson (línia vermella) i HAC amb Pearson (línia marró), tot i superar el filtre d'entropia per a aquest dataset, mostren un comportament més erràtic o una precisió inferior, amb NMAE al voltant de 0.146-0.148. Per a \texttt{books}, els mètodes basats en Euclidiana (K-means, FCM, HAC) i GMM són clarament superiors.

Finalment, per al conjunt \textbf{\texttt{Jester}} (Figura~\ref{fig:nmae-jester-d}), només tres configuracions van superar el filtre d'entropia de manera consistent. K-means amb Euclidiana (línia blava) i GMM (línia rosa) mostren un comportament similar, amb un NMAE que augmenta lleugerament amb K. No obstant això, GMM (línia rosa) aconsegueix mantenir un NMAE lleugerament inferior a K-means Euclidiana (línia blava) al llarg de gairebé tot el rang de K, partint ambdós d'un NMAE proper a 0.168 i acabant GMM al voltant de 0.170 i K-means Euclidiana al voltant de 0.171. K-means amb Pearson (línia taronja) presenta un NMAE significativament més alt, augmentant des de 0.169 fins a gairebé 0.178. Per a \texttt{jester}, GMM sembla oferir un petit avantatge en precisió sobre K-means Euclidiana, sent ambdós considerablement millors que K-means Pearson.

En síntesi, cap mètode de clustering ni configuració de distància emergeix com a universalment superior per a tots els conjunts de dades. Per a \texttt{ml-small}, HAC amb Pearson va oferir la millor precisió. Per a \texttt{ml-1m}, FCM amb Euclidiana va ser relativament millor per a K alts. Per a \texttt{books}, K-means amb Euclidiana va ser el guanyador clar. Per a \texttt{jester}, GMM va mostrar el millor rendiment, superant lleugerament K-means amb Euclidiana. Aquests resultats reforcen la idea que l'elecció òptima del mètode de clustering és altament dependent de les característiques específiques del conjunt de dades. La interacció entre l'algorisme de clustering, la mètrica de distància (quan aplica), i la naturalesa de les dades (densitat, escala de valoració, distribució de preferències) determina quina aproximació aconsegueix una segmentació que, en ser utilitzada per la fórmula de predicció, resulta en una millor o pitjor precisió. L'objectiu principal del clustering en aquest context no és necessàriament millorar la precisió respecte a un sistema no clusteritzat, sinó més aviat reduir la complexitat computacional del problema de recomanació restringint la cerca de veïns a subconjunts més petits d'usuaris. Tot i que s'ha intentat assegurar una segmentació mínimament útil mitjançant el filtre d'entropia, s'observa que aquesta reducció de complexitat sovint ve acompanyada d'una pèrdua de precisió, o, en el millor dels casos, d'una precisió comparable a la que s'obtindria amb pocs clústers o sense. Per exemple, per a \texttt{ml-small}, HAC amb Pearson ofereix una precisió relativament bona amb un nombre creixent de clústers. Per a \texttt{books}, K-means Euclidiana millora la precisió a mesura que segmenta més. No obstant això, per a \texttt{ml-1m} i \texttt{jester}, la tendència general és que una major segmentació (més clústers) no millora la precisió i sovint l'empitjora. Això planteja una qüestió fonamental sobre el compromís (trade-off) entre la reducció de la complexitat que ofereix el clustering i la potencial pèrdua de precisió. La decisió d'implementar una estratègia de clustering haurà de valorar si la disminució en el cost computacional o la possibilitat d'analitzar grups d'usuaris més petits compensa una eventual disminució en l'exactitud de les prediccions.escenaris.

\begin{figure}[H]
    \centering
    %---- PRIMERA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/FinalResults/nmae-pear-syn.png}
        \caption{Sintètic (base)}
        \label{fig:nmae-syn-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/FinalResults/nmae-pear-syn03.png}
        \caption{Sintètic (baixa densitat)}
        \label{fig:nmae-syn03-b}
    \end{subfigure}

    \vspace{1em}

    %---- SEGONA FILA ----%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figuras/FinalResults/nmae-pear-syn80.png}
        \caption{Sintètic (alta densitat)}
        \label{fig:nmae-syn80-c}
    \end{subfigure}
    \hfill

    %---- LLEGENDA COMUNA ----%
    \vspace{1em}
    \begin{minipage}{0.8\textwidth}
        \centering
        \footnotesize
        \textbf{Llegenda dades:}  
        \begin{tabular}{@{}ll@{}}
            \tikz{\draw[customBlue, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{K-means Euclidiana} \\
            \tikz{\draw[customOrange, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Kmeans Pearson} \\
            \tikz{\draw[customGreen, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Fuzzy C-means Euclidiana} \\
            \tikz{\draw[customRed, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{Fuzzy C-means Pearson} \\
            \tikz{\draw[customPurple, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{HAC Euclidiana} \\
            \tikz{\draw[customBrown, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{HAC Pearson} \\
            \tikz{\draw[customPink, line width=2pt] (0,0) -- (1.5cm,0);} & \texttt{GMM} \\
        \end{tabular}
    \end{minipage}

    \caption{Resultats finals de NMAE (dades sintètiques)}
    \label{fig:nmae-sin-final-results}
\end{figure}

L'anàlisi dels resultats de NMAE sobre els conjunts de dades sintètiques (Figura~\ref{fig:nmae-sin-final-results}), després d'aplicar el filtre d'entropia ($> 0.5$), revela comportaments distintius que reflecteixen les característiques extremes d'aquests conjunts generats.

Per al conjunt \textbf{\texttt{Sintètic (base)}} (Figura~\ref{fig:nmae-syn-a}), que busca emular les propietats del \texttt{ml-small}, s'observa un comportament generalment similar al del seu homòleg real. La majoria dels mètodes de clustering mostren un increment de l'error NMAE a mesura que augmenta el nombre de clústers K. K-means amb Euclidiana (línia blava), K-means amb Pearson (línia taronja), GMM (línia rosa) i HAC amb Euclidiana (línia porpra) segueixen aquesta tendència d'empitjorament de la precisió amb més segmentació. FCM amb Euclidiana (línia verda) presenta més variabilitat, però també tendeix a augmentar l'error. Similar al cas de \texttt{ml-small} real, HAC amb Pearson (línia marró) destaca per oferir el NMAE més baix i més estable al llarg de K, suggerint que, fins i tot en dades sintètiques amb característiques similars a \texttt{ml-small}, aquesta combinació específica de clustering jeràrquic amb distància de Pearson aconsegueix una segmentació que resulta menys perjudicial, i fins i tot lleugerament millor, per a la precisió.

En el cas del conjunt \textbf{\texttt{Sintètic (baixa densitat)}} (\texttt{syn03}, Figura~\ref{fig:nmae-syn03-b}), els resultats són notablement diferents. Pràcticament tots els mètodes de clustering que van superar el filtre d'entropia (K-means Euclidiana, FCM Euclidiana, HAC Euclidiana, i GMM) convergeixen a un valor de NMAE gairebé idèntic i constant, al voltant de 0.11375, independentment del nombre de clústers K (excepte per a K molt baixos on hi ha una petita variació inicial). Aquesta precisió extremadament estable i idèntica entre mètodes es pot atribuir a la manera com el sistema de predicció gestiona la manca d'informació. En un entorn de densitat tan baixa (0,29\,\%), és molt probable que per a moltes prediccions no es trobin veïns dins del clúster que hagin valorat l'ítem en qüestió. En aquests casos, tal com es va definir l'arquitectura del sistema, la predicció retorna la mitjana de les valoracions de l'usuari actiu. Si aquesta situació és predominant, l'algorisme de clustering té molt poc impacte, ja que la predicció es basa principalment en una heurística simple i no en la col·laboració dins del clúster. La petita fluctuació de FCM Euclidiana per a K=55 podria ser un artefacte o una inestabilitat puntual.

Finalment, per al conjunt \textbf{\texttt{Sintètic (alta densitat)}} (\texttt{syn80}, Figura~\ref{fig:nmae-syn80-c}), on la densitat és del 83,9\,\% i els usuaris han valorat molts ítems, tots els mètodes de clustering que superen el filtre d'entropia mostren un increment de l'error NMAE a mesura que K augmenta. Aquest comportament és esperable: amb tanta informació disponible per a cada usuari, el sistema de recomanació sense una segmentació excessiva (o amb K baix) ja és capaç de fer prediccions molt precises. La segmentació addicional en múltiples clústers sembla diluir la informació i degradar lleugerament la precisió. K-means amb Euclidiana (línia blava) i K-means amb Pearson (línia taronja) tenen tendències similars d'augment. FCM amb Pearson (línia vermella) mostra un increment més pronunciat. És destacable que GMM (línia rosa) aconsegueix mantenir el NMAE més baix de manera consistent al llarg de tot el rang de K, tot i que també mostra una tendència a l'augment. Això suggereix que la capacitat de GMM per modelar la distribució de les dades podria ser particularment avantatjosa en aquest escenari d'alta densitat, aconseguint una segmentació que, tot i no millorar la precisió respecte a K baixos, és la menys perjudicial entre les opcions.

\chapter{Conclusions}
\label{chap:conclusions}

Aquest capítol final resumeix les principals troballes del projecte, avalua l'assoliment dels objectius establerts, proposa línies de treball futur i reflexiona sobre l'experiència personal i acadèmica adquirida durant la realització d'aquest treball de final de grau.

\section{Objectius assolits}
\label{sec:objectius_assolits}

El present treball de final de grau s'ha centrat en el disseny, la implementació i l'anàlisi comparativa de diverses tècniques de clustering aplicades com a pas previ al filtratge col·laboratiu en sistemes de recomanació. A continuació, s'avalua el grau d'assoliment dels objectius plantejats inicialment:

\begin{itemize}
    \item \textbf{Implementació d'un mòdul de clustering basat en tècniques particionals clàssiques, difuses, jeràrquiques, basades en densitat i en models:} S'han implementat i avaluat amb èxit els algorismes K-means (clustering dur), Fuzzy C-Means amb assignació dura (clustering difús), Aglomeratiu Jeràrquic (HAC), DBSCAN (clustering per densitat) i Gaussian Mixture Models (GMM). Cada algorisme s'ha provat amb diferents configuracions, incloent-hi l'ús de mètriques de distància Euclidiana i de Pearson (quan aplicable) i variant el nombre de clústers (K) o el paràmetre \(\varepsilon\) (per a DBSCAN).

    \item \textbf{Anàlisi i comprensió del funcionament i les particularitats de cada mètode de clustering:} L'anàlisi detallada dels resultats, presentada al Capítol \ref{chap:analisi_resultats}, ha permès identificar les fortaleses i debilitats de cada tècnica en diferents escenaris. S'ha observat que no existeix un mètode universalment superior; el rendiment depèn fortament de les característiques del conjunt de dades (densitat, escala de valoració, nombre d'usuaris/ítems) i de la mètrica de distància emprada. Per exemple, mentre que K-means i GMM van mostrar comportaments de precisió similars, HAC amb distància de Pearson va destacar per al conjunt \texttt{ml-small}, i K-means amb Euclidiana per a \texttt{books}. DBSCAN, en canvi, va presentar dificultats significatives per generar una segmentació útil i equilibrada en la majoria dels casos.

    \item \textbf{Avaluació, mitjançant mètriques específiques, de la precisió, l'eficiència computacional i la robustesa de cada enfocament:} La precisió s'ha avaluat exhaustivament mitjançant NMAE i NRMSE, i la qualitat de la segmentació amb l'entropia normalitzada. S'ha constatat que, si bé l'objectiu principal del clustering en aquest context pot ser la reducció de la complexitat computacional (restringint la cerca de veïns a clústers més petits), això sovint comporta una pèrdua de precisió o, en el millor dels casos, una precisió comparable a la que s'obtindria amb pocs clústers. La robustesa s'ha explorat mitjançant l'ús de múltiples conjunts de dades reals i sintètics, revelant la sensibilitat dels algorismes a aquestes variacions. L'eficiència computacional, tot i no ser el focus principal de les mètriques presentades gràficament, ha estat una consideració en el disseny i en la discussió de la complexitat dels algorismes.

    \item \textbf{Contribució al coneixement en l'àmbit dels sistemes de recomanació:} Aquest treball aporta una comparativa sistemàtica i rigorosa de múltiples tècniques de clustering, oferint una visió detallada de com la seva aplicació prèvia al filtratge col·laboratiu impacta el rendiment. S'ha evidenciat la importància crítica de l'elecció de la mètrica de distància i la necessitat d'avaluar no només la precisió sinó també la qualitat de la segmentació (entropia). Els resultats subratllen que l'aplicació de clustering no garanteix una millora en la precisió i pot, en alguns casos, degradar-la, la qual cosa planteja un compromís important entre la reducció de la complexitat i el manteniment del rendiment predictiu.
\end{itemize}

En resum, s'han assolit els objectius principals del projecte, proporcionant una anàlisi exhaustiva i documentada que pot servir de referència per a futures investigacions i desenvolupaments en l'àrea dels sistemes de recomanació híbrids basats en clustering.

\section{Treball futur}
\label{sec:treball_futur}

Tot i l'abast de l'anàlisi realitzada, aquest treball obre diverses línies d'investigació i millora que podrien abordar-se en el futur:

\begin{itemize}
    \item \textbf{Exploració d'altres algorismes de clustering:} Es podrien investigar altres famílies d'algorismes de clustering, com ara el clustering espectral, Affinity Propagation, o tècniques més avançades basades en densitat (com OPTICS o HDBSCAN, donades les limitacions trobades amb DBSCAN) per veure si ofereixen millors segmentacions o precisió.

    \item \textbf{Integració amb altres mètodes de recomanació:} Investigar com aquestes tècniques de clustering interactuen amb altres paradigmes de recomanació, com ara el filtratge basat en contingut o mètodes de factorització de matrius aplicats dins de cada clúster.

    \item \textbf{Aplicació a conjunts de dades més grans i escalabilitat:} Provar les tècniques en conjunts de dades de major volum i explorar solucions per millorar l'escalabilitat dels algorismes de clustering més costosos (per exemple, utilitzant versions aproximades o distribuïdes). Això inclouria l'ús de matrius disperses de manera més generalitzada per gestionar la mida de les dades.

    \item \textbf{Clustering dinàmic i incremental:} Investigar com adaptar aquestes tècniques a entorns dinàmics on els usuaris, ítems i valoracions canvien amb el temps, explorant algorismes de clustering incremental.
\end{itemize}

Aquestes propostes podrien aprofundir en la comprensió de la interacció entre el clustering i els sistemes de recomanació, i potencialment conduir a sistemes més eficients i efectius.

\section{Impacte i valoració personal}
\label{sec:valoracio_personal}

La realització d'aquest Treball de Final de Grau ha representat una experiència d'aprenentatge molt significativa tant a nivell personal com acadèmic. Des d'una perspectiva personal, el projecte ha suposat un repte considerable que ha requerit dedicació, organització i perseverança. La gestió del temps, la resolució autònoma de problemes tècnics i la necessitat d'aprofundir en conceptes complexos d'aprenentatge automàtic han estat aspectes clau que han contribuït al meu creixement i maduresa com a futur enginyer.

Acadèmicament, el treball m'ha permès consolidar i aplicar coneixements adquirits durant el Grau en Enginyeria Informàtica, especialment en les àrees d'Intel·ligència Artificial, Aprenentatge Automàtic i Anàlisi de Dades. La necessitat de comprendre en detall el funcionament intern de diversos algorismes de clustering, les mètriques d'avaluació i els fonaments dels sistemes de recomanació ha enriquit enormement la meva formació. La implementació pràctica d'aquests conceptes utilitzant Python i llibreries especialitzades com Scikit-learn, Pandas, NumPy i Surprise, així com la redacció de la memòria amb LaTeX, han estat exercicis molt valuosos.

Un dels aprenentatges més importants ha estat la constatació de la complexitat inherent a l'avaluació i comparació de tècniques d'aprenentatge automàtic en escenaris reals. Els resultats obtinguts han demostrat que no hi ha solucions universals i que l'elecció de la tècnica més adequada depèn intrínsecament de les característiques del problema i de les dades disponibles. Comprendre que la teoria sovint requereix adaptacions i anàlisis curoses quan s'aplica a la pràctica ha estat una lliçó fonamental.

L'anàlisi dels resultats, en particular, ha estat un procés iteratiu de formulació d'hipòtesis, experimentació i interpretació, que m'ha ensenyat la importància del rigor metodològic i del pensament crític. Veure com diferents configuracions i algorismes produïen resultats diversos i, de vegades, contraintuïtius, ha reforçat la idea que l'anàlisi de dades és tant una ciència com un art.

Finalment, considero que aquest treball no només compleix els requisits acadèmics, sinó que també m'ha proporcionat una base sòlida i una motivació per continuar explorant el fascinant camp de la intel·ligència artificial i els sistemes intel·ligents en la meva futura carrera professional.

\part{Memoria de Gestió}

\chapter{Planificació temporal}

En aquesta secció es realitzarà una planificació temporal del projecte definint tasques específiques i blocs de tasques, per tal d'obtenir una estimació realista de la seva durada.
El treball s'inicia el 27 de gener i la presentació està prevista per al 25 de juny, de manera que aquesta seria la data màxima de finalització.

El projecte es desenvoluparà durant aproximadament 140 dies, amb una durada estimada de 450 hores, equivalent als 18 crèdits ECTS que conformen el treball.
Aquesta quantitat d'hores es té en compte per calcular el temps que s'ha de dedicar a cada activitat, tal com es mostra a la Taula \ref{tab:tasques}.
La dedicació diària de dilluns a divendres serà aproximadament de 3-4 hores, i els caps de setmana es podrà dedicar més temps.
Es disposarà d'una gran flexibilitat ja que el projecte es realitza íntegrament des de casa.
Aquestes hores es podran destinar tant a la documentació com a la programació segons la necessitat.

\section{Descripció de les tasques}

En aquesta secció es descriuran totes les tasques que conformen el projecte.
Les tasques més grans es dividiran en subtasques per facilitar la comprensió de les diferents fases.
Finalment, es presentarà una taula resum de totes les activitats amb la seva descripció, durada, dependències, recursos i rols assignats, així com el diagrama de Gantt.

\subsection{GP - Gestió del Projecte}

Una gestió eficient és essencial per assegurar-ne l’èxit.
Aquesta proporciona un marc estructurat que facilita l’assoliment dels objectius de manera organitzada, dins dels límits establerts de temps, recursos i pressupost.
A més, una bona gestió permet minimitzar riscos, garantir la qualitat i adaptar-se a possibles imprevistos.
La duració total estimada per a la gestió del projecte és de \textbf{65 hores}. Aquest apartat es divideix en diverses fases clau, que es detallen a continuació.

\begin{itemize}
    \item \textbf{GP1 - Contextualització i Abast del projecte}
    
    Una bona definició de l’abast del projecte és fonamental per garantir-ne l’èxit.
    En aquesta fase inicial es determinen els objectius, els requisits, les possibles dificultats i els riscos associats.
    A més, cal haver recopilat informació prèvia sobre les tecnologies i metodologies que es volen aplicar.
    Es calcula que aquesta tasca requerirà aproximadament \textbf{15 hores}.

    \item \textbf{GP2 - Planificació temporal}
    
    Per tal de garantir una execució òptima, cal establir una planificació temporal adequada, identificant les tasques específiques, assignant responsabilitats i establint terminis clars.
    Això permet una millor organització del treball, minimitzant possibles retards i optimitzant els recursos.
    Es calcula que aquesta tasca requerirà aproximadament \textbf{15 hores}.

    \item \textbf{GP3 - Gestió Econòmica i sostenibilitat}
    
    Es durà a terme una anàlisi detallada dels costos associats al projecte, incloent tant les despeses de personal com els materials necessaris.
    A més, es tindran en compte possibles imprevistos i costos indirectes.
    També es realitzarà un informe sobre l’impacte ambiental, social i econòmic del projecte per assegurar-ne la sostenibilitat.
    En total, es preveu una dedicació de \textbf{15 hores} per aquesta tasca.

    \item \textbf{GP4 - Seguiment del projecte}
    
    El seguiment del projecte és essencial per garantir que es compleixen els objectius i els terminis establerts.
    Es realitzaran reunions quinzenals amb el supervisor per revisar l’evolució del projecte, resoldre possibles problemes i ajustar la planificació si cal.
    Aquestes trobades també permetran rebre orientació i millorar la presa de decisions.
    S’estima que es destinaran \textbf{20 hores} a aquesta fase.
\end{itemize}

\subsection{TP - Treball Previ}

El Treball Previ constitueix la base teòrica i tècnica del projecte.
Aquesta fase s’articula en tres parts fonamentals:

\begin{itemize}
    \item \textbf{TP1 - Estudi sobre sistemes de recomanació}
    
    Es realitzarà una revisió exhaustiva de la literatura existent per comprendre els principis bàsics, les metodologies actuals i les tendències en el desenvolupament de sistemes de recomanació.
    Aquesta anàlisi permetrà identificar els punts forts i les limitacions dels enfocaments tradicionals.
    Es calcula que aquesta tasca requerirà aproximadament \textbf{25 hores}.

    \item \textbf{TP2 - Estudi sobre clustering en sistemes de recomanació}
    
    S’analitzaran les diferents tècniques de clustering aplicades en entorns de recomanació, valorant com la seva implementació pot millorar la densitat de dades i reduir la complexitat computacional.
    L’objectiu és entendre l’impacte d’aquestes tècniques en la precisió i eficiència del sistema.
    Es calcula que aquesta tasca requerirà aproximadament \textbf{25 hores}.

    \item \textbf{TP3 - Preparació de l'entorn de treball}

    Consistirà en la configuració i comprovació de totes les eines i llibreries necessàries (com Python, Jupyter Notebook, Git, LaTeX, etc.) per assegurar que el desenvolupament posterior es realitzi en un entorn òptim i estable.
    Es calcula que aquesta tasca requerirà aproximadament \textbf{5 hores}.
\end{itemize}

\subsection{SD - Selecció i tractament del conjunt de dades}

Aquest apartat es centra en la gestió dels conjunts de dades que serviran de base per als experiments:

\begin{itemize}
    \item \textbf{SD1 - Recol·lecció de datasets}
    
    S’identificaran i obtindran conjunts de dades reals que representin diversos escenaris en sistemes de recomanació, tenint en compte diferents volums i nivells de dispersió.
    Aquesta tasca és clau per garantir que els experiments reflecteixin situacions reals.
    Es calcula que aquesta tasca requerirà aproximadament \textbf{15 hores}.

    \item \textbf{SD2 - Creació de datasets sintètics}
    
    Paral·lelament, es generaran conjunts de dades sintètics que permetin simular escenaris específics.
    D’aquesta manera, es podran avaluar els algorismes en entorns controlats, facilitant la comparació dels resultats i l’anàlisi dels comportaments davant diferents configuracions.
    Es calcula que aquesta tasca requerirà aproximadament \textbf{25 hores}.
\end{itemize}

\subsection{SR - Desenvolupament del sistema de recomanació}

En aquesta fase es durà a terme la implementació del sistema de recomanació mitjançant diferents mètodes de clustering.
Aquesta etapa constitueix el nucli del projecte, ja que el correcte desenvolupament de les tècniques determinarà la qualitat dels resultats obtinguts.

A cada mètode se li assignarà un temps estimat de desenvolupament segons la seva complexitat i la necessitat d'ajustament de paràmetres.
Es seguirà un esquema comú per a totes les implementacions, per garantir la comparabilitat dels resultats i la coherència en el codi.

\begin{itemize}
    \item \textbf{SR1 - Implementació amb clustering dur}
    
    Aquesta fase consistirà en la implementació amb clustering dur amb un algoritme com K-Means, un mètode de clustering dur on cada element es classifica de manera exclusiva en un clúster.
    Es realitzarà una anàlisi prèvia per determinar el nombre òptim de clústers utilitzant mètriques com l'Elbow Method o la Silhouette Score.
    Tindrà una duració estimada de \textbf{30 hores}.

    \item \textbf{SR2 - Implementació amb clustering difús}
    
    Es desenvoluparà la implementació amb clustering difús amb un mètode com Fuzzy C-Means, que permet una assignació parcial d'un element a diferents clústers amb un cert grau de pertinença.
    Aquest enfocament és especialment útil per a dades amb transicions difuses entre categories.
    S'estima una duració de \textbf{40 hores}.

    \item \textbf{SR3 - Implementació amb clustering jeràrquic}
    
    El clustering jeràrquic permet estructurar les dades en una jerarquia de clústers mitjançant l'agrupació ascendent (agglomerative) o descendent (divisive).
    Aquesta implementació inclourà l'elecció de la mesura de distància i el mètode d'enllaç per a la creació dels clústers.
    Tindrà una duració estimada de \textbf{40 hores}.

    \item \textbf{SR4 - Implementació amb clustering basat en densitat}
    
    Amb algoritmes com DBSCAN identificarà regions d'alta densitat de punts, permetent detectar patrons sense necessitat de definir prèviament el nombre de clústers.
    Això el fa ideal per a conjunts de dades amb estructures no lineals.
    Tindrà una duració estimada de \textbf{25 hores}.

    \item \textbf{SR5 - Implementació amb clustering basat en models}
    
    Es desenvoluparà una implementació amb un algoritme com el de Mixtures Gaussiana (GMM), que assumeix que les dades provenen de diverses distribucions gaussianes.
    Aquesta tècnica proporcionarà una estimació probabilística de l'assignació de cada punt a un clúster.
    S'estima una duració de \textbf{40 hores}.
\end{itemize}

\subsection{AR - Comparació de resultats i anàlisi del rendiment}

Aquesta fase està dedicada a l’avaluació i comparació dels algorismes implementats.
Amb índexs com el MAE (Mean Absolute Error) o el RMSE (Root Mean Square Error) per quantificar la precisió de les recomanacions generades.
I també mesurar l’eficiència computacional i el temps d’execució de cada tècnica, contrastant els resultats obtinguts. Aquesta anàlisi permetrà identificar l’algorisme que ofereix el millor equilibri entre precisió i rendiment en diferents escenaris.
Es preveu dedicar \textbf{40 hores} a aquesta tasca.

\subsection{D - Documentació}

Per a l'avaluació del treball és necessària la entrega d'una memòria final perquè aquest sigui avaluat.
La documentació s'anirà redactant durant tot el procés del treball.
Aquest recull tots els aspectes del projecte, incloent-hi la introducció, els objectius, la metodologia, els resultats obtinguts i les conclusions.
Es calcula que aquesta tasca requerirà aproximadament \textbf{60 hores} degut a que és bastant extensa.

\subsection{DT - Defensa del treball}

Per a defensar el treball davant del tribunal és necessari preparar unes diapositives i un guió que recullin tots els punts claus treballats. 
A més a més, caldrà realitzar una sèrie d'assaigs.
S'estima una duració de \textbf{15 hores}.

\section{Recursos}

\subsection{Recursos humans}

En aquest projecte es distingiran quatre rols clau: Cap del projecte, Investigador, Desenvolupador i Analista.  
Tenint en compte que el projecte es realitza de manera individual, l'autor del treball assumirà els diferents rols en funció de les tasques a realitzar.  
A la taula \ref{tab:tasques} es detallen els rols assignats a cada tasca.

\begin{itemize}
    \item \textbf{Cap del projecte (J):} Responsable de la gestió global del projecte, incloent-hi la planificació, el seguiment i la coordinació de les tasques. També s'encarregarà de l'elaboració de la documentació final i de la defensa del treball davant del tribunal.
    \item \textbf{Investigador (I):} Encarregat de la recerca bibliogràfica, de l'estudi de les tècniques de clustering i de la seva aplicació en sistemes de recomanació.
    \item \textbf{Desenvolupador (D):} Responsable de la implementació del sistema de recomanació amb els diferents algorismes de clustering. Aquest rol se centrarà en la programació.
    \item \textbf{Analista (A):} Encarregat de l'anàlisi dels resultats obtinguts, incloent-hi la comparació de les diferents tècniques de clustering i l'avaluació de la precisió i de l'eficiència computacional de cada mètode.
\end{itemize}

A més a més, es comptarà amb el suport del director del projecte, que actuarà com a guia i consultor durant el desenvolupament del treball, i amb el tutor de GEP, que s'encarregarà de donar suport en la gestió del projecte durant el primer mes.

\subsection{Recursos materials}

Els recursos materials inclouen totes les eines, tant de programari com de maquinari, necessàries per al desenvolupament del projecte.

\begin{itemize}
    \item \textbf{Recursos de maquinari:} S'utilitzarà un ordinador portàtil amb una capacitat de processament suficient per dur a terme les tasques de programació i d'anàlisi de dades, així com per a la investigació i la redacció de la documentació.
    \item \textbf{Recursos de programari:} Es farà servir un conjunt d'eines com Python, Jupyter Notebook, Matplotlib, NumPy, Pandas, Scikit-learn, Git, LaTeX i Notion. Aquestes eines permetran la programació, la visualització de dades, la gestió de versions i la redacció de la documentació.
\end{itemize}

\section{Taula resum de les tasques}

A continuació, es presenta una taula resum \ref{tab:tasques} de totes les tasques amb la seva descripció, durada, dependències, recursos i rols assignats.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{|c|p{4cm}|c|c|c|c|}
    \hline
    \textbf{ID} & \textbf{Tasca}                                     & \textbf{Temps (h)} & \textbf{Recursos} & \textbf{Dep.} & \textbf{Rols} \\ \hline
    \textbf{GP}          & \textbf{Gestió del Projecte}                                & \textbf{65}                 & \textbf{-}                 & \textbf{-}                     & \textbf{-}             \\ \hline
    GP1         & Contextualització            & 15                 & Portàtil, LaTeX   & TP                   & J             \\
    GP2         & Planificació temporal                              & 15                 & Portàtil, LaTeX   & GP1                   & J             \\
    GP3         & Sostenibilitat                  & 15                 & Portàtil, LaTeX   & GP2                   & J             \\
    GP4         & Seguiment del projecte                             & 20                 & -                 & -                     & J, I, D, A    \\ \hline
    \textbf{TP}          & \textbf{Treball Previ}                                      & \textbf{55}                 & \textbf{-}                 & \textbf{-}                     & \textbf{-}             \\ \hline
    TP1         & Sis. de Recomanació               & 25                 & Portàtil          & -                     & I             \\
    TP2         & Clustering & 25                 & Portàtil          & TP1                   & I             \\
    TP3         & Entorn de treball                  & 5                  & Portàtil, Python  & TP2                   & I             \\ \hline
    \textbf{SD}          & \textbf{Selecció dades}         & \textbf{40}                 & \textbf{-}                 & \textbf{-}                     & \textbf{-}             \\ \hline
    SD1         & Recol·lecció de datasets                           & 15                 & Portàtil, Python  & GP3                    & A             \\
    SD2         & Creació de datasets                    & 25                 & Portàtil, Python  & SD1                    & A             \\ \hline
    \textbf{SR}          & \textbf{Sis. de recomanació}         & \textbf{175}                & \textbf{-}                  & \textbf{-}                     & \textbf{-}             \\ \hline
    SR1         & Clustering dur                   & 30                 & Portàtil, Python  & SD                    & D             \\
    SR2         & Clustering difús                 & 40                 & Portàtil, Python  & SR1                    & D             \\
    SR3         & Clustering jeràrquic             & 40                 & Portàtil, Python  & SR2                    & D             \\
    SR4         & Clustering densitat     & 25                 & Portàtil, Python  & SR3                    & D             \\
    SR5         & Clustering models       & 40                 & Portàtil, Python  & SR4                    & D             \\ \hline
    \textbf{AR}          & \textbf{Anàlisi resultats}    & \textbf{40}                 & \textbf{Portàtil, Python}  & \textbf{SR}                    & \textbf{A}             \\ \hline
    \textbf{D}           & \textbf{Documentació}                                       & \textbf{60}                 & \textbf{Portàtil, LaTeX}   & \textbf{-}                     & \textbf{J, I, D, A}    \\ \hline
    \textbf{DT}          & \textbf{Defensa del treball}                                & \textbf{15}                 & \textbf{Portàtil, LaTeX}   & \textbf{D}                     & \textbf{J}             \\ \hline
    \textbf{-}           & \textbf{Total}                                              & \textbf{450}                & \textbf{-}                 & \textbf{-}                     & \textbf{-}             \\ \hline
    \end{tabular}

    \caption[Taula resum de les tasques del projecte] {Taula resum de les tasques del projecte
    
    Llegenda de rols: J - Cap del projecte, I - Investigador, D - Desenvolupador, A - Analista}
    \label{tab:tasques}
\end{table}

\begin{landscape} % Rotar la página a horizontal

    \section{Diagrama de Gantt}

    A continuació, es mostra la figura \ref{fig:gantt} amb el diagrama de Gantt de la planificació temporal del projecte. Aquest inclou totes les tasques definides anteriorment, agrupades per colors, juntament amb les seves dependències i durades.

    \begin{figure} [H]
        \centering
        \includegraphics[width=1.5\textwidth]{Figuras/DiagramaGantt.png}
        \caption{Diagrama de Gantt creat amb GanttPro \cite{GanttPRO}}
        \label{fig:gantt}
    \end{figure}

\end{landscape} % Tornar a la orientació vertical

\section{Gestió del risc}

Tots els projectes impliquen certs riscos que cal avaluar per garantir una cobertura eficient.
La gestió de riscos és un element clau en qualsevol iniciativa, ja que permet identificar, analitzar i classificar els riscos per reduir-ne els possibles efectes negatius.
En aquest sentit, es detallen els principals riscos que podrien afectar el projecte i les estratègies de mitigació.
A la taula \ref{tab:riscos} es resumeix la probabilitat d'ocurrència i una valoració qualitativa sobre com cada risc podria influir en el desenvolupament del projecte.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Risc}              & \textbf{Probabilitat} & \textbf{Impacte} \\ \hline
    Falta de coneixement       & Alta                  & Mitjà            \\ \hline
    Retards en la planificació & Mitjana               & Elevat           \\ \hline
    Dificultats tècniques      & Alta                  & Mitjà            \\ \hline
    Problemes amb els equips   & Baixa                 & Elevat           \\ \hline
    Complexitat computacional  & Mitjana               & Mitjà            \\ \hline
    \end{tabular}
    \caption{Taula de riscos del projecte}
    \label{tab:riscos}
\end{table}

\begin{itemize}
    \item \textbf{Falta de coneixement:} Aquest risc fa referència a la manca de coneixements previs en sistemes de recomanació i clustering, que podria dificultar el desenvolupament del projecte.
    Per mitigar aquest risc, s'ha reservat temps per a la recerca bibliogràfica i l'aprenentatge de les tècniques necessàries durant la planificació del projecte.
    \item \textbf{Retards en la planificació:} Aquest risc es refereix a la possibilitat que les tasques no s'executin segons el calendari establert, provocant endarreriments en la finalització del projecte.
    Per evitar aquest risc, es programaran reunions de seguiment periòdiques per avaluar l'estat d'execució de les tasques, es revisarà periòdicament el diagrama de Gantt i es reservaran marges temporals per absorbir possibles retards.
    \item \textbf{Dificultats tècniques:} En desenvolupar codi, és molt probable que sorgeixin imprevistos, com ara errors inesperats, incompatibilitats de llibreries o problemes de rendiment.
    Per resoldre aquests problemes, es reservarà temps extra per al desenvolupament de cada tècnica.
    \item \textbf{Problemes amb els equips:} Aquest risc contempla la possibilitat de fallades o inestabilitat en el maquinari utilitzat durant el desenvolupament i els experiments.
    Si fos necessari, es disposarà d'equips de recanvi per minimitzar la interrupció de la feina.
    \item \textbf{Complexitat computacional:} Algunes tècniques de clustering, especialment aquelles basades en models o clustering difús, poden requerir un alt consum de recursos computacionals, afectant el temps d'execució i la resposta del sistema.
    Per mitigar aquest risc, es poden optimitzar les implementacions o emprar hardware més potent, sol·licitant-lo a la universitat si cal.
\end{itemize}

\chapter{Gestió Econòmica i sostenibilitat}

\section{Gestió Econòmica}

Després d'establir la planificació temporal de la iniciativa, es procedeix a l'estimació dels costos requerits per al seu desenvolupament. S'identifiquen diverses categories de despeses, incloent-hi les associades al personal, l'espai de treball i les eines i dispositius emprats. A més, per afrontar possibles contratemps i cobrir despeses no previstes, s'elabora un pla de contingència, es defineix una partida per a imprevistos i s'estableixen mecanismes per al control pressupostari.

\subsection{Costos de personal}

A partir de la planificació de tasques, es calcula el cost de personal, tenint en compte els quatre rols definits anteriorment: Cap de projecte, Investigador, Desenvolupador i Analista.
Els costos anuals s'obtenen de la web de PayScale, que ofereix informació sobre salaris en la indústria, suposant menys d'un any d'experiència. El salari anual de cada rol es divideix pel nombre d'hores anuals de treball per obtenir el cost per hora, suposant 1.760 hores en total, tenint en compte vacances i festius.
Aquestes dades es mostren a la taula \ref{tab:costos_personal}.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Rol}     & \textbf{Cost anual} & \textbf{Cost per hora}  \\ \hline
    Cap del projecte & 46.303€\cite{ProjectManager}             & 46.303/1.760 = 26,30€/h \\ \hline
    Investigador     & 34.000€\cite{Research}             & 34.000/1.760 = 19,32€/h \\ \hline
    Desenvolupador   & 21.237€\cite{SoftwareDeveloper}             & 21.237/1.760 = 12,07€/h \\ \hline
    Analista         & 24.439€\cite{DataAnalyst}             & 24.439/1.760 = 13,89€/h \\ \hline
    \end{tabular}
    \caption{Costos de personal}
    \label{tab:costos_personal}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|c|l|c|c|}
    \hline
    \textbf{ID} & \textbf{Tasca}               & \multicolumn{1}{l|}{\textbf{Temps (h)}} & \textbf{Rols}       & \multicolumn{1}{l|}{\textbf{Cost (€)}} & \multicolumn{1}{l|}{\textbf{Cost SS (€)}} \\ \hline
    \textbf{GP} & \textbf{Gestió del Projecte} & \textbf{65}                             & \textbf{-}          & \textbf{1541,4}                        & \textbf{2003,82}                          \\ \hline
    GP1         & Contextualització            & 15                                      & J                   & 394,5                                  & 512,85                                    \\
    GP2         & Planificació temporal        & 15                                      & J                   & 394,5                                  & 512,85                                    \\
    GP3         & Sostenibilitat               & 15                                      & J                   & 394,5                                  & 512,85                                    \\
    GP4         & Seguiment del projecte       & 20                                      & J, I, D, A          & 357,9                                  & 465,27                                    \\ \hline
    \textbf{TP} & \textbf{Treball Previ}       & \textbf{55}                             & \textbf{-}          & \textbf{1062,6}                        & \textbf{1381,38}                          \\ \hline
    TP1         & Sis. de recomanació          & 25                                      & I                   & 483                                    & 627,9                                     \\
    TP2         & Clustering                   & 25                                      & I                   & 483                                    & 627,9                                     \\
    TP3         & Entorn de treball            & 5                                       & I                   & 96,6                                   & 125,58                                    \\ \hline
    \textbf{SD} & \textbf{Selecció dades}      & \textbf{40}                             & \textbf{-}          & \textbf{555,6}                         & \textbf{722,28}                           \\ \hline
    SD1         & Recol·lecció de datasets     & 15                                      & A                   & 208,35                                 & 270,855                                   \\
    SD2         & Creació de datasets          & 25                                      & A                   & 347,25                                 & 451,425                                   \\ \hline
    \textbf{SR} & \textbf{Sis. de recomanació} & \textbf{175}                            & \textbf{-}          & \textbf{2112,25}                       & \textbf{2745,925}                         \\ \hline
    SR1         & Clustering dur               & 30                                      & D                   & 362,1                                  & 470,73                                    \\
    SR2         & Clustering difús             & 40                                      & D                   & 482,8                                  & 627,64                                    \\
    SR3         & Clustering jeràrquic         & 40                                      & D                   & 482,8                                  & 627,64                                    \\
    SR4         & Clustering densitat          & 25                                      & D                   & 301,75                                 & 392,275                                   \\
    SR5         & Clustering models            & 40                                      & D                   & 482,8                                  & 627,64                                    \\ \hline
    \textbf{AR} & \textbf{Anàlisi resultats}   & \textbf{40}                             & \textbf{A}          & \textbf{555,6}                         & \textbf{722,28}                           \\ \hline
    \textbf{D}  & \textbf{Documentació}        & \textbf{60}                             & \textbf{J, I, D, A} & \textbf{1073,7}                        & \textbf{1395,81}                          \\ \hline
    \textbf{DT} & \textbf{Defensa del treball} & \textbf{15}                             & \textbf{J}          & \textbf{394,5}                         & \textbf{512,85}                           \\ \hline
    \textbf{-}  & \textbf{Total}               & \textbf{450}                            & \textbf{-}          & \textbf{7295,65}                       & \textbf{9484,345}                         \\ \hline
    \end{tabular}
    \caption[Costos de personal per tasca] {Costos de personal per tasca. 
    
    Llegenda de rols: J - Cap del projecte, I - Investigador, D - Desenvolupador, A - Analista}
    \label{tab:costos_personal_tasques}
\end{table}

A la taula \ref{tab:costos_personal_tasques} es mostren els costos de personal per tasca, on s'ha calculat el cost total de cada tasca en funció dels rols que hi intervenen i el temps que hi dediquen. En les tasques on el rol que la realitzi podria ser qualsevol, s'ha suposat el cost mitjà per hora dels rols.

El cost total de personal és de 7.295,65 €, mentre que el cost total de personal amb seguretat social, suposant un 30 \%, és de 9.484,35 €.

\subsection{Costos genèrics}

Tot el software utilitzat en el projecte és de codi obert, per tant, no hi ha cap cost associat a l'adquisició de llicències. El cost total de software és de 0 €.

Per realitzar el projecte, s'utilitza un ordinador portàtil ASUS amb un preu de 699€. Es calcula l'amortització tenint en compte que a l'any hi ha 220 dies laborables i 8 hores laborables al dia, segons la següent fórmula:

Es considera una vida útil de 5 anys.

\begin{equation}
    \text{Amortització} = \frac{\text{Preu ordinador}}{\text{Dies laborables} \times \text{Hores laborables} \times \text{Vida útil}} \times \text{Hores d'ús}
\end{equation}

\begin{equation}
    \text{Amortització} = \frac{699}{220 \times 8 \times 5} \times 450 = 35,74 \text{€}
\end{equation}

El projecte també té associats una sèrie de costos indirectes:

\begin{itemize}
    \item L'electricitat té un cost mitjà de 0,215€/kWh. El portàtil té un consum de 45W. Suposant que el portàtil estarà encès 430 hores, el cost de l'electricitat és de 0,215€/kWh * 0,045kW * 430h = 4,16€.
    \item Internet té un cost de 40€/mes. Suposant que el projecte duri 5 mesos, el cost total és de 40€/mes * 5 mesos = 200€.
    \item El desplaçament per a reunions de seguiment i altres té un cost de 44€ de manera trimestral, així que el cost total és de 44€ * 2 = 88€.
    \item El projecte es desenvolupa íntegrament a l'habitatge propi de l'autor, però si hagués de llogar l'habitació, el preu aproximat seria de 400€/mes. Suposant que el projecte duri 5 mesos, el cost total és de 400€/mes * 5 mesos = 2000€.
\end{itemize}

A la taula \ref{tab:costos_generics} es mostra un resum dels costos genèrics del projecte. El cost total de costos genèrics és de 2.277,9 €.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Concepte} & \textbf{Cost (€)} \\ \hline
    Amortització      & 35,74             \\
    Electricitat      & 4,16              \\
    Internet          & 150               \\
    Desplaçament      & 88                \\ 
    Lloc de treball   & 2000              \\ \hline
    \textbf{Total}    & \textbf{2277,9}  \\ \hline
    \end{tabular}
    \caption{Costos genèrics}
    \label{tab:costos_generics}
\end{table}

\subsection{Contingències}

En qualsevol iniciativa, és essencial incorporar un marge addicional per afrontar possibles contratemps i imprevistos. En aquest context, atès que es tracta d’un estudi basat en tecnologies emergents, hi ha una alta probabilitat d’enfrontar-se a desafiaments durant la seva execució. Per això, s’ha determinat establir un marge del 15 \% per cobrir aquests costos addicionals.

El cost de contingència es calcula amb la fórmula següent:

\begin{equation}
    \text{Cost contingència} = (\text{Cost total de personal} + \text{Cost total genèric}) \times 0,15
\end{equation}

\begin{equation}
    \text{Cost contingència} = (9484,35 + 2277,9) \times 0,15 = 1764,34 \text{€}
\end{equation}

\subsection{Imprevistos}

És fonamental tenir en compte les despeses derivades dels eventuals contratemps que puguin sorgir al llarg de l’execució d’aquest projecte. Com s’ha indicat prèviament, és important establir mesures per mitigar els possibles riscos associats a la realització de la tasca.

Per determinar el cost dels imprevistos, s’aplica la fórmula següent:

\begin{equation}
    \text{Cost imprevist} = \text{Cost} \times \frac{\text{Probabilitat (\%)}}{100}
\end{equation}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Imprevist}               & \textbf{Cost (€)} & \textbf{Prob. (\%)} & \textbf{Rol} & \textbf{Cost imprevist (€)} \\ \hline
    Increment desenvolupament (25 h) & 392,275           & 20                  & D            & 78,455                      \\
    Nou portàtil                     & 699               & 5                   & -            & 34,95                       \\ \hline
    \textbf{Total}                   & \textbf{-}        & \textbf{-}          & \textbf{-}   & \textbf{113,405}            \\ \hline
    \end{tabular}
    \caption[Costos imprevistos] {Costos imprevistos.

    Llegenda de rols: D - Desenvolupador}
    \label{tab:costos_imprevistos}
\end{table}

A la taula \ref{tab:costos_imprevistos} es mostren els costos dels imprevistos, on s'ha calculat el cost total de cada imprevist en funció de la probabilitat que succeeixi.

El risc de necessitar més temps de desenvolupament és alt, per la qual cosa s'estima una probabilitat del 20\% i un cost de 25 hores de desenvolupament. El risc de necessitar un nou portàtil és baix, per la qual cosa s'estima una probabilitat del 5\% i el cost d'un portàtil.

\subsection{Cost total}

A continuació es presenta una taula resum \ref{tab:cost_total} amb el cost total del projecte. El cost total del projecte és de 13.639,99 €.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Tipus}       & \textbf{Cost}     \\ \hline
    Cost personal        & 9484,345          \\
    Cost genèrics        & 2277,9            \\
    Cost de contingència & 1764,34           \\
    Cost imprevistos     & 113,405           \\ \hline
    \textbf{Cost total}  & \textbf{13639,99} \\ \hline
    \end{tabular}
    \caption{Cost total del projecte}
    \label{tab:cost_total}
\end{table}

\subsection{Control de gestió}

El control de gestió té com a objectiu principal supervisar, avaluar i ajustar de manera contínua totes les operacions del projecte per tal d’assegurar el compliment dels objectius establerts. Per aconseguir-ho, és imprescindible definir indicadors quantitatius que permetin mesurar el rendiment i detectar desviacions respecte al pressupost, el cronograma i els recursos planificats.

A continuació, es presenten alguns dels indicadors i mètriques que s’utilitzaran:

\begin{enumerate}
    \item Desviació cost personal per tasca:
    \[
    (\text{cost\_estimat} - \text{cost\_real}) * \text{hores\_reals}
    \]

    \item Desviació realització tasques:
    \[
    (\text{hores\_estimades} - \text{hores\_reals}) * \text{cost\_real}
    \]

    \item Desviació total realització tasca:
    \[
    \text{cost\_estimat} - \text{cost\_real}
    \]

    \item Desviació cost genèric:
    \[
    \text{cost\_estimat\_genèric} - \text{cost\_real\_genèric}
    \]

    \item Desviació cost imprevistos:
    \[
    \text{cost\_imprevistos\_estimat} - \text{cost\_imprevistos\_real}
    \]

    \item Desviació hores del projecte:
    \[
    \text{hores\_estimades} - \text{hores\_reals}
    \]

\end{enumerate}

Aquest control es durà a terme de manera periòdica mitjançant reunions bisetmanals, on es revisaran i actualitzaran els indicadors mitjançant una fulla de càlcul. En acabar cada tasca, es compararan les dades reals amb les estimades. A més, es registraran de forma detallada tots els despeses extra derivats d’imprevistos i contingències, de manera que qualsevol desviació, especialment aquella superior al 5\, pugui ser identificada i corregida a temps.

En cas de detectar desviacions negatives (per exemple, un cost superior al previst o retards en el cronograma), es prendran mesures correctives com ara:

\begin{itemize}
    \item Augmentar el pressupost mitjançant el fons reservat per a contingències.
    \item Ajustar el temps assignat o reassignar tasques per optimitzar el consum d’hores.
    \item Reduir o modificar parts del projecte per minimitzar els efectes dels desajustos detectats.
\end{itemize}

Aquest enfocament proactiu permetrà no només identificar ràpidament qualsevol desviació, sinó també actuar de manera eficaç per mantenir el projecte dins dels paràmetres planificats i assegurar-ne l’èxit global.

\section{Sostenibilitat}

Al llarg dels meus estudis en el Grau d'Enginyeria Informàtica a la FIB, he tingut l'oportunitat d'aprofundir en els aspectes clau de la sostenibilitat a través de diverses assignatures, reconeixent la seva importància durant el desenvolupament del TFG.

La sostenibilitat, entesa des d'una perspectiva integral que inclou les dimensions econòmica, social i ambiental, és un factor essencial en qualsevol projecte, tant si és de gran com de petita escala. A nivell personal, sempre he considerat fonamental l'anàlisi econòmica per garantir la viabilitat d'un projecte, ja que sovint es busca millorar l'eficiència econòmica d'una solució existent.

A més, aquest treball m'ha permès reflexionar sobre la importància d'abordar la sostenibilitat de manera global. He observat que, en moltes ocasions, es dona més pes als beneficis econòmics en detriment de les dimensions social i ambiental, un fet que considero crucial per assolir un impacte positiu i sostenible en el temps.

Finalment, crec que tots els projectes s'haurien de desenvolupar amb l'objectiu de cobrir una necessitat real de la societat.

\subsection{Dimensió econòmica}

El projecte s'ha plantejat amb una acurada estimació de costos, on s'han identificat i analitzat de manera detallada les diferents parts de personal, despeses genèriques, contingències i imprevistos. Així, en el PPP s'estableix un pressupost realista (aproximadament 13.640 €) que permet debatre tant la viabilitat econòmica del projecte com l'adequada assignació de recursos.

Pel que fa a la vida útil i a la resolució actual dels aspectes de costos del problema, l’estat de l’art aposta pel ús de tecnologies de codi obert i models d’anàlisi que optimitzen la gestió de dades. No obstant això, moltes de les solucions vigents no aborden completament el repte del cost computacional derivat del processament de grans volums de dades, aspecte que es compensa en el nostre enfocament mitjançant l’aplicació de tècniques de clustering que redueixen la dispersió i milloren l’eficiència.

Finalment, la solució proposada ofereix una millora econòmica significativa en comparació amb els mètodes existents. En incorporar el clustering com a pas previ al filtratge col·laboratiu, s’optimitzen els temps de processament i es redueix el consum de recursos computacionals, cosa que es tradueix en menys costos operatius i en una major rendibilitat i sostenibilitat a llarg termini.

\subsection{Dimensió ambiental}

Atès que es tracta d’un projecte de recerca i investigació dels diferents mètodes de clustering en sistemes recomanadors, l’impacte ambiental associat prové principalment del consum d’electricitat, sobretot si aquesta electricitat es genera a partir de fonts d’energia no renovables. El càlcul del consum elèctric que s'ha determinat correspon a l’electricitat consumida pel portàtil.

Al reutilitzar el portàtil, s’evita la producció de residus electrònics i es redueix l’impacte ambiental associat a la fabricació de nous dispositius. A més, la durada de vida útil del portàtil es veu ampliada gràcies a la seva correcta gestió i manteniment, la qual cosa contribueix a minimitzar el consum de recursos naturals i a reduir l’emissió de gasos contaminants.

En l’estat de l’art actual les solucions aborden el problema principal des d’un punt de vista tecnològic, però sovint no integren de manera completa criteris ambientals. La nostra proposta millora ambientalment les solucions existents, ja que no només optimitza els processos i redueix el consum de recursos, sinó que també incorpora mesures específiques per disminuir l’emissió de residus i l’ús energètic, contribuint així a una reducció global de l’impacte ambiental.

\subsection{Dimensió social}

La realització del projecte posat en producció suposa, a nivell personal, una gran oportunitat per créixer tant professionalment com humanament. A més d’ampliar els meus coneixements en àrees com la intel·ligència artificial i l’anàlisi de dades, em permetrà desenvolupar habilitats en la gestió i coordinació de projectes reals, fet que enriquirà el meu perfil professional.

D’altra banda, actualment el problema que es vol abordar es resol amb sistemes de recomanació basats en mètodes tradicionals, com el filtratge col·laboratiu, que tot i ser útils, presenten limitacions pel que fa a la personalització i l’eficiència. La solució que proposo pretén millorar socialment la qualitat de vida dels usuaris, oferint recomanacions més ajustades a les seves necessitats i interessos, fet que afavoreix una interacció més rellevant i satisfactòria.

inalment, existeix una necessitat real d’aquest projecte, ja que l’increment exponencial de la informació digital exigeix noves eines que superin les limitacions dels mètodes actuals. Això no sols permetrà una millor gestió dels recursos, sinó que també contribuirà a millorar la qualitat de vida social a través d’una experiència d’usuari més adaptada i eficient.


\printbibliography[heading=bibintoc]

\end{document}
